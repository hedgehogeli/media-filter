{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8dedcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.amp import autocast, GradScaler\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torchvision.transforms.v2 as transforms\n",
    "from PIL import Image\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6814b96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_dataloaders import *\n",
    "\n",
    "from models import *\n",
    "\n",
    "from custom_logging import *\n",
    "\n",
    "from mean_teacher import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cd2fdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    num_classes = 3\n",
    "    initial_lr = 1e-4\n",
    "    lr_backbone = 3e-5\n",
    "    batch_size = 32\n",
    "    num_epochs = 5\n",
    "    freeze_until_epoch = 0\n",
    "    checkpoint_path = \"./checkpoints\"\n",
    "    log_interval = 10\n",
    "    early_stopping_patience = 3\n",
    "    use_amp = True\n",
    "    consistency_weight = 0.5\n",
    "    ema_decay = 0.99\n",
    "    warmup_steps = 500\n",
    "    \n",
    "    # Create directories\n",
    "    os.makedirs(checkpoint_path, exist_ok=True)\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48c7b498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to: ./logs\\resnet152_mean_teacher_20250803_164505\n",
      "Initializing training on cuda\n",
      "CUDA available: True\n",
      "CUDA device: NVIDIA GeForce RTX 3090\n",
      "CUDA memory: 25.8 GB\n"
     ]
    }
   ],
   "source": [
    "logger = Logger(log_dir=\"./logs\", experiment_name=f\"resnet152_mean_teacher_{datetime.now().strftime('%Y%m%d_%H%M%S')}\")\n",
    "logger.log_config(config)\n",
    "\n",
    "print(f\"Initializing training on {config.device}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe761fa",
   "metadata": {},
   "source": [
    "## data transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8eca55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "weak_transform = transforms.Compose([\n",
    "    # transforms.ToImage(), transforms.ToDtype(torch.float32, scale=True), # move this out\n",
    "    \n",
    "    transforms.RandomRotation(degrees=(-5, 5), interpolation=transforms.InterpolationMode.BILINEAR, expand=True, fill=0),\n",
    "    transforms.Resize(size=256, interpolation=transforms.InterpolationMode.BILINEAR, antialias=True), # Resize maintaining aspect ratio, then pad to square\n",
    "    transforms.RandomCrop(224), \n",
    "\n",
    "    transforms.ColorJitter(brightness=0.05, contrast=0.05, saturation=0.05, hue=0.02),\n",
    "    transforms.Lambda(lambda x: x + torch.randn_like(x) * 0.005), transforms.Lambda(lambda x: torch.clamp(x, 0, 1)),\n",
    "\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "strong_transform = transforms.Compose([\n",
    "    # transforms.ToImage(), transforms.ToDtype(torch.float32, scale=True),\n",
    "    \n",
    "    transforms.RandomRotation(degrees=(-15, 15), interpolation=transforms.InterpolationMode.BILINEAR, expand=True, fill=0),\n",
    "    transforms.Resize(size=256, interpolation=transforms.InterpolationMode.BILINEAR, antialias=True), # Resize maintaining aspect ratio, then pad to square\n",
    "    transforms.RandomCrop(224), \n",
    "\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),\n",
    "    transforms.Lambda(lambda x: x + torch.randn_like(x) * 0.01), transforms.Lambda(lambda x: torch.clamp(x, 0, 1)),\n",
    "    \n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.ToImage(), transforms.ToDtype(torch.float32, scale=True),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    \n",
    "    transforms.Resize(size=256, interpolation=transforms.InterpolationMode.BILINEAR, antialias=True), # Resize maintaining aspect ratio, then pad to square\n",
    "    transforms.CenterCrop(224), \n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932d7ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating optimized datasets...\n",
      "Train batches: 7823\n",
      "Val batches: 978\n"
     ]
    }
   ],
   "source": [
    "train_dataset = StandardImageDataset(\n",
    "    csv_file=\"train.csv\", \n",
    "    root_dir=\".\\\\data\\\\train\",\n",
    "    weak_transform=weak_transform, \n",
    "    strong_transform=strong_transform,\n",
    "    device= torch.device('cuda'), # set to CPU if a batch of 64 takes longer than 5.5s on the model\n",
    "    # currently it costs around 1s of GPU time to do transforms on the GPU (probably a bit less, if scheudle optimizations are happening)\n",
    ")\n",
    "\n",
    "val_dataset = ImageDataset(\n",
    "    csv_file=\"val.csv\", \n",
    "    root_dir=\".\\\\data\\\\val\", \n",
    "    transform=val_transform\n",
    ")\n",
    "\n",
    "\n",
    "# Wrap with CUDA prefetcher\n",
    "train_loader = DataLoader(train_dataset, config.batch_size, True)\n",
    "val_loader = DataLoader(val_dataset, config.batch_size, True)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")\n",
    "\n",
    "# TODO: these batch_sizes aren't getting loaded from checkpoint. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91a37367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing models...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize models\n",
    "print(\"\\nInitializing models...\")\n",
    "model = ResNet152Classifier(num_classes=config.num_classes).to(config.device)\n",
    "teacher_model = copy.deepcopy(model).to(config.device)\n",
    "for p in teacher_model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "# Optimizer setup\n",
    "backbone_params = []\n",
    "fc_params = []\n",
    "\n",
    "for name, param in model.backbone.named_parameters():\n",
    "    if name.startswith(\"fc.\"):\n",
    "        fc_params.append(param)\n",
    "    elif name.startswith(\"conv1.\"):\n",
    "        param.requires_grad = False\n",
    "    else:\n",
    "        backbone_params.append(param)\n",
    "\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {'params': backbone_params, 'lr': config.lr_backbone, 'weight_decay': 1e-4},\n",
    "    {'params': fc_params, 'lr': config.initial_lr, 'weight_decay': 1e-4},\n",
    "])\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n",
    "\n",
    "# Training setup\n",
    "criterion = nn.CrossEntropyLoss().to(config.device)\n",
    "scaler = GradScaler() if config.use_amp else None\n",
    "monitor = PerformanceMonitor()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100e621a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c535c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(checkpoint_path, config, device='cuda'):\n",
    "    \"\"\"\n",
    "    Load a checkpoint and restore training state\n",
    "    \n",
    "    Args:\n",
    "        checkpoint_path: Path to the .pth file\n",
    "        config: Configuration object (will be updated with checkpoint config)\n",
    "        device: Device to load the model on\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with loaded components\n",
    "    \"\"\"\n",
    "    print(f\"Loading checkpoint from: {checkpoint_path}\")\n",
    "    \n",
    "    # Load checkpoint\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    \n",
    "    # Update config with checkpoint settings (optional)\n",
    "    if 'config' in checkpoint:\n",
    "        checkpoint_config = checkpoint['config']\n",
    "        print(\"Checkpoint configuration:\")\n",
    "        for key, value in checkpoint_config.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "            # Optionally update current config\n",
    "            # setattr(config, key, value)\n",
    "    \n",
    "    # Initialize models with same architecture\n",
    "    model = ResNet152Classifier(num_classes=config.num_classes).to(device)\n",
    "    teacher_model = ResNet152Classifier(num_classes=config.num_classes).to(device)\n",
    "    \n",
    "    # Load model weights\n",
    "    if 'model_state_dict' in checkpoint:\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(\"Loaded student model weights\")\n",
    "    \n",
    "    if 'teacher_state_dict' in checkpoint:\n",
    "        teacher_model.load_state_dict(checkpoint['teacher_state_dict'])\n",
    "        print(\"Loaded teacher model weights\")\n",
    "    \n",
    "    # Make teacher model non-trainable\n",
    "    for p in teacher_model.parameters():\n",
    "        p.requires_grad = False\n",
    "    \n",
    "    # Initialize optimizer (same structure as training)\n",
    "    backbone_params = []\n",
    "    fc_params = []\n",
    "    \n",
    "    for name, param in model.backbone.named_parameters():\n",
    "        if name.startswith(\"fc.\"):\n",
    "            fc_params.append(param)\n",
    "        elif name.startswith(\"conv1.\"):\n",
    "            param.requires_grad = False\n",
    "        else:\n",
    "            backbone_params.append(param)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW([\n",
    "        {'params': backbone_params, 'lr': config.lr_backbone, 'weight_decay': 1e-4},\n",
    "        {'params': fc_params, 'lr': config.initial_lr, 'weight_decay': 1e-4},\n",
    "    ])\n",
    "    \n",
    "    # Load optimizer state\n",
    "    if 'optimizer_state_dict' in checkpoint:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        print(\"Loaded optimizer state\")\n",
    "    \n",
    "    # Initialize scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n",
    "    \n",
    "    # Load scheduler state\n",
    "    if 'scheduler_state_dict' in checkpoint:\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        print(\"Loaded scheduler state\")\n",
    "    \n",
    "    # Get training progress info\n",
    "    start_epoch = checkpoint.get('epoch', -1) + 1  # Resume from next epoch\n",
    "    best_accuracy = checkpoint.get('val_accuracy', 0)\n",
    "    \n",
    "    print(f\"\\nCheckpoint info:\")\n",
    "    print(f\"  Saved at epoch: {checkpoint.get('epoch', 'unknown')}\")\n",
    "    print(f\"  Best validation accuracy: {best_accuracy:.4f}\")\n",
    "    print(f\"  Will resume from epoch: {start_epoch}\")\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'teacher_model': teacher_model,\n",
    "        'optimizer': optimizer,\n",
    "        'scheduler': scheduler,\n",
    "        'start_epoch': start_epoch,\n",
    "        'best_accuracy': best_accuracy,\n",
    "        'checkpoint': checkpoint\n",
    "    }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f3566a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint from: checkpoints/best_model_acc0.7807.pth\n",
      "Checkpoint configuration:\n",
      "  freeze_until_epoch: 1\n",
      "  num_epochs: 2\n",
      "Loaded student model weights\n",
      "Loaded teacher model weights\n",
      "Loaded optimizer state\n",
      "Loaded scheduler state\n",
      "\n",
      "Checkpoint info:\n",
      "  Saved at epoch: 1\n",
      "  Best validation accuracy: 0.7807\n",
      "  Will resume from epoch: 2\n"
     ]
    }
   ],
   "source": [
    "# IF WE ARE RESUMING:\n",
    "\n",
    "loaded = load_checkpoint(\"checkpoints/best_model_acc0.7807.pth\", config)\n",
    "\n",
    "model = loaded['model']\n",
    "teacher_model = loaded['teacher_model']\n",
    "optimizer = loaded['optimizer']\n",
    "scheduler = loaded['scheduler']\n",
    "start_epoch = loaded['start_epoch']\n",
    "best_accuracy = loaded['best_accuracy']\n",
    "\n",
    "monitor.best_accuracy = best_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff70c709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # run the one off code shit here\n",
    "config.num_epochs = 10\n",
    "# config.batch_size=\n",
    "start_epoch=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731dcc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4ac15b",
   "metadata": {},
   "source": [
    "## main training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86c0a30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def validate(model, val_loader, config, monitor, epoch):\n",
    "    \"\"\"Validate model and compute metrics\"\"\"\n",
    "    model.eval()\n",
    "    total, correct = 0, 0\n",
    "    class_correct = [0] * config.num_classes\n",
    "    class_total = [0] * config.num_classes\n",
    "    all_preds, all_labels = [], []\n",
    "    val_loss = 0\n",
    "    \n",
    "    # Handle both regular DataLoader and DataPrefetcher\n",
    "    if isinstance(val_loader, DataPrefetcher):\n",
    "        total_batches = len(val_loader.loader)\n",
    "    else:\n",
    "        total_batches = len(val_loader)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        val_pbar = tqdm(val_loader, desc=f'Validation Epoch {epoch}', total=total_batches, leave=False)\n",
    "        for batch_data in val_pbar:\n",
    "            \n",
    "            x, y = batch_data\n",
    "            x = x.to(config.device)\n",
    "            y = y.to(config.device)\n",
    "            \n",
    "            outputs = model(x)\n",
    "            loss = F.cross_entropy(outputs, y)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            preds = outputs.argmax(dim=1)\n",
    "            preds_cpu = preds.cpu().numpy()\n",
    "            y_cpu = y.cpu().numpy()\n",
    "            all_preds.extend(preds_cpu)\n",
    "            all_labels.extend(y_cpu)\n",
    "            \n",
    "            total += y.size(0)\n",
    "            correct += (preds == y).sum().item()\n",
    "            \n",
    "            for i, label in enumerate(y_cpu):\n",
    "                class_total[label] += 1\n",
    "                class_correct[label] += (preds_cpu[i] == label).item()\n",
    "            \n",
    "            val_pbar.set_postfix({'loss': f'{loss.item():.4f}', \n",
    "                                  'acc': f'{correct/total:.4f}'})\n",
    "\n",
    "    accuracy = correct / total \n",
    "    avg_val_loss = val_loss / total_batches\n",
    "    class_names = ['bad', 'neutral', 'good']\n",
    "    \n",
    "    # Calculate per-class metrics\n",
    "    class_recall = [class_correct[i] / class_total[i] if class_total[i] else 0 for i in range(config.num_classes)]\n",
    "    \n",
    "    # Calculate confusion matrix and classification report\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    report = classification_report(all_labels, all_preds, target_names=class_names, output_dict=True)\n",
    "    \n",
    "    # Log metrics\n",
    "    metrics = {\n",
    "        \"val/accuracy\": accuracy,\n",
    "        \"val/loss\": avg_val_loss,\n",
    "        \"val/recall_bad\": class_recall[0],\n",
    "        \"val/recall_neutral\": class_recall[1],\n",
    "        \"val/recall_good\": class_recall[2],\n",
    "    }\n",
    "    \n",
    "    # Add precision and F1 scores if available\n",
    "    for class_name in class_names:\n",
    "        if class_name in report:\n",
    "            metrics[f\"val/precision_{class_name}\"] = report[class_name]['precision']\n",
    "            metrics[f\"val/f1_{class_name}\"] = report[class_name]['f1-score']\n",
    "    \n",
    "    # Update monitor\n",
    "    monitor.accuracy_history.append(accuracy)\n",
    "    \n",
    "    if accuracy > monitor.best_accuracy:\n",
    "        monitor.best_accuracy = accuracy\n",
    "        monitor.epochs_without_improvement = 0\n",
    "    else:\n",
    "        monitor.epochs_without_improvement += 1\n",
    "    \n",
    "    model.train()\n",
    "    return accuracy, metrics, cm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bdf886",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28c9827f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training...\n",
      "Tensorboard logs: tensorboard --logdir ./logs\\resnet152_mean_teacher_20250803_164505\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "print(\"\\nStarting training...\")\n",
    "print(f\"Tensorboard logs: tensorboard --logdir {logger.get_log_dir()}\")\n",
    "global_step = start_epoch * len(train_loader)\n",
    "best_accuracy = 0 \n",
    "data_loading_times = []\n",
    "gpu_compute_times = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "18bb581e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epochs:  [5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "print(\"training epochs: \", list(range(start_epoch, config.num_epochs)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6e36e9ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5:  33%|███▎      | 2559/7823 [20:44<44:34,  1.97it/s, loss=0.2153, img/s=250.8, data_ms=0.0, gpu_ms=127.6]  c:\\Users\\hedge\\.conda\\envs\\img-classifier\\Lib\\site-packages\\PIL\\Image.py:1054: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "Epoch 5: 100%|██████████| 7823/7823 [1:03:20<00:00,  2.06it/s, loss=0.6607, img/s=473.4, data_ms=0.0, gpu_ms=67.6] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5 Summary:\n",
      "  Time: 63.3 minutes\n",
      "  Avg Loss: 0.3304\n",
      "  Avg Classification Loss: 0.2513\n",
      "  Avg Consistency Loss: 0.1581\n",
      "  Throughput: 65.9 images/second\n",
      "Running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation Accuracy: 0.8222\n",
      "  Best Accuracy: 0.8246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6:  48%|████▊     | 3717/7823 [29:39<31:56,  2.14it/s, loss=0.3502, img/s=250.9, data_ms=0.0, gpu_ms=127.5]  c:\\Users\\hedge\\.conda\\envs\\img-classifier\\Lib\\site-packages\\PIL\\Image.py:1054: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "Epoch 6: 100%|██████████| 7823/7823 [1:02:40<00:00,  2.08it/s, loss=0.6658, img/s=533.0, data_ms=0.0, gpu_ms=60.0] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6 Summary:\n",
      "  Time: 62.7 minutes\n",
      "  Avg Loss: 0.3023\n",
      "  Avg Classification Loss: 0.2190\n",
      "  Avg Consistency Loss: 0.1665\n",
      "  Throughput: 66.6 images/second\n",
      "Running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation Accuracy: 0.8273\n",
      "  Best Accuracy: 0.8273\n",
      "Saved best model: ./checkpoints\\best_model_acc0.8273.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7:  17%|█▋        | 1348/7823 [10:44<53:02,  2.03it/s, loss=0.3502, img/s=250.4, data_ms=0.0, gpu_ms=127.8]  c:\\Users\\hedge\\.conda\\envs\\img-classifier\\Lib\\site-packages\\PIL\\Image.py:1054: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "Epoch 7: 100%|██████████| 7823/7823 [1:02:37<00:00,  2.08it/s, loss=0.2742, img/s=509.4, data_ms=0.0, gpu_ms=62.8] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7 Summary:\n",
      "  Time: 62.6 minutes\n",
      "  Avg Loss: 0.2771\n",
      "  Avg Classification Loss: 0.1935\n",
      "  Avg Consistency Loss: 0.1672\n",
      "  Throughput: 66.6 images/second\n",
      "Running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation Accuracy: 0.8328\n",
      "  Best Accuracy: 0.8328\n",
      "Saved best model: ./checkpoints\\best_model_acc0.8328.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8:   6%|▌         | 473/7823 [03:44<55:50,  2.19it/s, loss=0.3083, img/s=250.7, data_ms=0.0, gpu_ms=127.7]  c:\\Users\\hedge\\.conda\\envs\\img-classifier\\Lib\\site-packages\\PIL\\Image.py:1054: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "Epoch 8: 100%|██████████| 7823/7823 [1:02:38<00:00,  2.08it/s, loss=0.1759, img/s=532.2, data_ms=0.0, gpu_ms=60.1] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8 Summary:\n",
      "  Time: 62.6 minutes\n",
      "  Avg Loss: 0.2619\n",
      "  Avg Classification Loss: 0.1776\n",
      "  Avg Consistency Loss: 0.1685\n",
      "  Throughput: 66.6 images/second\n",
      "Running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation Accuracy: 0.8272\n",
      "  Best Accuracy: 0.8328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9:  43%|████▎     | 3395/7823 [27:05<35:57,  2.05it/s, loss=0.3363, img/s=250.6, data_ms=0.0, gpu_ms=127.7] c:\\Users\\hedge\\.conda\\envs\\img-classifier\\Lib\\site-packages\\PIL\\Image.py:1054: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "Epoch 9: 100%|██████████| 7823/7823 [1:02:38<00:00,  2.08it/s, loss=0.1418, img/s=503.0, data_ms=0.0, gpu_ms=63.6] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9 Summary:\n",
      "  Time: 62.6 minutes\n",
      "  Avg Loss: 0.2537\n",
      "  Avg Classification Loss: 0.1687\n",
      "  Avg Consistency Loss: 0.1700\n",
      "  Throughput: 66.6 images/second\n",
      "Running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation Accuracy: 0.8227\n",
      "  Best Accuracy: 0.8328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(start_epoch, config.num_epochs):\n",
    "    # Unfreeze backbone if needed\n",
    "    if epoch == config.freeze_until_epoch:\n",
    "        print(f\"Unfreezing backbone at epoch {epoch}\")\n",
    "        for param in backbone_params:\n",
    "            param.requires_grad = True\n",
    "    \n",
    "    model.train()\n",
    "    epoch_start = time.time()\n",
    "    epoch_loss = 0\n",
    "    epoch_cls_loss = 0\n",
    "    epoch_consistency_loss = 0\n",
    "\n",
    "    # Training epoch\n",
    "    epoch_pbar = tqdm(train_loader, desc=f'Epoch {epoch}', total=len(train_loader))\n",
    "    \n",
    "    for batch_idx, batch_data in enumerate(epoch_pbar):\n",
    "        batch_start = time.time()\n",
    "        \n",
    "        x_weak, x_strong, y = batch_data\n",
    "        data_load_time = time.time() - batch_start\n",
    "        \n",
    "        compute_start = time.time()\n",
    "        \n",
    "        # Mixed precision training\n",
    "        if config.use_amp:\n",
    "            with autocast(device_type='cuda'):\n",
    "                student_outputs = model(x_strong)\n",
    "                cls_loss = criterion(student_outputs, y.to(config.device))\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    teacher_outputs = teacher_model(x_weak)\n",
    "                \n",
    "                consistency_weight = config.consistency_weight * min(1.0, global_step / config.warmup_steps)\n",
    "                consistency = compute_consistency_loss(student_outputs, teacher_outputs)\n",
    "                loss = cls_loss + consistency_weight * consistency\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            student_outputs = model(x_strong)\n",
    "            cls_loss = criterion(student_outputs, y)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                teacher_outputs = teacher_model(x_weak)\n",
    "            \n",
    "            consistency_weight = config.consistency_weight * min(1.0, global_step / config.warmup_steps)\n",
    "            consistency = compute_consistency_loss(student_outputs, teacher_outputs)\n",
    "            loss = cls_loss + consistency_weight * consistency\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Update teacher model\n",
    "        update_ema_variables(model, teacher_model, alpha=config.ema_decay, global_step=global_step)\n",
    "        \n",
    "        # Timing\n",
    "        gpu_compute_time = time.time() - compute_start\n",
    "        total_time = time.time() - batch_start\n",
    "        \n",
    "        # Track times\n",
    "        if len(data_loading_times) < 1000:\n",
    "            data_loading_times.append(data_load_time)\n",
    "            gpu_compute_times.append(gpu_compute_time)\n",
    "        \n",
    "        # Update epoch stats\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_cls_loss += cls_loss.item()\n",
    "        epoch_consistency_loss += consistency.item()\n",
    "        \n",
    "        # Calculate throughput\n",
    "        images_per_second = config.batch_size / total_time\n",
    "        \n",
    "        # Update progress bar\n",
    "        epoch_pbar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'img/s': f'{images_per_second:.1f}',\n",
    "            'data_ms': f'{data_load_time*1000:.1f}',\n",
    "            'gpu_ms': f'{gpu_compute_time*1000:.1f}'\n",
    "        })\n",
    "        \n",
    "        # Logging\n",
    "        if global_step % config.log_interval == 0:\n",
    "            avg_data_time = np.mean(data_loading_times[-100:]) if data_loading_times else 0\n",
    "            avg_gpu_time = np.mean(gpu_compute_times[-100:]) if gpu_compute_times else 0\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            \n",
    "            # Log to TensorBoard\n",
    "            train_metrics = {\n",
    "                \"train/loss\": loss.item(),\n",
    "                \"train/cls_loss\": cls_loss.item(),\n",
    "                \"train/consistency\": consistency.item(),\n",
    "                \"train/consistency_weight\": consistency_weight,\n",
    "                \"train/learning_rate\": current_lr,\n",
    "                \"train/images_per_second\": images_per_second,\n",
    "                \"timing/data_load_ms\": avg_data_time * 1000,\n",
    "                \"timing/gpu_compute_ms\": avg_gpu_time * 1000,\n",
    "                \"timing/data_load_percentage\": (avg_data_time / (avg_data_time + avg_gpu_time)) * 100 if (avg_data_time + avg_gpu_time) > 0 else 0,\n",
    "                \"system/gpu_memory_mb\": get_gpu_memory_usage()\n",
    "            }\n",
    "            \n",
    "            logger.log_metrics(train_metrics, global_step)\n",
    "            \n",
    "            # Log to CSV\n",
    "            logger.log_train_step(global_step, epoch, {\n",
    "                'loss': loss.item(),\n",
    "                'cls_loss': cls_loss.item(),\n",
    "                'consistency_loss': consistency.item(),\n",
    "                'learning_rate': current_lr,\n",
    "                'consistency_weight': consistency_weight\n",
    "            })\n",
    "            \n",
    "            # System metrics\n",
    "            queue_size = train_loader.dataset.get_stats().get('queue_size', 0) if hasattr(train_loader.dataset, 'get_stats') else 0\n",
    "            logger.log_system_metrics(global_step, {\n",
    "                'images_per_second': images_per_second,\n",
    "                'data_load_ms': avg_data_time * 1000,\n",
    "                'gpu_compute_ms': avg_gpu_time * 1000,\n",
    "                'queue_size': queue_size,\n",
    "                'gpu_memory_mb': get_gpu_memory_usage()\n",
    "            })\n",
    "        \n",
    "        global_step += 1\n",
    "            \n",
    "    # End of epoch\n",
    "    epoch_time = time.time() - epoch_start\n",
    "    avg_epoch_loss = epoch_loss / len(train_loader) if len(train_loader) > 0 else 0\n",
    "    avg_epoch_cls_loss = epoch_cls_loss / len(train_loader) if len(train_loader) > 0 else 0\n",
    "    avg_epoch_consistency_loss = epoch_consistency_loss / len(train_loader) if len(train_loader) > 0 else 0\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch} Summary:\")\n",
    "    print(f\"  Time: {epoch_time/60:.1f} minutes\")\n",
    "    print(f\"  Avg Loss: {avg_epoch_loss:.4f}\")\n",
    "    print(f\"  Avg Classification Loss: {avg_epoch_cls_loss:.4f}\")\n",
    "    print(f\"  Avg Consistency Loss: {avg_epoch_consistency_loss:.4f}\")\n",
    "    print(f\"  Throughput: {len(train_dataset) / epoch_time:.1f} images/second\")\n",
    "    \n",
    "    # Validation\n",
    "    print(\"Running validation...\")\n",
    "    val_acc, val_metrics, cm = validate(teacher_model, val_loader, config, monitor, epoch)\n",
    "    \n",
    "    # Log validation metrics\n",
    "    logger.log_metrics(val_metrics, epoch)\n",
    "    logger.log_validation(epoch, val_metrics)\n",
    "    logger.log_confusion_matrix(cm, ['bad', 'neutral', 'good'], epoch)\n",
    "    \n",
    "    print(f\"  Validation Accuracy: {val_acc:.4f}\")\n",
    "    print(f\"  Best Accuracy: {monitor.best_accuracy:.4f}\")\n",
    "    \n",
    "    # Save checkpoint if best\n",
    "    if val_acc > best_accuracy:\n",
    "        best_accuracy = val_acc\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'teacher_state_dict': teacher_model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'val_accuracy': val_acc,\n",
    "            'config': vars(config)\n",
    "        }\n",
    "        checkpoint_path = os.path.join(config.checkpoint_path, f\"best_model_acc{val_acc:.4f}.pth\")\n",
    "        torch.save(checkpoint, checkpoint_path)\n",
    "        print(f\"Saved best model: {checkpoint_path}\")\n",
    "    \n",
    "    # Learning rate scheduling\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Early stopping\n",
    "    if monitor.epochs_without_improvement >= config.early_stopping_patience:\n",
    "        print(f\"Early stopping at epoch {epoch}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "68dfbd75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaning up...\n",
      "\n",
      "=== Training Complete ===\n",
      "Best validation accuracy: 0.8328\n",
      "Logs saved to: ./logs\\resnet152_mean_teacher_20250803_164505\n",
      "View in TensorBoard: tensorboard --logdir ./logs\\resnet152_mean_teacher_20250803_164505\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./checkpoints\\\\best_model_acc0.8328.pth'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Cleanup\n",
    "print(\"\\nCleaning up...\")\n",
    "if hasattr(train_loader.dataset, 'cleanup'):\n",
    "    train_loader.dataset.cleanup()\n",
    "if hasattr(train_dataset, 'cleanup'):\n",
    "    train_dataset.cleanup()\n",
    "\n",
    "# Final statistics\n",
    "print(\"\\n=== Training Complete ===\")\n",
    "print(f\"Best validation accuracy: {best_accuracy:.4f}\")\n",
    "print(f\"Logs saved to: {logger.get_log_dir()}\")\n",
    "print(f\"View in TensorBoard: tensorboard --logdir {logger.get_log_dir()}\")\n",
    "\n",
    "# Close logger\n",
    "logger.close()\n",
    "\n",
    "checkpoint_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d070c429",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea291fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "742255f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 978\n"
     ]
    }
   ],
   "source": [
    "test_dataset = ImageDataset(\n",
    "    csv_file=\"test.csv\", \n",
    "    root_dir=\".\\\\data\\\\test\", \n",
    "    transform=val_transform\n",
    ")\n",
    "test_loader = DataLoader(test_dataset, config.batch_size, False)\n",
    "print(f\"Train batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa61cd8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    }
   ],
   "source": [
    "test_acc, test_metrics, cm = validate(teacher_model, test_loader, config, monitor, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1bd863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded teacher model for inference\n"
     ]
    }
   ],
   "source": [
    "inf_model = load_for_inference(checkpoint_path=\".\\\\checkpoints\\\\best_model_acc0.8328.pth\", device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b7d612",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "img-classifier",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
