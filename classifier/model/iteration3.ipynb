{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb3e625f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "import torchvision.transforms.v2 as transforms\n",
    "from torchvision.transforms.v2 import functional as v2F\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler\n",
    "\n",
    "from ultralytics import YOLO\n",
    "from transformers import CLIPVisionModel, CLIPImageProcessor\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "import platform\n",
    "import gc\n",
    "import psutil\n",
    "import time\n",
    "\n",
    "# from models import *\n",
    "from custom_logging import *\n",
    "from mean_teacher import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691d38e6",
   "metadata": {},
   "source": [
    "# Hardware/system stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a67b9496",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1f81511",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('linux', 10)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_system_type():\n",
    "        system = platform.system()\n",
    "        \n",
    "        if system == \"Linux\":\n",
    "            if \"microsoft\" in platform.uname().release.lower() or \\\n",
    "            \"wsl\" in platform.uname().release.lower():\n",
    "                return \"wsl\"\n",
    "            return \"linux\"\n",
    "        elif system == \"Windows\":\n",
    "            return \"windows\"\n",
    "        else:\n",
    "            return \"other\"\n",
    "\n",
    "def get_num_workers():\n",
    "    \n",
    "    system_type = get_system_type()\n",
    "    if system_type == \"linux\":\n",
    "        return 10\n",
    "    elif system_type == \"windows\":\n",
    "        return 0\n",
    "    elif system_type == \"wsl\":\n",
    "        return 4\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "get_system_type(), get_num_workers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fd72e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gpu_memory_usage():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.cuda.memory_allocated() / 1024 / 1024  # MB\n",
    "    return 0\n",
    "\n",
    "def print_memory_usage():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    mem_info = process.memory_info()\n",
    "    print(f\"Main process RSS: {mem_info.rss / 1024**3:.2f} GB\")\n",
    "    \n",
    "    # Check worker processes\n",
    "    children = process.children()\n",
    "    for i, child in enumerate(children):\n",
    "        try:\n",
    "            child_mem = child.memory_info()\n",
    "            print(f\"Worker {i} RSS: {child_mem.rss / 1024**3:.2f} GB\")\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "def get_system_memory_usage():\n",
    "    return psutil.virtual_memory().percent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f8b668",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc7d5b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to: ./logs/resnet_clip_yolo_mean_teacher_20251029_112330\n"
     ]
    }
   ],
   "source": [
    "run_name = f\"resnet_clip_yolo_mean_teacher_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "\n",
    "logger = Logger(log_dir=\"./logs\", experiment_name=run_name)\n",
    "\n",
    "class Config:\n",
    "    device = device\n",
    "    use_amp = True\n",
    "    batch_size = 40 # 20 # basically the max\n",
    "\n",
    "    num_classes = 3\n",
    "\n",
    "    initial_lr = 5e-5\n",
    "    lr_backbone = 1e-5\n",
    "    consistency_weight = 2.2 # 0.6\n",
    "    ema_decay = 0.99\n",
    "\n",
    "    warmup_steps = 40000 # scaling (in num batches) from 0 to consistency_weight for consistency loss\n",
    "\n",
    "    cur_epoch = 0\n",
    "    num_epochs = 4\n",
    "    freeze_until_epoch = 0\n",
    "\n",
    "    checkpoint_path = f\"./checkpoints/{run_name}\"\n",
    "    log_interval = 5\n",
    "    \n",
    "    # Create directories\n",
    "    os.makedirs(checkpoint_path, exist_ok=True)\n",
    "\n",
    "config = Config()\n",
    "\n",
    "logger.log_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4442ae51",
   "metadata": {},
   "outputs": [],
   "source": [
    "dry_run = None # set to None if not a dry run, set to desired num of rows if dry run\n",
    "# dry_run = config.batch_size * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50f83e2",
   "metadata": {},
   "source": [
    "# MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9aa95d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLOv11(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = YOLO(\"yolov11l-face.pt\").model\n",
    "        # self.backbone = torch.nn.Sequential(*list(self.model.model.children())[:7])  # Stops after C3k2 (layer 6)\n",
    "        self.feature_model = torch.nn.Sequential(*list(self.model.model.children())[:10])  # Stops after SPPF (layer 9)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.feature_model(x)\n",
    "\n",
    "# model = YOLOv11().to(device)\n",
    "# features = model(images) \n",
    "# features.shape # torch.Size([B, 512, 20, 20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00b0641a",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIP_PROCESSOR = CLIPImageProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "class CLIP(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.clip_model = CLIPVisionModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        # CLIP's final hidden state before projection (not the projection itself)\n",
    "        self.clip_output_dim = self.clip_model.config.hidden_size \n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        outputs = self.clip_model(**x)\n",
    "        pooled_output = outputs.pooler_output  # shape: [batch_size, 512]\n",
    "        return pooled_output\n",
    "\n",
    "# img = CLIP_PROCESSOR(Image.open(\"image.jpg\"), return_tensors=\"pt\").to(device)\n",
    "# img['pixel_values'].shape # torch.Size([1, 3, 224, 224])\n",
    "# model = CLIP().to(device)\n",
    "# outputs = model(img) \n",
    "# outputs.shape # torch.Size([B, 768])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53b0595d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # self.resnet = models.resnet152(weights=models.ResNet152_Weights.DEFAULT)\n",
    "        self.resnet = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "        \n",
    "        self.feature_extractor = nn.Sequential(*list(self.resnet.children())[:-3])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        features = self.feature_extractor(x) # [B, 1024, H/16, W/16]\n",
    "        return features\n",
    "    \n",
    "# model = ResNet152().to(device)\n",
    "# outputs = model(torch.randn(16, 3, 224, 224).to(device)) \n",
    "# outputs.shape # torch.Size([B, 1024, 14, 14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c31e31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiggerClassifier(torch.nn.Module):\n",
    "    def __init__(self, output_dim=3):\n",
    "        super().__init__()\n",
    "        self.clip = CLIP() # CLIP outputs: [B, 768]\n",
    "        self.yolo = YOLOv11() # YOLO outputs: [B, 512, 20, 20]\n",
    "        self.resnet = ResNet() # ResNet outputs: [B, 1024, H/16, W/16]\n",
    "\n",
    "        # Global average pooling for feature maps\n",
    "        self.yolo_pool = torch.nn.AdaptiveAvgPool2d((1, 1))\n",
    "        # self.resnet_pool = torch.nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        # self.fc1 = torch.nn.Linear(768 + 512 + 1024, 2048)\n",
    "        self.fc1 = torch.nn.Linear(768 + 512, 2048)\n",
    "        self.activation1 = torch.nn.GELU()\n",
    "        self.dropout1 = torch.nn.Dropout(0.3)\n",
    "        self.fc2 = torch.nn.Linear(2048, 1024)\n",
    "        self.activation2 = torch.nn.GELU()\n",
    "        self.dropout2 = torch.nn.Dropout(0.3)\n",
    "        self.fc3 = torch.nn.Linear(1024, output_dim)\n",
    "        \n",
    "    def forward(self, clip_inputs, img_tensor):\n",
    "        clip_features = self.clip(clip_inputs)  # [B, 768]\n",
    "        yolo_features = self.yolo(img_tensor)  # [B, 512, 20, 20]\n",
    "        # resnet_features = self.resnet(img_tensor) # [B, 1024, _, _]\n",
    "\n",
    "        # Pool YOLO features to [B, 512, 1, 1] then to [B, 512]\n",
    "        yolo_features = self.yolo_pool(yolo_features).flatten(1)\n",
    "        # resnet_features = self.resnet_pool(resnet_features).flatten(1)\n",
    "\n",
    "        # combined_features = torch.cat([clip_features, yolo_features, resnet_features], dim=1)  # [B, 2304]\n",
    "        combined_features = torch.cat([clip_features, yolo_features], dim=1)\n",
    "        \n",
    "        x = self.fc1(combined_features)\n",
    "        x = self.activation1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.activation2(x)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387fdf8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cfbe9a77",
   "metadata": {},
   "source": [
    "# DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7afbcd",
   "metadata": {},
   "source": [
    "### transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "927a8895",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_tensor = transforms.Compose([\n",
    "    transforms.ToImage(), \n",
    "    transforms.ToDtype(torch.float32, scale=True),\n",
    "])\n",
    "\n",
    "yolo_intermediate_input_size = 700\n",
    "yolo_final_input_size = 640\n",
    "\n",
    "base_transform = transforms.Compose([\n",
    "    to_tensor,\n",
    "    transforms.Resize(size=yolo_intermediate_input_size, interpolation=transforms.InterpolationMode.BILINEAR, antialias=True), # Resize maintaining aspect ratio, then pad to square\n",
    "    transforms.RandomRotation(degrees=(-15, 15), interpolation=transforms.InterpolationMode.BILINEAR, expand=True, fill=0),\n",
    "    transforms.RandomCrop(yolo_final_input_size),\n",
    "    transforms.RandomHorizontalFlip(p=0.5), # random flip\n",
    "])\n",
    "\n",
    "\n",
    "yolo_weak_transform = transforms.Compose([    \n",
    "    transforms.ColorJitter(brightness=0.05, contrast=0.05, saturation=0.05, hue=0.02),\n",
    "    transforms.Lambda(lambda x: x + torch.randn_like(x) * 0.005), transforms.Lambda(lambda x: torch.clamp(x, 0, 1)),\n",
    "])\n",
    "yolo_strong_transform = transforms.Compose([    \n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),\n",
    "    transforms.Lambda(lambda x: x + torch.randn_like(x) * 0.01), transforms.Lambda(lambda x: torch.clamp(x, 0, 1)),\n",
    "])\n",
    "\n",
    "clip_base_transform = transforms.Compose([ \n",
    "    transforms.Resize(size=224, interpolation=transforms.InterpolationMode.BILINEAR, antialias=True),\n",
    "])\n",
    "clip_weak_transform = transforms.Compose([    \n",
    "    transforms.ColorJitter(brightness=0.05, contrast=0.05, saturation=0.05, hue=0.02),\n",
    "    transforms.Lambda(lambda x: x + torch.randn_like(x) * 0.005), transforms.Lambda(lambda x: torch.clamp(x, 0, 1)),\n",
    "])\n",
    "clip_strong_transform = transforms.Compose([    \n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),\n",
    "    transforms.Lambda(lambda x: x + torch.randn_like(x) * 0.005), transforms.Lambda(lambda x: torch.clamp(x, 0, 1)),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "769389db",
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_val_transform = transforms.Compose([\n",
    "    to_tensor,\n",
    "    transforms.Resize(size=yolo_intermediate_input_size, interpolation=transforms.InterpolationMode.BILINEAR, antialias=True), # Resize maintaining aspect ratio, then pad to square\n",
    "    transforms.CenterCrop(yolo_final_input_size), \n",
    "    transforms.Lambda(lambda x: torch.clamp(x, 0, 1)),\n",
    "])\n",
    "\n",
    "clip_val_transform = transforms.Compose([ \n",
    "    to_tensor,\n",
    "    transforms.Resize(size=256, interpolation=transforms.InterpolationMode.BILINEAR, antialias=True),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.Lambda(lambda x: torch.clamp(x, 0, 1)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e42e48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8bbc79ed",
   "metadata": {},
   "source": [
    "### datasets/loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "851d961b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_images = []\n",
    "\n",
    "class MeanTeacherDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, val=False, supervised=False, supervised_ratio=0.5, upsample=None):\n",
    "        self.root_dir = os.path.expanduser(root_dir) # root_dir\n",
    "        self.annotations = pd.read_csv(os.path.join(self.root_dir, csv_file))\n",
    "        if dry_run:\n",
    "            self.annotations = self.annotations.sample(n=min(dry_run, len(self.annotations)), random_state=42)\n",
    "            self.annotations = self.annotations.reset_index(drop=True)  # Reset index after sampling\n",
    "        if supervised: # only provide supervised data\n",
    "            # only labeled data:\n",
    "            # self.annotations = self.annotations[self.annotations['label'] != 1].reset_index(drop=True)\n",
    "\n",
    "            labeled = self.annotations[self.annotations['label'].isin([0, 2])]\n",
    "            unlabeled = self.annotations[self.annotations['label'] == 1]\n",
    "            n_labeled = len(labeled) \n",
    "            total_len = int(n_labeled / supervised_ratio)\n",
    "            target_len = total_len - n_labeled\n",
    "            # target_len = int(n_labeled * 0.5)\n",
    "            if len(unlabeled) > target_len:\n",
    "                unlabeled = unlabeled.sample(n=target_len) #, random_state=42)\n",
    "            else: \n",
    "                print(f\"not enough unlabeled. asked for {target_len}, only have {len(unlabeled)}\")\n",
    "            self.annotations = pd.concat([labeled, unlabeled], ignore_index=True)\n",
    "        if upsample: # upsample supervised labels. Either an int, or default as None\n",
    "            labeled = self.annotations[self.annotations['label'].isin([0, 2])].copy()\n",
    "            unlabeled = self.annotations[self.annotations['label'] == 1].copy()\n",
    "            n_labeled = len(labeled)\n",
    "            n_unlabeled = len(unlabeled)\n",
    "            print(f\"Upsampling labeled data: {n_labeled} samples Ã— {upsample} = {n_labeled * upsample}\")\n",
    "            print(f\"Unlabeled data: {n_unlabeled} samples\")\n",
    "            # Create upsampled copies with unique IDs\n",
    "            labeled_copies = []\n",
    "            for i in range(upsample):\n",
    "                labeled_copy = labeled.copy()\n",
    "                # Append suffix to unique_id for each copy\n",
    "                labeled_copy['unique_id'] = labeled_copy['unique_id'].astype(str) + f'_copy{i}'\n",
    "                labeled_copies.append(labeled_copy)\n",
    "            labeled_upsampled = pd.concat(labeled_copies, ignore_index=True)\n",
    "            self.annotations = pd.concat([labeled_upsampled, unlabeled], ignore_index=True)\n",
    "            self.annotations = self.annotations.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "            print(f\"Final dataset size: {len(self.annotations)} samples\")\n",
    "            print(f\"Labeled ratio: {len(labeled_upsampled) / len(self.annotations):.2%}\")\n",
    "        # csv headers: relative_path,label,width,height,size_kb,source\n",
    "        self.transform_times = []\n",
    "        self.val = val\n",
    "\n",
    "        self.error_log_path = f'dataset_errors_{\"val\" if val else \"train\"}.log'\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "    \n",
    "    def get_id(self, idx):\n",
    "        return self.annotations.at[idx, 'unique_id']\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        try:\n",
    "            # 1. process metadata\n",
    "\n",
    "            original_label = self.annotations.iloc[idx, 1]\n",
    "            label = torch.tensor(original_label, dtype=torch.long) # TODO: check this +0 offset should b right? \n",
    "            \n",
    "            img_path = os.path.join(self.root_dir, self.annotations.iloc[idx, 0])\n",
    "            img_path = img_path.replace('\\\\', '/')\n",
    "            image_pil = Image.open(img_path).convert('RGB')\n",
    "\n",
    "            metadata = {\n",
    "                'img_path': img_path,\n",
    "                'label': original_label,\n",
    "                'width': self.annotations.iloc[idx, 2],\n",
    "                'height': self.annotations.iloc[idx, 3],\n",
    "                'size_kb': self.annotations.iloc[idx, 4],\n",
    "                'source': str(self.annotations.iloc[idx, 5]),\n",
    "            }\n",
    "\n",
    "            # 2. process images \n",
    "\n",
    "            if not self.val: # for training loop\n",
    "                item_transform_start = time.time()\n",
    "                \n",
    "                base_image = base_transform(image_pil)\n",
    "                \n",
    "                # YOLO branch\n",
    "                # yolo_base_image = base_transform(base_image)\n",
    "                yolo_weak_image = yolo_weak_transform(base_image)\n",
    "                yolo_strong_image = yolo_strong_transform(base_image)\n",
    "                \n",
    "                # CLIP branch\n",
    "                clip_base_image = clip_base_transform(base_image)\n",
    "                clip_weak_image = clip_weak_transform(clip_base_image)\n",
    "                clip_strong_image = clip_strong_transform(clip_base_image)\n",
    "\n",
    "                clip_weak_image = CLIP_PROCESSOR(images=clip_weak_image, return_tensors=\"pt\", do_rescale=False)\n",
    "                clip_weak_image['pixel_values'] = clip_weak_image['pixel_values'].squeeze(0)  # [1, 3, 224, 224] -> [3, 224, 224]\n",
    "                clip_strong_image = CLIP_PROCESSOR(images=clip_strong_image, return_tensors=\"pt\", do_rescale=False)\n",
    "                clip_strong_image['pixel_values'] = clip_strong_image['pixel_values'].squeeze(0)  # [1, 3, 224, 224] -> [3, 224, 224]\n",
    "\n",
    "                item_transform_time = time.time() - item_transform_start\n",
    "                if len(self.transform_times) < 1000:\n",
    "                    self.transform_times.append(item_transform_time)\n",
    "\n",
    "                # clip_weak_image['pixel_values'] = clip_weak_image['pixel_values'].to(device, non_blocking=True)\n",
    "                # clip_strong_image['pixel_values'] = clip_strong_image['pixel_values'].to(device, non_blocking=True)\n",
    "                # yolo_weak_image = yolo_weak_image.to(device, non_blocking=True)\n",
    "                # yolo_strong_image = yolo_strong_image.to(device, non_blocking=True)\n",
    "                # label = label.to(device, non_blocking=True)\n",
    "\n",
    "                return clip_weak_image, clip_strong_image, yolo_weak_image, yolo_strong_image, label, metadata\n",
    "\n",
    "            else: # for validation\n",
    "                yolo_image = yolo_val_transform(image_pil)\n",
    "\n",
    "                clip_image = clip_val_transform(image_pil)\n",
    "                clip_image = CLIP_PROCESSOR(images=clip_image, return_tensors=\"pt\", do_rescale=False)\n",
    "                clip_image['pixel_values'] = clip_image['pixel_values'].squeeze(0)\n",
    "                return clip_image, clip_image, yolo_image, yolo_image, label, metadata\n",
    "            \n",
    "        except Exception as e:\n",
    "            \n",
    "            with open(os.path.join(logger.get_log_dir(), self.error_log_path), 'a') as f:\n",
    "                f.write(f\"Error loading image at index {idx}: {e}\\n\\n\")\n",
    "\n",
    "            # Create dummy CLIP inputs (matching CLIP_PROCESSOR output format)\n",
    "            dummy_clip_input = {'pixel_values': torch.zeros(3, 224, 224)}\n",
    "            dummy_yolo_tensor = torch.zeros(3, 640, 640)\n",
    "            dummy_label = torch.tensor(1, dtype=torch.long)\n",
    "            dummy_metadata = {\n",
    "                'img_path': f'error_at_idx_{idx}',\n",
    "                'label': 1,\n",
    "                'width': 640,\n",
    "                'height': 640,\n",
    "                'size_kb': 0.0,\n",
    "                'source': -1,\n",
    "            }\n",
    "            \n",
    "            return dummy_clip_input, dummy_clip_input, dummy_yolo_tensor, dummy_yolo_tensor, dummy_label, dummy_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32c591b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomVersionSampler(Sampler):\n",
    "    def __init__(self, base_dataset):\n",
    "        self.base_dataset = base_dataset\n",
    "        \n",
    "        self.grouped = {} # Group indices by ID\n",
    "        for idx in range(len(base_dataset)):\n",
    "            img_id = base_dataset.get_id(idx)\n",
    "            if img_id not in self.grouped:\n",
    "                self.grouped[img_id] = []\n",
    "            self.grouped[img_id].append(idx)\n",
    "        self.ids = list(self.grouped.keys())\n",
    "\n",
    "    def __iter__(self):\n",
    "        # For each ID, pick a random index\n",
    "        chosen_indices = [random.choice(self.grouped[img_id]) for img_id in self.ids]\n",
    "        # Shuffle the chosen indices for batching\n",
    "        random.shuffle(chosen_indices)\n",
    "        return iter(chosen_indices)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbb2b68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d06b267b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data._utils.collate import default_collate\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    # Collate everything except metadata normally\n",
    "    clip_weak = default_collate([item[0] for item in batch])\n",
    "    clip_strong = default_collate([item[1] for item in batch])\n",
    "    yolo_weak = default_collate([item[2] for item in batch])\n",
    "    yolo_strong = default_collate([item[3] for item in batch])\n",
    "    labels = default_collate([item[4] for item in batch])\n",
    "    \n",
    "    # Keep metadata as a list of dicts (no tensor conversion)\n",
    "    metadata_list = [item[5] for item in batch]\n",
    "    \n",
    "    # Restructure to dict of lists for easier access\n",
    "    metadata = {\n",
    "        'img_path': [m['img_path'] for m in metadata_list],\n",
    "        'label': [m['label'] for m in metadata_list],\n",
    "        'width': [m['width'] for m in metadata_list],\n",
    "        'height': [m['height'] for m in metadata_list],\n",
    "        'size_kb': [m['size_kb'] for m in metadata_list],\n",
    "        'source': [m['source'] for m in metadata_list],\n",
    "    }\n",
    "    \n",
    "    return clip_weak, clip_strong, yolo_weak, yolo_strong, labels, metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb37da2",
   "metadata": {},
   "source": [
    "##### The actual datasets and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c239a75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "487327"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "supervised_train_dataset = MeanTeacherDataset(\n",
    "    csv_file = \"train_2.csv\", \n",
    "    root_dir = \"~/Workspace/data-v2/train\",\n",
    "    supervised = True,\n",
    ")\n",
    "train_dataset = MeanTeacherDataset(\n",
    "    csv_file = \"train_2.csv\", \n",
    "    root_dir = \"~/Workspace/data-v2/train\",\n",
    "    val = False,\n",
    ")\n",
    "val_dataset = MeanTeacherDataset(\n",
    "    csv_file = \"val_2.csv\", \n",
    "    root_dir = \"~/Workspace/data-v2/val\",\n",
    "    val = True,\n",
    ")\n",
    "\n",
    "supervised_sampler = RandomVersionSampler(supervised_train_dataset)\n",
    "supervised_train_dataloader = DataLoader(\n",
    "    supervised_train_dataset, \n",
    "    batch_size=config.batch_size, \n",
    "    sampler=supervised_sampler,\n",
    "    collate_fn=custom_collate_fn,\n",
    "    num_workers=get_num_workers(),\n",
    "    persistent_workers=False, # True if get_num_workers() > 0 else False,\n",
    "    # pin_memory=False, # WSL does not support pin_memory well\n",
    "    prefetch_factor=2 if get_num_workers() > 0 else None,\n",
    ")\n",
    "\n",
    "sampler = RandomVersionSampler(train_dataset)\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=config.batch_size, \n",
    "    sampler=sampler,\n",
    "    collate_fn=custom_collate_fn,\n",
    "    num_workers=get_num_workers(),\n",
    "    persistent_workers=False, # True if get_num_workers() > 0 else False,\n",
    "    # pin_memory=False, # WSL does not support pin_memory well\n",
    "    prefetch_factor=2 if get_num_workers() > 0 else None,\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=config.batch_size, \n",
    "    collate_fn=custom_collate_fn,\n",
    "    num_workers=get_num_workers(),\n",
    "    persistent_workers=False, # True if get_num_workers() > 0 else False,\n",
    "    # pin_memory=False, # WSL does not support pin_memory well\n",
    "    prefetch_factor=3 if get_num_workers() > 0 else None,\n",
    ")\n",
    "\n",
    "len(train_dataset) - len(train_dataloader) * config.batch_size # > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e84e14d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18472, 28399)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(supervised_train_dataloader), len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb393b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd17ea35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_and_sanity_check(base_dataset, num_batches = 50):\n",
    "    total_samples = 0\n",
    "    print(f\"Benchmarking dataloader for {num_batches} batches...\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    for i, (student_clip_inputs, teacher_clip_inputs, yolo_weak_tensors, yolo_strong_tensors, labels, _) in enumerate(base_dataset):\n",
    "        clip_tensors = teacher_clip_inputs['pixel_values']\n",
    "        yolo_tensors = yolo_strong_tensors\n",
    "        if i >= num_batches:\n",
    "            break\n",
    "        total_samples += len(labels)\n",
    "    total_time = time.time() - start_time\n",
    "\n",
    "    avg_time_per_batch = total_time / num_batches\n",
    "    avg_time_per_sample = total_time / total_samples\n",
    "    samples_per_second = total_samples / total_time\n",
    "\n",
    "    print(f\"Total time: {total_time:.2f} seconds.\", f\"Total samples: {total_samples}\")\n",
    "    print(f\"Average time per batch: {avg_time_per_batch:.4f} seconds.\", f\"Average time per sample: {avg_time_per_sample:.4f} seconds\")\n",
    "    print(f\"Throughput: {samples_per_second:.2f} samples/second\")\n",
    "\n",
    "    # sanity check dataset\n",
    "    clip, _, yolo, _, _, _= base_dataset[0]\n",
    "    first_img = yolo.permute(1, 2, 0).cpu().numpy()  # [640, 640, 3]\n",
    "    first_img = (first_img * 255).astype('uint8')\n",
    "    pil_img = Image.fromarray(first_img)\n",
    "    plt.imshow(pil_img)\n",
    "    plt.show()\n",
    "\n",
    "    # sanity check clip img\n",
    "    first_img = clip_tensors[0]\n",
    "    mean = torch.tensor([0.48145466, 0.4578275, 0.40821073]).view(3, 1, 1)\n",
    "    std = torch.tensor([0.26862954, 0.26130258, 0.27577711]).view(3, 1, 1)\n",
    "    first_img = first_img * std + mean\n",
    "    first_img = first_img.permute(1, 2, 0).cpu().numpy()  # [224, 224, 3]\n",
    "    first_img = (first_img * 255).astype('uint8')\n",
    "    pil_img = Image.fromarray(first_img)\n",
    "    plt.imshow(pil_img)\n",
    "    plt.show()\n",
    "\n",
    "    # sanity check yolo img\n",
    "    first_img = yolo_tensors[0]\n",
    "    first_img = first_img.permute(1, 2, 0).cpu().numpy()  # [640, 640, 3]\n",
    "    first_img = (first_img * 255).astype('uint8')\n",
    "    pil_img = Image.fromarray(first_img)\n",
    "    plt.imshow(pil_img)\n",
    "    plt.show()\n",
    "\n",
    "    print(\"mean transform compute time:\", np.mean(base_dataset.transform_times))\n",
    "\n",
    "# benchmark_and_sanity_check(train_dataset, 10)\n",
    "# benchmark_and_sanity_check(val_dataset, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e929f863",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f2b9450",
   "metadata": {},
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f8a42476",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BiggerClassifier().to(config.device)\n",
    "teacher_model = copy.deepcopy(model).to(config.device)\n",
    "# teacher_model.eval()\n",
    "teacher_model.train()\n",
    "for p in teacher_model.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b39328e",
   "metadata": {},
   "source": [
    "### learn rate, scheduler, optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "98ee4a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing backbone parameters initially...\n",
      "scheduler lr: [1.0000000000000002e-06, 5e-06, 1e-05, 5e-05]\n"
     ]
    }
   ],
   "source": [
    "clip_params = []\n",
    "yolo_params = []\n",
    "resnet_params = []\n",
    "classifier_params = []\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if 'clip' in name:\n",
    "        clip_params.append(param)\n",
    "    elif 'yolo' in name:\n",
    "        yolo_params.append(param)\n",
    "    elif 'resnet' in name:\n",
    "        resnet_params.append(param)\n",
    "        # i can separate out earlier layers if i want to\n",
    "    else:\n",
    "        classifier_params.append(param)\n",
    "\n",
    "print(\"Freezing backbone parameters initially...\")\n",
    "for param in clip_params + yolo_params: # + resnet_params:\n",
    "    param.requires_grad = False\n",
    "\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {'params': clip_params, 'lr': config.lr_backbone*0.1, 'name': 'clip'},\n",
    "    {'params': yolo_params, 'lr': config.lr_backbone*0.5, 'name': 'yolo'},\n",
    "    {'params': resnet_params, 'lr': config.lr_backbone, 'name': 'resnet'},\n",
    "    {'params': classifier_params, 'lr': config.initial_lr, 'name': 'classifier'}\n",
    "])\n",
    "\n",
    "# scheduler =  torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config.num_epochs, eta_min=1e-6)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.7, patience=1)\n",
    "print('scheduler lr:', scheduler.get_last_lr())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cb60fa5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn as nn\n",
    "\n",
    "# for name, module in model.named_modules():\n",
    "#     if isinstance(module, (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d)):\n",
    "#         print(f\"Found BatchNorm: {name} ({type(module).__name__})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c4f75e",
   "metadata": {},
   "source": [
    "### loss and criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "711d304d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AsymmetricFocalLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Asymmetric Focal Loss variant that combines focal loss with asymmetric penalties.\n",
    "    Useful when you also want to handle class imbalance.\n",
    "    \"\"\"\n",
    "    def __init__(self, gamma=2.0, alpha=None, confusion_penalty_matrix=None):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        if confusion_penalty_matrix is None:\n",
    "            confusion_penalty_matrix = torch.tensor([\n",
    "                [1.0, 1.0, 1.0],   # True: BAD \n",
    "                [1.0, 1.0, 1.0],   # True: NEUTRAL\n",
    "                [1.0, 1.0, 1.0]    # True: GOOD\n",
    "            ])\n",
    "        self.confusion_penalty_matrix = confusion_penalty_matrix\n",
    "        \n",
    "    def forward(self, logits, targets):\n",
    "        ce_loss = F.cross_entropy(logits, targets, reduction='none')\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        p_t = probs.gather(1, targets.view(-1, 1)).squeeze(1)\n",
    "        \n",
    "        # print(f\"p_t min: {p_t.min():.6f}, max: {p_t.max():.6f}\")\n",
    "        # print(f\"ce_loss min: {ce_loss.min():.6f}, max: {ce_loss.max():.6f}\")\n",
    "        \n",
    "        # Focal term\n",
    "        focal_weight = (1 - p_t) ** self.gamma\n",
    "\n",
    "        # === Expected penalty ===\n",
    "        # penalty_matrix: [C, C] where penalty_matrix[true, pred] gives penalty\n",
    "        # penalty_for_true: [B, C] rows correspond to each sample's true class\n",
    "        penalty_for_true = self.confusion_penalty_matrix[targets]  # shape [B, num_classes]\n",
    "\n",
    "        # Expected penalty under predicted distribution\n",
    "        expected_penalty = (probs * penalty_for_true).sum(dim=1)  # shape [B]\n",
    "\n",
    "        # Combine focal weight with expected penalties\n",
    "        loss = focal_weight * ce_loss * expected_penalty\n",
    "\n",
    "        # Optional alpha weighting\n",
    "        if self.alpha is not None:\n",
    "            alpha_t = self.alpha.gather(0, targets)\n",
    "            loss = alpha_t * loss\n",
    "        \n",
    "        # # Get predicted classes for confusion penalties\n",
    "        # pred_classes = torch.argmax(logits, dim=1)\n",
    "        \n",
    "        # # Apply confusion-based penalties\n",
    "        # batch_size = targets.size(0)\n",
    "        # penalties = torch.zeros(batch_size, device=targets.device)\n",
    "        \n",
    "        # # for i in range(batch_size):\n",
    "        # #     true_class = targets[i].item()\n",
    "        # #     pred_class = pred_classes[i].item()\n",
    "        # #     penalties[i] = self.confusion_penalty_matrix[true_class, pred_class]\n",
    "        # penalties = self.confusion_penalty_matrix[targets, pred_classes]\n",
    "        \n",
    "        # # Combine focal weight with confusion penalties\n",
    "        # loss = focal_weight * ce_loss * penalties\n",
    "        \n",
    "        # if self.alpha is not None:\n",
    "        #     alpha_t = self.alpha.gather(0, targets)\n",
    "        #     loss = alpha_t * loss\n",
    "            \n",
    "        return loss.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d9e9dd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ce_criterion = nn.CrossEntropyLoss().to(config.device)\n",
    "\n",
    "af_criterion = AsymmetricFocalLoss(\n",
    "    gamma=1.2,\n",
    "    alpha=torch.tensor([1.1, 0.9, 1.2]).to(config.device),  \n",
    "    confusion_penalty_matrix=torch.tensor([\n",
    "        [1.0, 1.05, 1.15], \n",
    "        [0.88, 1.0, 0.90],\n",
    "        [1.1, 1.05, 1.0]\n",
    "    ]).to(config.device)\n",
    ").to(config.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fc80c7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i=0\n",
    "# logits = torch.tensor([[10, 1, 1]]).to(device) * 1.5  # 5 samples, 3 classes\n",
    "# # logits = torch.randn(5,3).to(device) * 1.5  # 5 samples, 3 classes\n",
    "# targets = torch.tensor([0] * 10).to(device)\n",
    "\n",
    "# # Forward pass\n",
    "# probs = F.softmax(logits[i].unsqueeze(0), dim=1)\n",
    "# loss = af_criterion(logits[i].unsqueeze(0), targets[i].unsqueeze(0))\n",
    "# ce_loss = ce_criterion(logits[i].unsqueeze(0), targets[i].unsqueeze(0))\n",
    "\n",
    "# print(probs, targets[i].unsqueeze(0).item())\n",
    "# print(loss.item())\n",
    "# print(ce_loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf376aa",
   "metadata": {},
   "source": [
    "### util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b75f72d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler = GradScaler() \n",
    "monitor = PerformanceMonitor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dbe3d884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# At the end of each epoch, after validation:\\nif val_accuracy > monitor.best_accuracy:\\n    save_best_checkpoint(\\n        model, teacher_model, optimizer, scheduler,\\n        epoch, global_step, config, val_accuracy,\\n        monitor, loss_history, cls_loss_history\\n    )\\n\\n# To resume training:\\ncheckpoint_path = \"./checkpoints/your_run_name/checkpoint_epoch10_step50000.pth\"\\nepoch, global_step, loss_history, cls_loss_history, val_accuracy = load_checkpoint(\\n    checkpoint_path, model, teacher_model, optimizer, scheduler,\\n    config, monitor, device=config.device\\n)\\n\\n# Then continue training loop from config.cur_epoch\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def save_checkpoint(model, teacher_model, optimizer, scheduler, epoch, global_step, config, \n",
    "                    val_accuracy, monitor, loss_history, cls_loss_history):\n",
    "\n",
    "    checkpoint_dir = config.checkpoint_path\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    filename = f\"checkpoint_epoch{epoch}_step{global_step}_acc{val_accuracy:.4f}_{timestamp}.pth\"\n",
    "    \n",
    "    checkpoint_path = os.path.join(checkpoint_dir, filename)\n",
    "    \n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'global_step': global_step,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'teacher_state_dict': teacher_model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'val_accuracy': val_accuracy,\n",
    "        'loss_history': loss_history,\n",
    "        'cls_loss_history': cls_loss_history,\n",
    "        'monitor_state': {\n",
    "            'best_accuracy': monitor.best_accuracy,\n",
    "            'epochs_without_improvement': monitor.epochs_without_improvement,\n",
    "            'accuracy_history': monitor.accuracy_history,\n",
    "        },\n",
    "        'config': {k: v for k, v in vars(config).items() if not k.startswith('_')},\n",
    "        \n",
    "        'save_timestamp': datetime.now().isoformat(),\n",
    "        'pytorch_version': torch.__version__,\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        torch.save(checkpoint, checkpoint_path)\n",
    "        print(f\"Checkpoint saved successfully: {checkpoint_path}\")\n",
    "        print(f\"Epoch: {epoch} | Step: {global_step} | Val Acc: {val_accuracy:.4f}\")\n",
    "        return checkpoint_path\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving checkpoint: {e}\")\n",
    "        raise\n",
    "\n",
    "def load_checkpoint(checkpoint_path, model, teacher_model, optimizer, scheduler, config, \n",
    "                    monitor, device='cuda', strict=True):\n",
    "\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        raise FileNotFoundError(f\"Checkpoint not found: {checkpoint_path}\")\n",
    "    print(f\"Loading checkpoint from: {checkpoint_path}\")\n",
    "    \n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    \n",
    "    model.load_state_dict(checkpoint['model_state_dict'], strict=strict)\n",
    "    teacher_model.load_state_dict(checkpoint['teacher_state_dict'], strict=strict)\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    \n",
    "    # Restore training state\n",
    "    epoch = checkpoint['epoch']\n",
    "    global_step = checkpoint['global_step']\n",
    "    val_accuracy = checkpoint['val_accuracy']\n",
    "    loss_history = checkpoint['loss_history']\n",
    "    cls_loss_history = checkpoint['cls_loss_history']\n",
    "    \n",
    "    # Restore performance monitor\n",
    "    monitor_state = checkpoint['monitor_state']\n",
    "    monitor.best_accuracy = monitor_state['best_accuracy']\n",
    "    monitor.accuracy_history = monitor_state['accuracy_history']\n",
    "    \n",
    "    # Update config with saved values (optional - be careful with paths)\n",
    "    saved_config = checkpoint['config']\n",
    "    for key, value in saved_config.items():\n",
    "        if hasattr(config, key) and key not in ['checkpoint_path', 'log_dir']:\n",
    "            setattr(config, key, value)\n",
    "    \n",
    "    config.cur_epoch = epoch + 1\n",
    "    \n",
    "    print(f\"Checkpoint loaded successfully\")\n",
    "    print(f\"Resuming from Epoch: {epoch + 1} | Step: {global_step}\")\n",
    "    print(f\"Previous Val Acc: {val_accuracy:.4f} | Best Acc: {monitor.best_accuracy:.4f}\")\n",
    "    print(f\"Loaded len(loss_history) = {len(loss_history)}\")\n",
    "    \n",
    "    return epoch, global_step, loss_history, cls_loss_history, val_accuracy\n",
    "\n",
    "# Example usage in your training loop:\n",
    "\"\"\"\n",
    "# At the end of each epoch, after validation:\n",
    "if val_accuracy > monitor.best_accuracy:\n",
    "    save_best_checkpoint(\n",
    "        model, teacher_model, optimizer, scheduler,\n",
    "        epoch, global_step, config, val_accuracy,\n",
    "        monitor, loss_history, cls_loss_history\n",
    "    )\n",
    "\n",
    "# To resume training:\n",
    "checkpoint_path = \"./checkpoints/your_run_name/checkpoint_epoch10_step50000.pth\"\n",
    "epoch, global_step, loss_history, cls_loss_history, val_accuracy = load_checkpoint(\n",
    "    checkpoint_path, model, teacher_model, optimizer, scheduler,\n",
    "    config, monitor, device=config.device\n",
    ")\n",
    "\n",
    "# Then continue training loop from config.cur_epoch\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b9d7a764",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, val_loader, config, epoch):\n",
    "    torch.cuda.empty_cache()\n",
    "    model.eval()\n",
    "\n",
    "    # cumulative lists for dataframes\n",
    "    image_data = []\n",
    "    batch_data = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_pbar = tqdm(val_loader, desc=f'Validation Epoch {epoch}', total=len(val_loader), leave=False)\n",
    "\n",
    "        for batch_num, batch_input in enumerate(val_pbar):\n",
    "            clip_inputs, _, yolo_tensors, _, labels, metadata = batch_input\n",
    "            clip_inputs['pixel_values'] = clip_inputs['pixel_values'].to(config.device)\n",
    "            yolo_tensors = yolo_tensors.to(config.device)\n",
    "            labels = labels.to(config.device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(clip_inputs, yolo_tensors)\n",
    "            loss = af_criterion(outputs, labels).cpu().numpy()\n",
    "            probs = F.softmax(outputs, dim=1).cpu().numpy()\n",
    "            predictions = outputs.argmax(dim=1).cpu().numpy()\n",
    "            labels = labels.cpu().numpy()\n",
    "\n",
    "            # Add batch-level data\n",
    "            batch_data.append({\n",
    "                'batch_num': batch_num,\n",
    "                'epoch': epoch,\n",
    "                'loss': loss.item()\n",
    "            })\n",
    "\n",
    "            # Add image-level data\n",
    "            for i in range(len(labels)):\n",
    "                true_label = labels[i]\n",
    "                pred_label = predictions[i]\n",
    "\n",
    "                # Check for severe misclassifications\n",
    "                # if true_label == 2 and pred_label == 0:  # True: GOOD, Pred: BAD\n",
    "                    # good_predicted_as_bad += 1\n",
    "                    # print(f\"[WARNING] Predicted BAD on true GOOD: {metadata['img_path'][i]}\")\n",
    "                # elif true_label == 0 and pred_label == 2:  # True: BAD, Pred: GOOD\n",
    "                    # bad_predicted_as_good += 1\n",
    "                    # print(f\"[WARNING] Predicted GOOD on true BAD: {metadata['img_path'][i]}\")\n",
    "\n",
    "                image_data.append({\n",
    "                    'batch_num': batch_num,\n",
    "                    'img_path': metadata['img_path'][i],\n",
    "                    'label': int(true_label),\n",
    "                    'width': int(metadata['width'][i]),\n",
    "                    'height': int(metadata['height'][i]),\n",
    "                    'size_kb': float(metadata['size_kb'][i]),\n",
    "                    'source': metadata['source'][i],\n",
    "                    'prediction': int(pred_label),\n",
    "                    'bad': float(probs[i, 0]),\n",
    "                    'neutral': float(probs[i, 1]),\n",
    "                    'good': float(probs[i, 2])\n",
    "                })\n",
    "\n",
    "            val_pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    image_df = pd.DataFrame(image_data)\n",
    "    batch_df = pd.DataFrame(batch_data)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    return image_df, batch_df, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bb0bc72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse(image_df, batch_df, config, monitor, epoch):\n",
    "    # Calculate metrics from dataframe\n",
    "    labeled_df = image_df[image_df['label'] != 1]\n",
    "    total = len(labeled_df)\n",
    "    correct = (labeled_df['label'] == labeled_df['prediction']).sum()\n",
    "    accuracy = correct / total\n",
    "\n",
    "    # Calculate per-class metrics\n",
    "    class_names = ['bad', 'neutral', 'good']\n",
    "    labels=[0, 1, 2]\n",
    "    all_labels = image_df['label'].values\n",
    "    all_preds = image_df['prediction'].values\n",
    "\n",
    "    # Confusion matrix and classification report\n",
    "    cm = confusion_matrix(y_true=all_labels, y_pred=all_preds, labels=labels)\n",
    "    report = classification_report(y_true=all_labels, y_pred=all_preds, labels=labels, target_names=class_names,\n",
    "                                   output_dict=True, zero_division=0)\n",
    "\n",
    "    # Calculate per-class recall\n",
    "    class_recall = []\n",
    "    for class_idx in range(config.num_classes):\n",
    "        class_mask = image_df['label'] == class_idx\n",
    "        if class_mask.sum() > 0:\n",
    "            class_correct = ((image_df['label'] == class_idx) &\n",
    "                           (image_df['prediction'] == class_idx)).sum()\n",
    "            class_recall.append(class_correct / class_mask.sum())\n",
    "        else:\n",
    "            class_recall.append(0.0)\n",
    "\n",
    "    # Compile metrics dictionary\n",
    "    avg_val_loss = batch_df['loss'].mean()\n",
    "\n",
    "    metrics = {\n",
    "        \"val/accuracy\": accuracy,\n",
    "        \"val/loss\": avg_val_loss,\n",
    "        \"val/recall_bad\": class_recall[0],\n",
    "        \"val/recall_neutral\": class_recall[1],\n",
    "        \"val/recall_good\": class_recall[2],\n",
    "    }\n",
    "\n",
    "    # Add precision and F1 scores from classification report\n",
    "    for class_name in class_names:\n",
    "        if class_name in report:\n",
    "            metrics[f\"val/precision_{class_name}\"] = report[class_name]['precision']\n",
    "            metrics[f\"val/f1_{class_name}\"] = report[class_name]['f1-score']\n",
    "\n",
    "    # Add severe misclassification counts\n",
    "    good_as_bad = image_df[(image_df['label'] == 2) & (image_df['prediction'] == 0)]\n",
    "    bad_as_good = image_df[(image_df['label'] == 0) & (image_df['prediction'] == 2)]\n",
    "    metrics[\"val/bad_as_good_count\"] = len(bad_as_good)\n",
    "    metrics[\"val/good_as_bad_count\"] = len(good_as_bad)\n",
    "\n",
    "    # Update monitor\n",
    "    monitor.accuracy_history.append(accuracy)\n",
    "    if accuracy > monitor.best_accuracy:\n",
    "        monitor.best_accuracy = accuracy\n",
    "        monitor.epochs_without_improvement = 0\n",
    "    else:\n",
    "        monitor.epochs_without_improvement += 1\n",
    "\n",
    "    print(f\"Validation Summary - Epoch {epoch}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f} | Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    return accuracy, metrics, cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8abc6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dec08a21",
   "metadata": {},
   "source": [
    "##### train_one_epoch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "854b6cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f880af4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8197c6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_epoch(image_df, batch_df, config, epoch):\n",
    "\n",
    "    avg_loss = batch_df['loss'].mean()\n",
    "    avg_cls_loss = batch_df['cls_loss'].mean()\n",
    "    avg_consistency_loss = batch_df['consistency_loss'].mean()\n",
    "    avg_consistency_weight = batch_df['consistency_weight'].mean()\n",
    "    # total_time = batch_df['batch_time'].sum()\n",
    "    # avg_throughput = batch_df['images_per_second'].mean()\n",
    "    \n",
    "    metrics = {\n",
    "        \"epoch/loss\": avg_loss,\n",
    "        \"epoch/cls_loss\": avg_cls_loss,\n",
    "        \"epoch/consistency_loss\": avg_consistency_loss,\n",
    "        \"epoch/consistency_weight\": avg_consistency_weight,\n",
    "        # \"epoch/time_minutes\": total_time / 60,\n",
    "        # \"epoch/avg_throughput\": avg_throughput,\n",
    "    }\n",
    "    \n",
    "    if image_df is not None:\n",
    "        total_samples = len(image_df)\n",
    "        correct = (image_df['label'] == image_df['prediction']).sum()\n",
    "        train_accuracy = correct / total_samples\n",
    "        \n",
    "        # Per-class accuracy\n",
    "        for class_idx in range(config.num_classes):\n",
    "            class_mask = image_df['label'] == class_idx\n",
    "            if class_mask.sum() > 0:\n",
    "                class_correct = ((image_df['label'] == class_idx) & \n",
    "                               (image_df['prediction'] == class_idx)).sum()\n",
    "                class_acc = class_correct / class_mask.sum()\n",
    "                metrics[f\"epoch/accuracy_class_{class_idx}\"] = class_acc\n",
    "        \n",
    "        # Average consistency loss per image\n",
    "        metrics[\"epoch/avg_image_consistency_loss\"] = image_df['consistency_loss'].mean()\n",
    "        metrics[\"epoch/train_accuracy\"] = train_accuracy\n",
    "        \n",
    "        # Severe misclassifications\n",
    "        good_as_bad = image_df[(image_df['label'] == 2) & (image_df['prediction'] == 0)]\n",
    "        bad_as_good = image_df[(image_df['label'] == 0) & (image_df['prediction'] == 2)]\n",
    "        metrics[\"epoch/bad_as_good_count\"] = len(bad_as_good)\n",
    "        metrics[\"epoch/good_as_bad_count\"] = len(good_as_bad)\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c2d1cc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_running_loss(loss_history, save_path, window_size=10):\n",
    "    \"\"\"\n",
    "    Plot running loss with moving average and save to file\n",
    "    \n",
    "    Args:\n",
    "        loss_history: List of loss values\n",
    "        save_path: Path to save the plot\n",
    "        window_size: Window size for moving average\n",
    "    \"\"\"\n",
    "    if len(loss_history) < window_size:\n",
    "        return\n",
    "    \n",
    "    # Calculate moving average\n",
    "    moving_avg = []\n",
    "    for i in range(window_size - 1, len(loss_history)):\n",
    "        window = loss_history[i - window_size + 1:i + 1]\n",
    "        moving_avg.append(sum(window) / window_size)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "\n",
    "    # Plot moving average in bold\n",
    "    x_raw = [i * config.log_interval for i in range(len(loss_history))]\n",
    "    x_moving = [i * config.log_interval for i in range(window_size - 1, len(loss_history))]\n",
    "\n",
    "    # Plot raw loss in light color\n",
    "    plt.plot(x_raw, loss_history, alpha=0.3, color='blue', label='Raw Loss')\n",
    "    \n",
    "    # Plot moving average in bold\n",
    "    plt.plot(x_moving, moving_avg, color='red', linewidth=2, label=f'Moving Avg (window={window_size})')\n",
    "    plt.xlabel('Batch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss Over Time')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add statistics\n",
    "    if moving_avg:\n",
    "        current_avg = moving_avg[-1]\n",
    "        min_avg = min(moving_avg)\n",
    "        plt.axhline(y=current_avg, color='green', linestyle='--', alpha=0.5, label=f'Current: {current_avg:.4f}')\n",
    "        plt.axhline(y=min_avg, color='orange', linestyle='--', alpha=0.5, label=f'Min: {min_avg:.4f}')\n",
    "    \n",
    "    # if 'train_loader' in globals() and len(train_loader) > 0:\n",
    "    #     batches_per_epoch = len(train_loader)\n",
    "    #     num_epochs = len(loss_history) // batches_per_epoch\n",
    "    #     for epoch in range(1, num_epochs + 1):\n",
    "    #         epoch_batch = epoch * batches_per_epoch\n",
    "    #         if epoch_batch < len(loss_history):\n",
    "    #             plt.axvline(x=epoch_batch, color='gray', linestyle=':', alpha=0.5)\n",
    "    #             plt.text(epoch_batch, plt.ylim()[1] * 0.95, f'Epoch {epoch}', \n",
    "    #                     rotation=90, verticalalignment='top', fontsize=8, alpha=0.7)\n",
    "                \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=100)\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"  Loss graph saved to: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316365d9",
   "metadata": {},
   "source": [
    "# TRAINING LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "55eed306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint_path = \"checkpoints/resnet_clip_yolo_mean_teacher_20251021_012456/checkpoint_epoch0_step8000_acc0.4842_20251021_025438.pth\"\n",
    "\n",
    "# epoch, global_step, loss_history, cls_loss_history, val_accuracy = load_checkpoint(\n",
    "#     checkpoint_path, model, teacher_model, optimizer, scheduler,\n",
    "#     config, monitor, device=config.device\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0e31d882",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4055"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion = ce_criterion\n",
    "validation_frequency = len(train_dataloader) // 7 # validate every X batches\n",
    "validation_frequency = validation_frequency - (validation_frequency % config.log_interval)\n",
    "validation_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "72badbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ema_decay = 0.999\n",
    "# try warmup_steps = 60000 or more\n",
    "# low consistency weight = 0.2 NOT REALLY\n",
    "# teacher receives student buffers for batchnorms NOPE\n",
    "# try freezing backbone\n",
    "# lower consistency weight on 0 and 2\n",
    "# \"semi supervised\" start epoch\n",
    "\n",
    "# next - try high consistency weight - the magnitudes should be roughly equal ... not like 1:50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "49966e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = 0\n",
    "track_images = False\n",
    "loss_history = []\n",
    "cls_loss_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470f61e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I HAVE NO WAY TO REDUCE CLS LOSS BASED ON TEACHER MODEL. \n",
    "# IF ITS CERTAIN THE LABEL IS WRONG< I HAVENT DONE IT\n",
    "# I REALLY NEED TO DO THIS\n",
    "\n",
    "# DO AN ACTUAL RUN WITHOUT TEACHER ! just see what happens. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f5d3b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upsampling labeled data: 512053 samples Ã— 3 = 1536159\n",
      "Unlabeled data: 1111234 samples\n",
      "Final dataset size: 2647393 samples\n",
      "Labeled ratio: 58.03%\n",
      "0.66 42483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   1%|          | 500/42483 [04:59<6:02:07,  1.93it/s, loss=0.7852, cls=0.7849, cons=0.0003, img/s=91.3]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unfreezing backbone at step 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:  10%|â–‰         | 4055/42483 [44:42<7:14:13,  1.47it/s, loss=0.5731, cls=0.5723, cons=0.0008, img/s=67.3]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running intermittent validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Summary - Epoch step4055\n",
      "Accuracy: 0.3875 | Loss: 0.6555\n",
      "scheduler lr: [1.0000000000000002e-06, 5e-06, 1e-05, 5e-05]\n",
      "  Validation Accuracy: 0.3875\n",
      "  Best Accuracy: 0.3875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:  10%|â–‰         | 4056/42483 [48:45<785:07:56, 73.55s/it, loss=0.5731, cls=0.5723, cons=0.0008, img/s=67.3]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved successfully: ./checkpoints/resnet_clip_yolo_mean_teacher_20251029_112330/checkpoint_epoch0_step4055_acc0.3875_20251029_121230.pth\n",
      "Epoch: 0 | Step: 4055 | Val Acc: 0.3875\n",
      "  Loss graph saved to: ./logs/resnet_clip_yolo_mean_teacher_20251029_112330/loss_graph.png\n",
      "  Loss graph saved to: ./logs/resnet_clip_yolo_mean_teacher_20251029_112330/cls_loss_graph.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:  19%|â–ˆâ–‰        | 8110/42483 [1:33:53<6:22:24,  1.50it/s, loss=0.8665, cls=0.7257, cons=0.1407, img/s=68.3]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running intermittent validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Summary - Epoch step8110\n",
      "Accuracy: 0.3863 | Loss: 3.0379\n",
      "scheduler lr: [1.0000000000000002e-06, 5e-06, 1e-05, 5e-05]\n",
      "  Validation Accuracy: 0.3863\n",
      "  Best Accuracy: 0.3875\n",
      "  Loss graph saved to: ./logs/resnet_clip_yolo_mean_teacher_20251029_112330/loss_graph.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:  19%|â–ˆâ–‰        | 8111/42483 [1:37:55<700:12:51, 73.34s/it, loss=0.8665, cls=0.7257, cons=0.1407, img/s=68.3]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Loss graph saved to: ./logs/resnet_clip_yolo_mean_teacher_20251029_112330/cls_loss_graph.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:  29%|â–ˆâ–ˆâ–Š       | 12165/42483 [2:23:32<5:40:55,  1.48it/s, loss=0.8743, cls=0.7121, cons=0.1621, img/s=68.1] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running intermittent validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Summary - Epoch step12165\n",
      "Accuracy: 0.4257 | Loss: 4.1521\n",
      "scheduler lr: [1.0000000000000002e-06, 5e-06, 1e-05, 5e-05]\n",
      "  Validation Accuracy: 0.4257\n",
      "  Best Accuracy: 0.4257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:  29%|â–ˆâ–ˆâ–Š       | 12166/42483 [2:27:39<628:31:00, 74.63s/it, loss=0.8743, cls=0.7121, cons=0.1621, img/s=68.1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved successfully: ./checkpoints/resnet_clip_yolo_mean_teacher_20251029_112330/checkpoint_epoch0_step12165_acc0.4257_20251029_135124.pth\n",
      "Epoch: 0 | Step: 12165 | Val Acc: 0.4257\n",
      "  Loss graph saved to: ./logs/resnet_clip_yolo_mean_teacher_20251029_112330/loss_graph.png\n",
      "  Loss graph saved to: ./logs/resnet_clip_yolo_mean_teacher_20251029_112330/cls_loss_graph.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 14176/42483 [2:50:11<5:27:35,  1.44it/s, loss=1.2942, cls=0.7701, cons=0.5241, img/s=59.7]  "
     ]
    }
   ],
   "source": [
    "# monitor mode collapse\n",
    "recent_predictions = [] \n",
    "max_recent_batches = 15\n",
    "\n",
    "# if epoch == config.freeze_until_epoch:\n",
    "if global_step >= 500:\n",
    "    # print(f\"Unfreezing backbone at epoch {epoch}\")\n",
    "    print(f\"unfreezing backbone at step {global_step}\")\n",
    "    for param in clip_params + yolo_params:\n",
    "        param.requires_grad = True\n",
    "\n",
    "for epoch in range(config.cur_epoch, config.num_epochs):\n",
    "    collapse_flag = False # model pred mode collapse\n",
    "    epoch_start = time.time()\n",
    "\n",
    "    # region Train one epoch ########################################\n",
    "    model.train()\n",
    "    teacher_model.train()\n",
    "    # Cumulative lists for dataframes\n",
    "    # image_data = [] if track_images else None\n",
    "    batch_data = []\n",
    "\n",
    "    if epoch <= 5:\n",
    "        # train_loader = supervised_train_dataloader\n",
    "        ratios = [0.66, 0.55, 0.55, 0.48, 0.40, 0.33, 0.01]\n",
    "        supervised_train_dataset = MeanTeacherDataset(\n",
    "            csv_file = \"train_2.csv\", \n",
    "            root_dir = \"~/Workspace/data-v2/train\",\n",
    "            # supervised = True,\n",
    "            # supervised_ratio = ratios[epoch],\n",
    "            upsample = 3,\n",
    "        )\n",
    "        supervised_sampler = RandomVersionSampler(supervised_train_dataset)\n",
    "        train_loader = DataLoader(\n",
    "            supervised_train_dataset, \n",
    "            batch_size=config.batch_size, \n",
    "            sampler=supervised_sampler,\n",
    "            collate_fn=custom_collate_fn,\n",
    "            num_workers=get_num_workers(),\n",
    "            persistent_workers=False, # True if get_num_workers() > 0 else False,\n",
    "            # pin_memory=False, # WSL does not support pin_memory well\n",
    "            prefetch_factor=2 if get_num_workers() > 0 else None,\n",
    "        )\n",
    "        print(ratios[epoch], len(train_loader))\n",
    "    else: \n",
    "        train_loader = train_dataloader\n",
    "    if epoch >= 1:\n",
    "        criterion = af_criterion\n",
    "    else:\n",
    "        criterion = ce_criterion\n",
    "    \n",
    "    epoch_pbar = tqdm(train_loader, desc=f'Epoch {epoch}', total=len(train_loader))\n",
    "    for batch_num, batch_input in enumerate(epoch_pbar):\n",
    "        if global_step == 500 : #len(train_dataloader)//4:\n",
    "            print(f\"unfreezing backbone at step {global_step}\")\n",
    "            for param in clip_params + yolo_params:\n",
    "                param.requires_grad = True\n",
    "                \n",
    "        batch_start = time.time()\n",
    "\n",
    "        # 1. Grab data from dataloader\n",
    "        clip_weak, clip_strong, yolo_weak, yolo_strong, labels, metadata = batch_input\n",
    "        clip_weak['pixel_values'] = clip_weak['pixel_values'].to(device, non_blocking=True)\n",
    "        clip_strong['pixel_values'] = clip_strong['pixel_values'].to(device, non_blocking=True)\n",
    "        yolo_weak = yolo_weak.to(device, non_blocking=True)\n",
    "        yolo_strong = yolo_strong.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "        \n",
    "        # 2. Forward pass \n",
    "        with torch.no_grad():\n",
    "            with autocast(device_type='cuda', enabled=False):\n",
    "                teacher_outputs = teacher_model(clip_weak, yolo_weak)\n",
    "        with autocast(device_type='cuda'):\n",
    "            student_outputs = model(clip_strong, yolo_strong)\n",
    "            cls_loss = criterion(student_outputs, labels)\n",
    "\n",
    "            # Compute per-sample consistency losses\n",
    "            consistency_losses = compute_consistency_loss(student_outputs, teacher_outputs, entropy_threshold=max(0.25, 0.7 - global_step / 90000))\n",
    "\n",
    "            # Create per-image consistency weights with warmup # currently sigmoid scheduling\n",
    "            if global_step > config.warmup_steps:\n",
    "                warmup_factor = 1.0\n",
    "            else:\n",
    "                phase = 1.0 - global_step / config.warmup_steps # p(x) = 1 - x/30000\n",
    "                warmup_factor = np.exp(-5.0 * phase * phase) # f(x) = e^(-5*p(x)*p(x))\n",
    "            # warmup_factor = min(1.0, global_step / config.warmup_steps) # linear\n",
    "            base_weight = config.consistency_weight * warmup_factor\n",
    "            # higher consistency weights for unlabeled data. lower for labeled. \n",
    "            consistency_weights = torch.zeros_like(consistency_losses)\n",
    "            unlabeled_mask = (labels == 1)\n",
    "            labeled_mask = ~unlabeled_mask\n",
    "            consistency_weights[unlabeled_mask] = base_weight * 2.0\n",
    "            consistency_weights[labeled_mask] = base_weight / 2.0\n",
    "            \n",
    "            # Apply per-image weights and compute mean\n",
    "            weighted_consistency = (consistency_losses * consistency_weights).mean()\n",
    "            loss = cls_loss + weighted_consistency\n",
    "\n",
    "            # if not torch.isfinite(loss):\n",
    "            #     print(f\"[WARN] Non-finite loss at step {global_step}, skipping batch.\")\n",
    "            #     print(\"student_outputs finite:\", torch.isfinite(student_outputs).all().item())\n",
    "            #     print(\"teacher_outputs finite:\", torch.isfinite(teacher_outputs).all().item())\n",
    "            #     print(metadata)\n",
    "            #     continue\n",
    "        \n",
    "        # 3. Backward pass\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # # forward pass\n",
    "        # student_outputs = model(clip_strong, yolo_strong)\n",
    "        # with torch.no_grad():\n",
    "        #     teacher_outputs = teacher_model(clip_weak, yolo_weak)\n",
    "        # cls_loss = criterion(student_outputs, labels)\n",
    "        # consistency_losses = compute_consistency_loss(student_outputs, teacher_outputs)\n",
    "\n",
    "        # if global_step > config.warmup_steps:\n",
    "        #     warmup_factor = 1.0\n",
    "        # else:\n",
    "        #     current = max(0.0, min(float(global_step), config.warmup_steps))\n",
    "        #     phase = 1.0 - current / config.warmup_steps\n",
    "        #     warmup_factor = np.exp(-5.0 * phase * phase)\n",
    "\n",
    "        # base_weight = config.consistency_weight * warmup_factor\n",
    "        # consistency_weights = torch.full_like(consistency_losses, base_weight)\n",
    "        # weighted_consistency = (consistency_losses * consistency_weights).mean()\n",
    "        # loss = cls_loss + weighted_consistency\n",
    "\n",
    "        # # --- Optional: check for NaNs ---\n",
    "        # if not torch.isfinite(loss):\n",
    "        #     print(f\"[WARN] Non-finite loss at step {global_step}, skipping batch.\")\n",
    "        #     print(\"student_outputs finite:\", torch.isfinite(student_outputs).all().item())\n",
    "        #     print(\"teacher_outputs finite:\", torch.isfinite(teacher_outputs).all().item())\n",
    "        #     print(metadata)\n",
    "        #     continue\n",
    "\n",
    "        # # ======================================\n",
    "        # # 3. Backward pass\n",
    "        # # ======================================\n",
    "        # optimizer.zero_grad(set_to_none=True)\n",
    "        # loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "        # optimizer.step()\n",
    "\n",
    "\n",
    "        \n",
    "        # 4. Update teacher model with EMA\n",
    "        # if batch_num % 3 == 0:\n",
    "            # update_ema_variables(model, teacher_model, alpha=config.ema_decay**3, global_step=global_step)\n",
    "        update_ema_variables(model, teacher_model, alpha=config.ema_decay, global_step=global_step)\n",
    "        \n",
    "        if global_step % config.log_interval != 0:\n",
    "            global_step += 1\n",
    "            continue\n",
    "        \n",
    "        # monitor mode collapse\n",
    "        with torch.no_grad():\n",
    "            current_predictions = F.softmax(student_outputs, dim=1).argmax(dim=1).cpu().numpy()\n",
    "        recent_predictions.append(current_predictions)\n",
    "        if len(recent_predictions) > max_recent_batches:\n",
    "            recent_predictions.pop(0)\n",
    "        all_recent_preds = np.concatenate(recent_predictions)\n",
    "        unique, counts = np.unique(all_recent_preds, return_counts=True)\n",
    "        dominant_class = unique[np.argmax(counts)]\n",
    "        dominant_ratio = counts.max() / len(all_recent_preds)\n",
    "        if collapse_flag==False and dominant_ratio > 0.95:  # If >95% of predictions are same class\n",
    "            print(f\"[WARN] Possible mode collapse at step {global_step}!\")\n",
    "            print(f\"       Class {dominant_class} represents {dominant_ratio:.1%} of last {len(all_recent_preds)} predictions\")\n",
    "            print(f\"       Distribution: {dict(zip(unique, counts))}\")\n",
    "            collapse_flag = True\n",
    "\n",
    "        # 5. Calculate metrics #############################################\n",
    "        batch_time = time.time() - batch_start\n",
    "        images_per_second = config.batch_size / batch_time\n",
    "        \n",
    "        # Move to CPU for storage\n",
    "        loss_cpu = loss.item()\n",
    "        cls_loss_cpu = cls_loss.item()\n",
    "        weighted_consistency_cpu = weighted_consistency.item()\n",
    "        \n",
    "        loss_history.append(loss_cpu)\n",
    "        cls_loss_history.append(cls_loss_cpu)\n",
    "\n",
    "        # Add batch-level data\n",
    "        batch_data.append({\n",
    "            'batch_num': batch_num,\n",
    "            'epoch': epoch,\n",
    "            'global_step': global_step,\n",
    "            'loss': loss_cpu,\n",
    "            'cls_loss': cls_loss_cpu,\n",
    "            'consistency_loss': weighted_consistency_cpu,\n",
    "            'consistency_weight': consistency_weights[0].item(), # TODO: remove after sanity check\n",
    "            # 'batch_time': batch_time,\n",
    "            # 'images_per_second': images_per_second,\n",
    "        })\n",
    "        \n",
    "        # Optionally track per-image data\n",
    "        # if track_images:\n",
    "        #     probs = F.softmax(student_outputs, dim=1).cpu().numpy()\n",
    "        #     predictions = student_outputs.argmax(dim=1).cpu().numpy()\n",
    "        #     labels_cpu = labels.cpu().numpy()\n",
    "        #     consistency_losses_cpu = consistency_losses.cpu().numpy()\n",
    "            \n",
    "        #     for i in range(len(labels)):\n",
    "        #         image_data.append({\n",
    "        #             'batch_num': batch_num,\n",
    "        #             'global_step': global_step,\n",
    "        #             'img_path': metadata['img_path'][i],\n",
    "        #             'label': int(labels_cpu[i]),\n",
    "        #             'width': int(metadata['width'][i]),\n",
    "        #             'height': int(metadata['height'][i]),\n",
    "        #             'size_kb': float(metadata['size_kb'][i]),\n",
    "        #             'source': metadata['source'][i],\n",
    "        #             'prediction': int(predictions[i]),\n",
    "        #             'bad': float(probs[i, 0]),\n",
    "        #             'neutral': float(probs[i, 1]),\n",
    "        #             'good': float(probs[i, 2]),\n",
    "        #             'consistency_loss': float(consistency_losses_cpu[i]),\n",
    "        #         })\n",
    "        \n",
    "        # Update progress bar\n",
    "        epoch_pbar.set_postfix({\n",
    "            'loss': f'{loss_cpu:.4f}',\n",
    "            'cls': f'{cls_loss_cpu:.4f}',\n",
    "            'cons': f'{weighted_consistency_cpu:.4f}',\n",
    "            'img/s': f'{images_per_second:.1f}'\n",
    "        })\n",
    "        \n",
    "        # Log to tensorboard/CSV at intervals\n",
    "        if global_step % config.log_interval == 0:\n",
    "            current_lr = optimizer.param_groups[-1]['lr'] # should correspond to my FC classifier layers\n",
    "            \n",
    "            train_metrics = {\n",
    "                \"train/loss\": loss_cpu,\n",
    "                \"train/cls_loss\": cls_loss_cpu,\n",
    "                \"train/consistency\": weighted_consistency_cpu,\n",
    "                \"train/consistency_weight\": consistency_weights[0].item(),\n",
    "                \"train/learning_rate\": current_lr,\n",
    "                # \"train/images_per_second\": images_per_second,\n",
    "                \"system/gpu_memory_mb\": get_gpu_memory_usage()\n",
    "            }\n",
    "            \n",
    "            logger.log_metrics(train_metrics, global_step)\n",
    "            logger.log_train_step(global_step, epoch, {\n",
    "                'loss': loss_cpu,\n",
    "                'cls_loss': cls_loss_cpu,\n",
    "                'consistency_loss': weighted_consistency_cpu,\n",
    "                'learning_rate': current_lr,\n",
    "                'consistency_weight': consistency_weights[0].item()\n",
    "            })\n",
    "\n",
    "        # 6. Intermittent validation\n",
    "        if global_step > 0 and global_step % validation_frequency == 0:\n",
    "            intermittent_epoch = \"step\" + str(global_step)\n",
    "            print(\"Running intermittent validation...\")\n",
    "            val_image_df, val_batch_df, val_loss = validate(teacher_model, val_dataloader, config, intermittent_epoch)\n",
    "            val_accuracy, val_metrics, cm = analyse(val_image_df, val_batch_df, config, monitor, intermittent_epoch)\n",
    "            scheduler.step(val_accuracy)\n",
    "            print('scheduler lr:', scheduler.get_last_lr())\n",
    "\n",
    "            # logger.log_metrics(val_metrics, epoch)\n",
    "            logger.log_validation(intermittent_epoch, val_metrics)\n",
    "            logger.log_confusion_matrix(cm, ['bad', 'neutral', 'good'], intermittent_epoch)\n",
    "            \n",
    "            print(f\"  Validation Accuracy: {val_accuracy:.4f}\")\n",
    "            print(f\"  Best Accuracy: {monitor.best_accuracy:.4f}\")\n",
    "\n",
    "            if val_accuracy >= monitor.best_accuracy:    \n",
    "                save_checkpoint(\n",
    "                    model, teacher_model, optimizer, scheduler, epoch, global_step, \n",
    "                    config, val_accuracy, monitor, loss_history, cls_loss_history\n",
    "                )\n",
    "            \n",
    "            plot_running_loss(loss_history, os.path.join(logger.get_log_dir(), f'loss_graph.png'))\n",
    "            plot_running_loss(cls_loss_history, os.path.join(logger.get_log_dir(), f'cls_loss_graph.png'))\n",
    "        global_step += 1\n",
    "\n",
    "    # Create dataframes\n",
    "    # image_df = pd.DataFrame(image_data) if track_images else None\n",
    "    batch_df = pd.DataFrame(batch_data)\n",
    "\n",
    "    train_image_df = None # image_df\n",
    "    train_batch_df = batch_df\n",
    "\n",
    "    # endregion #####################################################\n",
    "    \n",
    "    # Analyze epoch\n",
    "    epoch_metrics = analyse_epoch(train_image_df, train_batch_df, config, epoch)\n",
    "    logger.log_metrics(epoch_metrics, epoch)\n",
    "    \n",
    "    print(f\"Epoch {epoch} Summary:\")\n",
    "    print(f\"  Time: {(time.time() - epoch_start):.1f}s\")\n",
    "    print(f\"  Avg Loss: {epoch_metrics['epoch/loss']:.4f}\")\n",
    "    print(f\"  Avg Classification Loss: {epoch_metrics['epoch/cls_loss']:.4f}\")\n",
    "    print(f\"  Avg Consistency Loss: {epoch_metrics['epoch/consistency_loss']:.4f}\")\n",
    "    # print(f\"  Throughput: {epoch_metrics['epoch/avg_throughput']:.1f} images/second\")\n",
    "\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Validation\n",
    "    print(\"Running end of epoch validation...\")\n",
    "    val_image_df, val_batch_df, val_loss = validate(teacher_model, val_dataloader, config, epoch)\n",
    "    val_accuracy, val_metrics, cm = analyse(val_image_df, val_batch_df, config, monitor, epoch)\n",
    "    \n",
    "    logger.log_metrics(val_metrics, epoch)\n",
    "    logger.log_validation(epoch, val_metrics)\n",
    "    logger.log_confusion_matrix(cm, ['bad', 'neutral', 'good'], epoch)\n",
    "    \n",
    "    print(f\"  Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    print(f\"  Best Accuracy: {monitor.best_accuracy:.4f}\")\n",
    "    print(\"=\"*80)\n",
    "    print()\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    save_checkpoint(\n",
    "        model, teacher_model, optimizer, scheduler, epoch, global_step, \n",
    "        config, val_accuracy, monitor, loss_history, cls_loss_history\n",
    "    )\n",
    "    \n",
    "    scheduler.step(val_accuracy)\n",
    "    print('scheduler lr:', scheduler.get_last_lr())\n",
    "\n",
    "plot_running_loss(loss_history, os.path.join(logger.get_log_dir(), f'loss_graph.png'))\n",
    "plot_running_loss(cls_loss_history, os.path.join(logger.get_log_dir(), f'cls_loss_graph.png'))\n",
    "cons_loss_history = [loss - cons_loss for loss, cons_loss in zip(loss_history, cls_loss_history)]\n",
    "plot_running_loss(cons_loss_history, os.path.join(logger.get_log_dir(), f'cons_loss_graph.png'))\n",
    "logger.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62345d12",
   "metadata": {},
   "source": [
    "### the lands down under"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2642223",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9151b259",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    }
   ],
   "source": [
    "val_image_df, val_batch_df = validate(teacher_model, val_dataloader, config, -1)\n",
    "val_accuracy, val_metrics, cm = analyse(val_image_df, val_batch_df, config, monitor, -1)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7216d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Summary - Epoch -1\n",
      "Accuracy: 0.4842 | Loss: 1.1626\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[5280, 3550, 2577],\n",
       "       [7105, 9833, 6210],\n",
       "       [ 982, 1276, 2591]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_as_bad = val_image_df[(val_image_df['label'] == 2) & (val_image_df['prediction'] == 0)]\n",
    "bad_as_good = val_image_df[(val_image_df['label'] == 0) & (val_image_df['prediction'] == 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbec752d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in bad_as_good[\"img_path\"]:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22bfbc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-batch means (bad,neut,good): [     9.0645      11.903      2.8387]\n",
      "Fraction of batches with a single class: 0.0\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "num_batches = 100\n",
    "hist_over_batches = []\n",
    "for batch_i, (_, _, _, _, labels, _) in enumerate(supervised_train_dataloader):\n",
    "    labels = labels.numpy().tolist()\n",
    "    c = Counter(labels)\n",
    "    hist_over_batches.append([c.get(0,0), c.get(1,0), c.get(2,0)])\n",
    "    if batch_i+1 >= num_batches: break\n",
    "\n",
    "import numpy as np\n",
    "hist = np.array(hist_over_batches)\n",
    "print(\"Per-batch means (bad,neut,good):\", hist.mean(axis=0))\n",
    "print(\"Fraction of batches with a single class:\", np.mean((hist==0).sum(axis=1)==2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be8d014",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "svmem(total=46286409728, available=40099004416, percent=13.4, used=5640769536, free=37585137664, active=1640988672, inactive=6467284992, buffers=300068864, cached=2760433664, shared=89407488, slab=213061632)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "psutil.virtual_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b25a86e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9006cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8813fe2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ab' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mab\u001b[49m\n",
      "\u001b[31mNameError\u001b[39m: name 'ab' is not defined"
     ]
    }
   ],
   "source": [
    "ab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea6153f",
   "metadata": {},
   "source": [
    "validate more frequently, my epochs are obscenely long\n",
    "can I like ... cut out the resnet for now lmao\n",
    "\n",
    "i should start with 1 epoch on labeled data\n",
    "and mostly freeze \n",
    "\n",
    "and then let the consistency loss start creeping up after ^ epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80fa57a",
   "metadata": {},
   "source": [
    "# STUFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d18b059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b61fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Summary - Epoch -1\n",
      "Accuracy: 0.3433 | Loss: 0.6820\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 3924,  7204,   279],\n",
       "       [ 3703, 18692,   753],\n",
       "       [  388,  2805,  1656]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = MeanTeacherDataset(\n",
    "    csv_file = \"test_2.csv\", \n",
    "    root_dir = \"~/Workspace/data-v2/test\",\n",
    "    val = True,\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=config.batch_size, \n",
    "    collate_fn=custom_collate_fn,\n",
    "    num_workers=get_num_workers(),\n",
    "    persistent_workers=False, # True if get_num_workers() > 0 else False,\n",
    "    # pin_memory=False, # WSL does not support pin_memory well\n",
    "    prefetch_factor=3 if get_num_workers() > 0 else None,\n",
    ")\n",
    "test_image_df, test_batch_df, loss = validate(teacher_model, test_dataloader, config, -1)\n",
    "test_accuracy, test_metrics, cm = analyse(test_image_df, test_batch_df, config, monitor, -1)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cad6e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved successfully: ./checkpoints/resnet_clip_yolo_mean_teacher_20251027_112613/checkpoint_epoch2_step54258_acc0.3441_20251027_221157.pth\n",
      "Epoch: 2 | Step: 54258 | Val Acc: 0.3441\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./checkpoints/resnet_clip_yolo_mean_teacher_20251027_112613/checkpoint_epoch2_step54258_acc0.3441_20251027_221157.pth'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_checkpoint(\n",
    "        model, teacher_model, optimizer, scheduler, epoch, global_step, \n",
    "        config, val_accuracy, monitor, loss_history, cls_loss_history\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d649fab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def benchmark(num_workers, num_batches=100):\n",
    "    global dry_run\n",
    "    \n",
    "    # Temporarily set dry_run for benchmarking\n",
    "    original_dry_run = dry_run\n",
    "    dry_run = num_batches * config.batch_size  # Ensure we have enough samples\n",
    "    \n",
    "    # Create benchmark dataset and dataloader\n",
    "    bench_dataset = MeanTeacherDataset(\n",
    "        csv_file=\"train_2.csv\", \n",
    "        root_dir= \"/mnt/d/data-v2/train\",#\"~/data-v2/train\" if get_system_type() == \"wsl\" else \"D:\\\\data-v2\\\\train\",\n",
    "        val=False,\n",
    "    )\n",
    "    \n",
    "    bench_sampler = RandomVersionSampler(bench_dataset)\n",
    "    bench_dataloader = DataLoader(\n",
    "        bench_dataset, \n",
    "        batch_size=config.batch_size, \n",
    "        sampler=bench_sampler,\n",
    "        num_workers=num_workers,\n",
    "        persistent_workers=True if num_workers > 0 else False,\n",
    "    )\n",
    "    \n",
    "    # Reset model and optimizer to ensure fair comparison\n",
    "    temp_model = BiggerClassifier().to(config.device)\n",
    "    temp_teacher = copy.deepcopy(temp_model).to(config.device)\n",
    "    temp_teacher.eval()\n",
    "    for p in temp_teacher.parameters():\n",
    "        p.requires_grad = False\n",
    "    \n",
    "    temp_optimizer = torch.optim.AdamW([\n",
    "        {'params': temp_model.parameters(), 'lr': config.initial_lr}\n",
    "    ])\n",
    "\n",
    "    print(f\"Benchmarking with num_workers={num_workers}...\")\n",
    "    start_time = time.time()\n",
    "    batch_count = 0\n",
    "    \n",
    "    for epoch in range(1):\n",
    "        \n",
    "        global_step, train_image_df, train_batch_df = train_one_epoch(\n",
    "            temp_model, temp_teacher, bench_dataloader, temp_optimizer, criterion, \n",
    "            config, epoch, logger, 0, \n",
    "            track_images=False  # Set to True only if you need detailed per-image analysis\n",
    "        )\n",
    "\n",
    "        \n",
    "        \n",
    "        # Validation\n",
    "        # val_image_df, val_batch_df = validate(teacher_model, val_dataloader, config, epoch)\n",
    "        # val_accuracy, val_metrics, cm = analyse(val_image_df, val_batch_df, config, monitor, epoch)\n",
    "\n",
    "\n",
    "\n",
    "        batch_count += len(bench_dataloader)\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "\n",
    "    # Restore original dry_run\n",
    "    dry_run = original_dry_run\n",
    "    \n",
    "    # Clean up\n",
    "    del temp_model, temp_teacher, temp_optimizer, bench_dataset, bench_dataloader\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return total_time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597e9e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run benchmark\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BENCHMARKING NUM_WORKERS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "results = []\n",
    "for num_workers in range(0, 16):\n",
    "\n",
    "    elapsed_time = benchmark(num_workers, num_batches=100)\n",
    "    batches_per_second = 100 / elapsed_time\n",
    "    images_per_second = (100 * config.batch_size) / elapsed_time\n",
    "    \n",
    "    results.append({\n",
    "        'num_workers': num_workers,\n",
    "        'total_time': elapsed_time,\n",
    "        'batches_per_second': batches_per_second,\n",
    "        'images_per_second': images_per_second\n",
    "    })\n",
    "    \n",
    "    print(f\"\\nnum_workers={num_workers:2d} | \"\n",
    "            f\"Time: {elapsed_time:6.2f}s | \"\n",
    "            f\"Batches/s: {batches_per_second:5.2f} | \"\n",
    "            f\"Images/s: {images_per_second:6.1f}\")\n",
    "\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BENCHMARK SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Find optimal\n",
    "if len(results) > 0:\n",
    "    best_result = max(results, key=lambda x: x['images_per_second'])\n",
    "    print(f\"\\nOptimal num_workers: {best_result['num_workers']} \"\n",
    "          f\"({best_result['images_per_second']:.1f} images/s)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec42660",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a26a7f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting profiling...\n",
      "Profiling complete! Check ./logs/profiler for results\n",
      "View with: tensorboard --logdir=./logs/profiler\n"
     ]
    }
   ],
   "source": [
    "def profile_training_step(model, teacher_model, dataloader, optimizer, criterion, config):\n",
    "    model.train()\n",
    "    # Profile\n",
    "    print(\"Starting profiling...\")\n",
    "    with torch.profiler.profile(\n",
    "        activities=[torch.profiler.ProfilerActivity.CPU, \n",
    "                   torch.profiler.ProfilerActivity.CUDA],\n",
    "        schedule=torch.profiler.schedule(wait=40, warmup=2, active=3, repeat=1),\n",
    "        on_trace_ready=torch.profiler.tensorboard_trace_handler('./logs/profiler'),\n",
    "        record_shapes=True,\n",
    "        profile_memory=True,\n",
    "        with_stack=True\n",
    "    ) as prof:\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            if i >= 50: break\n",
    "            \n",
    "            clip_weak, clip_strong, yolo_weak, yolo_strong, labels, metadata = batch\n",
    "            clip_weak['pixel_values'] = clip_weak['pixel_values'].to(device, non_blocking=True)\n",
    "            clip_strong['pixel_values'] = clip_strong['pixel_values'].to(device, non_blocking=True)\n",
    "            yolo_weak = yolo_weak.to(device, non_blocking=True)\n",
    "            yolo_strong = yolo_strong.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "            \n",
    "            \n",
    "            # with autocast(device_type='cuda'):\n",
    "            student_outputs = model(clip_strong, yolo_strong)\n",
    "            cls_loss = criterion(student_outputs, labels)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                teacher_outputs = teacher_model(clip_weak, yolo_weak)\n",
    "            \n",
    "            consistency_losses = compute_consistency_loss(student_outputs, teacher_outputs)\n",
    "            warmup_factor = min(1.0, i / config.warmup_steps)\n",
    "            base_weight = config.consistency_weight * warmup_factor\n",
    "            consistency_weights = torch.full_like(consistency_losses, base_weight)\n",
    "            weighted_consistency = (consistency_losses * consistency_weights).mean()\n",
    "            loss = cls_loss + weighted_consistency\n",
    "            loss.backward()\n",
    "            # optimizer.zero_grad()\n",
    "            # scaler.scale(loss).backward()\n",
    "            # scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)\n",
    "            # scaler.step(optimizer)\n",
    "            # scaler.update()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            update_ema_variables(model, teacher_model, alpha=config.ema_decay, global_step=i)\n",
    "            \n",
    "            prof.step()\n",
    "    \n",
    "    print(\"Profiling complete! Check ./logs/profiler for results\")\n",
    "    print(\"View with: tensorboard --logdir=./logs/profiler\")\n",
    "\n",
    "# Usage:\n",
    "profile_training_step(model, teacher_model, train_dataloader, optimizer, criterion, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1c9868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "environment: wsl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a36a13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3a345abe",
   "metadata": {},
   "source": [
    "# NOTES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76856a4d",
   "metadata": {},
   "source": [
    "remember to collect the original file name without suffix when making your csv\n",
    "\n",
    "if my teacher model strongly misclassifies, toss the file name out into a log\n",
    "\n",
    "compute_consistency_loss should evolve over epochs - later on, use higher thresholding, and maybe move to sigmoid/binary thresholding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2922307c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a0885e95",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "img-classifier",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
