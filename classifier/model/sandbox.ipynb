{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cbd623-1f85-41e7-8808-78f1976842f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from ultralytics import YOLO \n",
    "from transformers import CLIPVisionModel, CLIPImageProcessor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2098bb80-75ec-4b6e-bc1e-27d942f5e748",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5b06baca-0890-45ce-b840-83f6bc5617a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 7, 7])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class YOLOv11(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = YOLO(\"yolov11l-face.pt\").model\n",
    "        # self.backbone = torch.nn.Sequential(*list(self.model.model.children())[:7])  # Stops after C3k2 (layer 6)\n",
    "        self.feature_model = torch.nn.Sequential(*list(self.model.model.children())[:10])  # Stops after SPPF (layer 9)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.feature_model(x)\n",
    "\n",
    "extractor = YOLOv11()\n",
    "features = extractor(torch.rand(1, 3, 224, 224)) \n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3fe24294-dbfa-49d7-9b6e-a24d4879d6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: Custom forward pass by modifying the encoder\n",
    "class CLIP_Method2(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.clip_model = CLIPVisionModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        self.processor = CLIPImageProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        \n",
    "    def forward(self, images):\n",
    "        # Process images\n",
    "        if isinstance(images, Image.Image):\n",
    "            images = [images]\n",
    "        elif not isinstance(images, list):\n",
    "            images = images\n",
    "            \n",
    "        inputs = self.processor(images=images, return_tensors=\"pt\")\n",
    "        pixel_values = inputs['pixel_values']\n",
    "        \n",
    "        # Get embeddings\n",
    "        vision_model = self.clip_model.vision_model\n",
    "        embeddings = vision_model.embeddings(pixel_values)\n",
    "        embeddings = vision_model.pre_layrnorm(embeddings)\n",
    "        \n",
    "        # Pass through first 10 encoder layers only\n",
    "        hidden_states = embeddings\n",
    "        for idx, encoder_layer in enumerate(vision_model.encoder.layers[:10]):\n",
    "            layer_outputs = encoder_layer(\n",
    "                hidden_states,\n",
    "                attention_mask=None,\n",
    "                causal_attention_mask=None,\n",
    "                output_attentions=False,\n",
    "            )\n",
    "            hidden_states = layer_outputs[0]\n",
    "        \n",
    "        return hidden_states\n",
    "\n",
    "model2 = CLIP_Method2()\n",
    "output2 = model2(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "90cb6254-5c13-4dd4-a493-5ec409cfa692",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIPVisionModel(\n",
       "  (vision_model): CLIPVisionTransformer(\n",
       "    (embeddings): CLIPVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "      (position_embedding): Embedding(50, 768)\n",
       "    )\n",
       "    (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CLIP(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.clip_model = CLIPVisionModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "        # CLIP's final hidden state before projection (not the projection itself)\n",
    "        self.clip_output_dim = self.clip_model.config.hidden_size  # usually 512\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Resize and normalize input for CLIP\n",
    "        processor = CLIPImageProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        inputs = processor(images=x, return_tensors=\"pt\")#.to(device)\n",
    "        outputs = self.clip_model(**inputs)\n",
    "        pooled_output = outputs.pooler_output  # shape: [batch_size, 512]\n",
    "        return pooled_output\n",
    "\n",
    "model = CLIP()\n",
    "image = Image.open(\"image.jpg\")\n",
    "inputs = model(image) \n",
    "outputs.shape\n",
    "\n",
    "model.clip_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "17ced753-7aac-494e-96e3-ba332ed40242",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4277434784.py, line 9)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[53]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mself.linear_head = pass\u001b[39m\n                       ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class MyModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.yolov11 = YOLOv11()\n",
    "        self.clip = CLIP()\n",
    "\n",
    "        # TODO:\n",
    "        self.linear_head = pass\n",
    "        self.softmax = pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO: preprocess accordingly, since the models expect different normalizations\n",
    "        yolo_output = self.yolov11(x)\n",
    "        clip_output = self.clip(x)\n",
    "\n",
    "        # concat yolo_output + clip_output\n",
    "\n",
    "        # run through linear and softmax\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dd5bd4e7-96fb-4837-9655-46edb73f6528",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "\n",
    "# 1. Load model and processor\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# 2. Process image\n",
    "image = Image.open(\"image.jpg\")\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "# 3. Get image features\n",
    "with torch.no_grad():\n",
    "    outputs = model.get_image_features(**inputs)\n",
    "\n",
    "outputs.shape\n",
    "# outputs.shape = [1, 512] (512-dimensional embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ddbe594-045c-4f6b-8f51-3a53de4e12ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4d68180a-20b1-4bd0-ad33-3a8deebabccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 3090\n",
      "\n",
      "=== Testing Different Solutions ===\n",
      "\n",
      "\n",
      "\n",
      "=== GPU Bottleneck Diagnosis ===\n",
      "\n",
      "Model parameters: 87.5M\n",
      "Batch size   1: 0.127s, 0.69 TFLOPS\n",
      "Batch size   8: 0.128s, 5.51 TFLOPS\n",
      "Batch size  32: 0.282s, 9.98 TFLOPS\n",
      "Batch size 128: 1.133s, 9.95 TFLOPS\n",
      "\n",
      "Diagnosis: If TFLOPS doesn't scale with batch size, model is memory-bandwidth bound.\n",
      "Solutions: Use larger models, optimize memory access patterns, or use multiple models in parallel.\n",
      "\n",
      "=== Recommendations ===\n",
      "1. Your GPU is likely memory-bandwidth limited for this small model\n",
      "2. Consider using ViT-L models which better utilize GPU compute\n",
      "3. For production, consider model serving frameworks like TorchServe or Triton\n",
      "4. If you must use ViT-B/32, run multiple models in parallel on different CUDA streams\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from transformers import CLIPVisionModel, CLIPImageProcessor\n",
    "import numpy as np\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import threading\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "    # Enable TF32 for Ampere GPUs (A100, RTX 30xx)\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "# SOLUTION 1: CPU Preprocessing Bottleneck Fix\n",
    "class CLIP_ParallelPreprocess(torch.nn.Module):\n",
    "    \"\"\"Addresses CPU preprocessing bottleneck by parallelizing image preprocessing\"\"\"\n",
    "    def __init__(self, num_workers=4):\n",
    "        super().__init__()\n",
    "        self.clip_model = CLIPVisionModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        self.clip_model = self.clip_model.to(device)\n",
    "        self.clip_model.eval()\n",
    "        self.processor = CLIPImageProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        self.num_workers = num_workers\n",
    "        \n",
    "    def preprocess_batch(self, images):\n",
    "        \"\"\"Preprocess images in parallel using ThreadPoolExecutor\"\"\"\n",
    "        with ThreadPoolExecutor(max_workers=self.num_workers) as executor:\n",
    "            # Process each image in parallel\n",
    "            processed = list(executor.map(\n",
    "                lambda img: self.processor(images=img, return_tensors=\"pt\")['pixel_values'].squeeze(0),\n",
    "                images\n",
    "            ))\n",
    "        # Stack into batch tensor\n",
    "        return torch.stack(processed).to(device, non_blocking=True)\n",
    "    \n",
    "    def forward(self, images):\n",
    "        if isinstance(images, Image.Image):\n",
    "            images = [images]\n",
    "        \n",
    "        # Parallel preprocessing\n",
    "        pixel_values = self.preprocess_batch(images)\n",
    "        \n",
    "        # Get embeddings directly\n",
    "        vision_model = self.clip_model.vision_model\n",
    "        with torch.no_grad():\n",
    "            with torch.cuda.amp.autocast(dtype=torch.float16):\n",
    "                embeddings = vision_model.embeddings(pixel_values)\n",
    "                embeddings = vision_model.pre_layrnorm(embeddings)\n",
    "                \n",
    "                # First 10 layers only\n",
    "                hidden_states = embeddings\n",
    "                for layer in vision_model.encoder.layers[:10]:\n",
    "                    layer_outputs = layer(hidden_states, None, None, False)\n",
    "                    hidden_states = layer_outputs[0]\n",
    "        \n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "# SOLUTION 2: Use Larger Model / Different Architecture\n",
    "class CLIP_LargerModel(torch.nn.Module):\n",
    "    \"\"\"Use a larger model that better utilizes GPU compute\"\"\"\n",
    "    def __init__(self, model_name=\"openai/clip-vit-large-patch14\"):\n",
    "        super().__init__()\n",
    "        # ViT-L/14 is 4x larger than ViT-B/32, better GPU utilization\n",
    "        self.clip_model = CLIPVisionModel.from_pretrained(model_name)\n",
    "        self.clip_model = self.clip_model.to(device)\n",
    "        self.clip_model.eval()\n",
    "        self.processor = CLIPImageProcessor.from_pretrained(model_name)\n",
    "        \n",
    "    def forward(self, images):\n",
    "        if isinstance(images, Image.Image):\n",
    "            images = [images]\n",
    "            \n",
    "        inputs = self.processor(images=images, return_tensors=\"pt\")\n",
    "        pixel_values = inputs['pixel_values'].to(device, non_blocking=True)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            with torch.cuda.amp.autocast(dtype=torch.float16):\n",
    "                outputs = self.clip_model(pixel_values=pixel_values, output_hidden_states=True)\n",
    "        \n",
    "        # Return 10th layer (or adjust based on model depth)\n",
    "        return outputs.hidden_states[min(10, len(outputs.hidden_states)-1)]\n",
    "\n",
    "\n",
    "# SOLUTION 3: Memory-Compute Trade-off with Flash Attention\n",
    "class CLIP_OptimizedAttention(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Optimize attention computation for better memory bandwidth utilization.\n",
    "    Note: Requires PyTorch 2.0+ with Flash Attention support\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.clip_model = CLIPVisionModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        \n",
    "        # Enable Flash Attention if available (PyTorch 2.0+)\n",
    "        if hasattr(torch.nn.functional, 'scaled_dot_product_attention'):\n",
    "            print(\"Flash Attention available - enabling optimized attention\")\n",
    "            # This enables memory-efficient attention computation\n",
    "            torch.backends.cuda.enable_flash_sdp(True)\n",
    "            torch.backends.cuda.enable_mem_efficient_sdp(True)\n",
    "        \n",
    "        self.clip_model = self.clip_model.to(device)\n",
    "        self.clip_model.eval()\n",
    "        self.processor = CLIPImageProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    \n",
    "    def forward(self, images):\n",
    "        if isinstance(images, Image.Image):\n",
    "            images = [images]\n",
    "            \n",
    "        inputs = self.processor(images=images, return_tensors=\"pt\")\n",
    "        pixel_values = inputs['pixel_values'].to(device, non_blocking=True)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            with torch.cuda.amp.autocast(dtype=torch.float16):\n",
    "                outputs = self.clip_model(pixel_values=pixel_values, output_hidden_states=True)\n",
    "        \n",
    "        return outputs.hidden_states[10]\n",
    "\n",
    "\n",
    "# SOLUTION 4: True Batch Processing with Pre-allocated Buffers\n",
    "class CLIP_PreallocatedBatch(torch.nn.Module):\n",
    "    \"\"\"Pre-allocate buffers to avoid memory allocation overhead\"\"\"\n",
    "    def __init__(self, max_batch_size=64):\n",
    "        super().__init__()\n",
    "        self.clip_model = CLIPVisionModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        self.clip_model = self.clip_model.to(device)\n",
    "        self.clip_model.eval()\n",
    "        self.processor = CLIPImageProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        \n",
    "        # Pre-allocate buffers\n",
    "        self.max_batch_size = max_batch_size\n",
    "        self.pixel_buffer = torch.zeros(\n",
    "            (max_batch_size, 3, 224, 224), \n",
    "            device=device, \n",
    "            dtype=torch.float16\n",
    "        )\n",
    "        \n",
    "    def forward(self, images):\n",
    "        if isinstance(images, Image.Image):\n",
    "            images = [images]\n",
    "        \n",
    "        batch_size = len(images)\n",
    "        assert batch_size <= self.max_batch_size\n",
    "        \n",
    "        # Process images and fill pre-allocated buffer\n",
    "        for i, img in enumerate(images):\n",
    "            processed = self.processor(images=img, return_tensors=\"pt\")['pixel_values']\n",
    "            self.pixel_buffer[i] = processed.squeeze(0).to(device, dtype=torch.float16)\n",
    "        \n",
    "        # Use only the filled portion of the buffer\n",
    "        pixel_values = self.pixel_buffer[:batch_size]\n",
    "        \n",
    "        vision_model = self.clip_model.vision_model\n",
    "        with torch.no_grad():\n",
    "            with torch.cuda.amp.autocast():\n",
    "                embeddings = vision_model.embeddings(pixel_values)\n",
    "                embeddings = vision_model.pre_layrnorm(embeddings)\n",
    "                \n",
    "                hidden_states = embeddings\n",
    "                for layer in vision_model.encoder.layers[:10]:\n",
    "                    layer_outputs = layer(hidden_states, None, None, False)\n",
    "                    hidden_states = layer_outputs[0]\n",
    "        \n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "# SOLUTION 5: The Real Fix - Understanding the Problem\n",
    "def diagnose_gpu_bottleneck():\n",
    "    \"\"\"Diagnose why batching isn't helping\"\"\"\n",
    "    \n",
    "    print(\"=== GPU Bottleneck Diagnosis ===\\n\")\n",
    "    \n",
    "    # 1. Check if model is compute or memory bound\n",
    "    model = CLIPVisionModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device).eval()\n",
    "    \n",
    "    # Count parameters and compute requirements\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Model parameters: {total_params/1e6:.1f}M\")\n",
    "    \n",
    "    # 2. Measure memory bandwidth vs compute\n",
    "    batch_sizes = [1, 8, 32, 128]\n",
    "    \n",
    "    for bs in batch_sizes:\n",
    "        dummy_input = torch.randn(bs, 3, 224, 224, device=device, dtype=torch.float16)\n",
    "        \n",
    "        # Warmup\n",
    "        for _ in range(5):\n",
    "            with torch.no_grad():\n",
    "                _ = model(dummy_input)\n",
    "        \n",
    "        torch.cuda.synchronize()\n",
    "        start = time.time()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for _ in range(20):\n",
    "                _ = model(dummy_input)\n",
    "        \n",
    "        torch.cuda.synchronize()\n",
    "        elapsed = time.time() - start\n",
    "        \n",
    "        # Calculate theoretical FLOPS\n",
    "        # ViT-B/32: ~4.4 GFLOPs per image\n",
    "        gflops_per_image = 4.4\n",
    "        total_gflops = gflops_per_image * bs * 20\n",
    "        achieved_tflops = total_gflops / elapsed / 1000\n",
    "        \n",
    "        print(f\"Batch size {bs:3d}: {elapsed:.3f}s, {achieved_tflops:.2f} TFLOPS\")\n",
    "    \n",
    "    print(\"\\nDiagnosis: If TFLOPS doesn't scale with batch size, model is memory-bandwidth bound.\")\n",
    "    print(\"Solutions: Use larger models, optimize memory access patterns, or use multiple models in parallel.\")\n",
    "\n",
    "\n",
    "# Benchmark all solutions\n",
    "if __name__ == \"__main__\":\n",
    "    image = Image.open(\"image.jpg\")\n",
    "    batch_sizes = [1, 8, 16, 32]\n",
    "    \n",
    "    print(\"\\n=== Testing Different Solutions ===\\n\")\n",
    "    \n",
    "    # Test each solution\n",
    "    solutions = [\n",
    "        (\"Parallel Preprocessing\", CLIP_ParallelPreprocess()),\n",
    "        (\"Pre-allocated Buffers\", CLIP_PreallocatedBatch()),\n",
    "        # (\"Larger Model (ViT-L)\", CLIP_LargerModel()),  # Uncomment if you have enough memory\n",
    "    ]\n",
    "\n",
    "    \n",
    "    # Run diagnosis\n",
    "    print(\"\\n\")\n",
    "    diagnose_gpu_bottleneck()\n",
    "    \n",
    "    # Additional recommendations\n",
    "    print(\"\\n=== Recommendations ===\")\n",
    "    print(\"1. Your GPU is likely memory-bandwidth limited for this small model\")\n",
    "    print(\"2. Consider using ViT-L models which better utilize GPU compute\")\n",
    "    print(\"3. For production, consider model serving frameworks like TorchServe or Triton\")\n",
    "    print(\"4. If you must use ViT-B/32, run multiple models in parallel on different CUDA streams\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1b248a-7ee2-47bc-ae01-31cfeb5942fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
