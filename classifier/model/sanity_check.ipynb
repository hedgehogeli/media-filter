{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5893cc9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to: ./logs/sanitycheck_20251030_013129\n",
      "Freezing backbone parameters initially...\n",
      "scheduler lr: [1.0000000000000002e-06, 5e-06, 1e-05, 5e-05]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "import torchvision.transforms.v2 as transforms\n",
    "from torchvision.transforms.v2 import functional as v2F\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler\n",
    "\n",
    "from ultralytics import YOLO\n",
    "from transformers import CLIPVisionModel, CLIPImageProcessor\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "import platform\n",
    "import gc\n",
    "import psutil\n",
    "import time\n",
    "\n",
    "# REMOVED: from mean_teacher import *\n",
    "from custom_logging import *\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "device\n",
    "\n",
    "def get_system_type():\n",
    "        system = platform.system()\n",
    "        \n",
    "        if system == \"Linux\":\n",
    "            if \"microsoft\" in platform.uname().release.lower() or \\\n",
    "            \"wsl\" in platform.uname().release.lower():\n",
    "                return \"wsl\"\n",
    "            return \"linux\"\n",
    "        elif system == \"Windows\":\n",
    "            return \"windows\"\n",
    "        else:\n",
    "            return \"other\"\n",
    "\n",
    "def get_num_workers():\n",
    "    \n",
    "    system_type = get_system_type()\n",
    "    if system_type == \"linux\":\n",
    "        return 8\n",
    "    elif system_type == \"windows\":\n",
    "        return 0\n",
    "    elif system_type == \"wsl\":\n",
    "        return 4\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "get_system_type(), get_num_workers()\n",
    "\n",
    "def get_gpu_memory_usage():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.cuda.memory_allocated() / 1024 / 1024  # MB\n",
    "    return 0\n",
    "\n",
    "def print_memory_usage():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    mem_info = process.memory_info()\n",
    "    print(f\"Main process RSS: {mem_info.rss / 1024**3:.2f} GB\")\n",
    "    \n",
    "    # Check worker processes\n",
    "    children = process.children()\n",
    "    for i, child in enumerate(children):\n",
    "        try:\n",
    "            child_mem = child.memory_info()\n",
    "            print(f\"Worker {i} RSS: {child_mem.rss / 1024**3:.2f} GB\")\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "def get_system_memory_usage():\n",
    "    return psutil.virtual_memory().percent\n",
    "\n",
    "# CHANGED: Removed \"mean_teacher\" from run name\n",
    "run_name = f\"sanitycheck_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "\n",
    "logger = Logger(log_dir=\"./logs\", experiment_name=run_name)\n",
    "\n",
    "class Config:\n",
    "    device = device\n",
    "    use_amp = True\n",
    "    batch_size = 40\n",
    "\n",
    "    num_classes = 3\n",
    "\n",
    "    initial_lr = 5e-5\n",
    "    lr_backbone = 1e-5\n",
    "    # REMOVED: consistency_weight (not needed without mean teacher)\n",
    "    # REMOVED: ema_decay (not needed without mean teacher)\n",
    "    # REMOVED: warmup_steps (not needed without mean teacher)\n",
    "\n",
    "    cur_epoch = 0\n",
    "    num_epochs = 4\n",
    "    freeze_until_epoch = 0\n",
    "\n",
    "    checkpoint_path = f\"./checkpoints/{run_name}\"\n",
    "    log_interval = 5\n",
    "    \n",
    "    # Create directories\n",
    "    os.makedirs(checkpoint_path, exist_ok=True)\n",
    "\n",
    "config = Config()\n",
    "\n",
    "logger.log_config(config)\n",
    "\n",
    "dry_run = None\n",
    "# dry_run = config.batch_size * 100\n",
    "\n",
    "class YOLOv11(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = YOLO(\"yolov11l-face.pt\").model\n",
    "        self.feature_model = torch.nn.Sequential(*list(self.model.model.children())[:10])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.feature_model(x)\n",
    "\n",
    "CLIP_PROCESSOR = CLIPImageProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "class CLIP(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.clip_model = CLIPVisionModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        self.clip_output_dim = self.clip_model.config.hidden_size \n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self.clip_model(**x)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        return pooled_output\n",
    "\n",
    "class ResNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.resnet = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "        self.feature_extractor = nn.Sequential(*list(self.resnet.children())[:-3])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        features = self.feature_extractor(x)\n",
    "        return features\n",
    "\n",
    "class BiggerClassifier(torch.nn.Module):\n",
    "    def __init__(self, output_dim=3):\n",
    "        super().__init__()\n",
    "        self.clip = CLIP()\n",
    "        self.yolo = YOLOv11()\n",
    "        self.resnet = ResNet()\n",
    "\n",
    "        self.yolo_pool = torch.nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        self.fc1 = torch.nn.Linear(768 + 512, 2048)\n",
    "        self.activation1 = torch.nn.GELU()\n",
    "        self.dropout1 = torch.nn.Dropout(0.3)\n",
    "        self.fc2 = torch.nn.Linear(2048, 1024)\n",
    "        self.activation2 = torch.nn.GELU()\n",
    "        self.dropout2 = torch.nn.Dropout(0.3)\n",
    "        self.fc3 = torch.nn.Linear(1024, output_dim)\n",
    "        \n",
    "    def forward(self, clip_inputs, img_tensor):\n",
    "        clip_features = self.clip(clip_inputs)\n",
    "        yolo_features = self.yolo(img_tensor)\n",
    "\n",
    "        yolo_features = self.yolo_pool(yolo_features).flatten(1)\n",
    "\n",
    "        combined_features = torch.cat([clip_features, yolo_features], dim=1)\n",
    "        \n",
    "        x = self.fc1(combined_features)\n",
    "        x = self.activation1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.activation2(x)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "to_tensor = transforms.Compose([\n",
    "    transforms.ToImage(), \n",
    "    transforms.ToDtype(torch.float32, scale=True),\n",
    "])\n",
    "\n",
    "yolo_intermediate_input_size = 700\n",
    "yolo_final_input_size = 640\n",
    "\n",
    "base_transform = transforms.Compose([\n",
    "    to_tensor,\n",
    "    transforms.Resize(size=yolo_intermediate_input_size, interpolation=transforms.InterpolationMode.BILINEAR, antialias=True),\n",
    "    transforms.RandomRotation(degrees=(-15, 15), interpolation=transforms.InterpolationMode.BILINEAR, expand=True, fill=0),\n",
    "    transforms.RandomCrop(yolo_final_input_size),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "])\n",
    "\n",
    "# CHANGED: Only keeping weak augmentation (removed strong augmentation)\n",
    "yolo_weak_transform = transforms.Compose([    \n",
    "    transforms.ColorJitter(brightness=0.05, contrast=0.05, saturation=0.05, hue=0.02),\n",
    "    transforms.Lambda(lambda x: x + torch.randn_like(x) * 0.005), \n",
    "    transforms.Lambda(lambda x: torch.clamp(x, 0, 1)),\n",
    "])\n",
    "# REMOVED: yolo_strong_transform\n",
    "\n",
    "clip_base_transform = transforms.Compose([ \n",
    "    transforms.Resize(size=224, interpolation=transforms.InterpolationMode.BILINEAR, antialias=True),\n",
    "])\n",
    "\n",
    "# CHANGED: Only keeping weak augmentation (removed strong augmentation)\n",
    "clip_weak_transform = transforms.Compose([    \n",
    "    transforms.ColorJitter(brightness=0.05, contrast=0.05, saturation=0.05, hue=0.02),\n",
    "    transforms.Lambda(lambda x: x + torch.randn_like(x) * 0.005), \n",
    "    transforms.Lambda(lambda x: torch.clamp(x, 0, 1)),\n",
    "])\n",
    "# REMOVED: clip_strong_transform\n",
    "\n",
    "yolo_val_transform = transforms.Compose([\n",
    "    to_tensor,\n",
    "    transforms.Resize(size=yolo_intermediate_input_size, interpolation=transforms.InterpolationMode.BILINEAR, antialias=True),\n",
    "    transforms.CenterCrop(yolo_final_input_size), \n",
    "    transforms.Lambda(lambda x: torch.clamp(x, 0, 1)),\n",
    "])\n",
    "\n",
    "clip_val_transform = transforms.Compose([ \n",
    "    to_tensor,\n",
    "    transforms.Resize(size=256, interpolation=transforms.InterpolationMode.BILINEAR, antialias=True),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.Lambda(lambda x: torch.clamp(x, 0, 1)),\n",
    "])\n",
    "\n",
    "\n",
    "bad_images = []\n",
    "\n",
    "# CHANGED: Simplified dataset to only return single augmented version\n",
    "class SingleModelDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, val=False, supervised=False, supervised_ratio=0.5, upsample=None):\n",
    "        self.root_dir = os.path.expanduser(root_dir)\n",
    "        self.annotations = pd.read_csv(os.path.join(self.root_dir, csv_file))\n",
    "        if dry_run:\n",
    "            self.annotations = self.annotations.sample(n=min(dry_run, len(self.annotations)), random_state=42)\n",
    "            self.annotations = self.annotations.reset_index(drop=True)\n",
    "        if supervised:\n",
    "            labeled = self.annotations[self.annotations['label'].isin([0, 2])]\n",
    "            unlabeled = self.annotations[self.annotations['label'] == 1]\n",
    "            n_labeled = len(labeled) \n",
    "            total_len = int(n_labeled / supervised_ratio)\n",
    "            target_len = total_len - n_labeled\n",
    "            if len(unlabeled) > target_len:\n",
    "                unlabeled = unlabeled.sample(n=target_len)\n",
    "            else: \n",
    "                print(f\"not enough unlabeled. asked for {target_len}, only have {len(unlabeled)}\")\n",
    "            self.annotations = pd.concat([labeled, unlabeled], ignore_index=True)\n",
    "        if upsample:\n",
    "            labeled = self.annotations[self.annotations['label'].isin([0, 2])].copy()\n",
    "            unlabeled = self.annotations[self.annotations['label'] == 1].copy()\n",
    "            n_labeled = len(labeled)\n",
    "            n_unlabeled = len(unlabeled)\n",
    "            print(f\"Upsampling labeled data: {n_labeled} samples × {upsample} = {n_labeled * upsample}\")\n",
    "            print(f\"Unlabeled data: {n_unlabeled} samples\")\n",
    "            labeled_copies = []\n",
    "            for i in range(upsample):\n",
    "                labeled_copy = labeled.copy()\n",
    "                labeled_copy['unique_id'] = labeled_copy['unique_id'].astype(str) + f'_copy{i}'\n",
    "                labeled_copies.append(labeled_copy)\n",
    "            labeled_upsampled = pd.concat(labeled_copies, ignore_index=True)\n",
    "            self.annotations = pd.concat([labeled_upsampled, unlabeled], ignore_index=True)\n",
    "            self.annotations = self.annotations.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "            print(f\"Final dataset size: {len(self.annotations)} samples\")\n",
    "            print(f\"Labeled ratio: {len(labeled_upsampled) / len(self.annotations):.2%}\")\n",
    "        \n",
    "        self.transform_times = []\n",
    "        self.val = val\n",
    "\n",
    "        self.error_log_path = f'dataset_errors_{\"val\" if val else \"train\"}.log'\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "    \n",
    "    def get_id(self, idx):\n",
    "        return self.annotations.at[idx, 'unique_id']\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            # 1. process metadata\n",
    "            original_label = self.annotations.iloc[idx, 1]\n",
    "            label = torch.tensor(original_label, dtype=torch.long)\n",
    "            \n",
    "            img_path = os.path.join(self.root_dir, self.annotations.iloc[idx, 0])\n",
    "            img_path = img_path.replace('\\\\', '/')\n",
    "            image_pil = Image.open(img_path).convert('RGB')\n",
    "\n",
    "            metadata = {\n",
    "                'img_path': img_path,\n",
    "                'label': original_label,\n",
    "                'width': self.annotations.iloc[idx, 2],\n",
    "                'height': self.annotations.iloc[idx, 3],\n",
    "                'size_kb': self.annotations.iloc[idx, 4],\n",
    "                'source': str(self.annotations.iloc[idx, 5]),\n",
    "            }\n",
    "\n",
    "            # 2. process images \n",
    "            # CHANGED: Only return single augmented version (weak augmentation)\n",
    "            if not self.val:\n",
    "                item_transform_start = time.time()\n",
    "                \n",
    "                base_image = base_transform(image_pil)\n",
    "                \n",
    "                # YOLO branch - only weak augmentation\n",
    "                yolo_image = yolo_weak_transform(base_image)\n",
    "                \n",
    "                # CLIP branch - only weak augmentation\n",
    "                clip_base_image = clip_base_transform(base_image)\n",
    "                clip_image = clip_weak_transform(clip_base_image)\n",
    "\n",
    "                clip_image = CLIP_PROCESSOR(images=clip_image, return_tensors=\"pt\", do_rescale=False)\n",
    "                clip_image['pixel_values'] = clip_image['pixel_values'].squeeze(0)\n",
    "\n",
    "                item_transform_time = time.time() - item_transform_start\n",
    "                if len(self.transform_times) < 1000:\n",
    "                    self.transform_times.append(item_transform_time)\n",
    "\n",
    "                # CHANGED: Return single version instead of weak/strong pairs\n",
    "                return clip_image, yolo_image, label, metadata\n",
    "\n",
    "            else:  # for validation\n",
    "                yolo_image = yolo_val_transform(image_pil)\n",
    "\n",
    "                clip_image = clip_val_transform(image_pil)\n",
    "                clip_image = CLIP_PROCESSOR(images=clip_image, return_tensors=\"pt\", do_rescale=False)\n",
    "                clip_image['pixel_values'] = clip_image['pixel_values'].squeeze(0)\n",
    "                return clip_image, yolo_image, label, metadata\n",
    "            \n",
    "        except Exception as e:\n",
    "            with open(os.path.join(logger.get_log_dir(), self.error_log_path), 'a') as f:\n",
    "                f.write(f\"Error loading image at index {idx}: {e}\\n\\n\")\n",
    "\n",
    "            dummy_clip_input = {'pixel_values': torch.zeros(3, 224, 224)}\n",
    "            dummy_yolo_tensor = torch.zeros(3, 640, 640)\n",
    "            dummy_label = torch.tensor(1, dtype=torch.long)\n",
    "            dummy_metadata = {\n",
    "                'img_path': f'error_at_idx_{idx}',\n",
    "                'label': 1,\n",
    "                'width': 640,\n",
    "                'height': 640,\n",
    "                'size_kb': 0.0,\n",
    "                'source': -1,\n",
    "            }\n",
    "            \n",
    "            return dummy_clip_input, dummy_yolo_tensor, dummy_label, dummy_metadata\n",
    "\n",
    "class RandomVersionSampler(Sampler):\n",
    "    def __init__(self, base_dataset):\n",
    "        self.base_dataset = base_dataset\n",
    "        \n",
    "        self.grouped = {}\n",
    "        for idx in range(len(base_dataset)):\n",
    "            img_id = base_dataset.get_id(idx)\n",
    "            if img_id not in self.grouped:\n",
    "                self.grouped[img_id] = []\n",
    "            self.grouped[img_id].append(idx)\n",
    "        self.ids = list(self.grouped.keys())\n",
    "\n",
    "    def __iter__(self):\n",
    "        chosen_indices = [random.choice(self.grouped[img_id]) for img_id in self.ids]\n",
    "        random.shuffle(chosen_indices)\n",
    "        return iter(chosen_indices)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "\n",
    "from torch.utils.data._utils.collate import default_collate\n",
    "\n",
    "# CHANGED: Simplified collate function for single model\n",
    "def custom_collate_fn(batch):\n",
    "    clip_inputs = default_collate([item[0] for item in batch])\n",
    "    yolo_tensors = default_collate([item[1] for item in batch])\n",
    "    labels = default_collate([item[2] for item in batch])\n",
    "    \n",
    "    metadata_list = [item[3] for item in batch]\n",
    "    \n",
    "    metadata = {\n",
    "        'img_path': [m['img_path'] for m in metadata_list],\n",
    "        'label': [m['label'] for m in metadata_list],\n",
    "        'width': [m['width'] for m in metadata_list],\n",
    "        'height': [m['height'] for m in metadata_list],\n",
    "        'size_kb': [m['size_kb'] for m in metadata_list],\n",
    "        'source': [m['source'] for m in metadata_list],\n",
    "    }\n",
    "    \n",
    "    return clip_inputs, yolo_tensors, labels, metadata\n",
    "\n",
    "# CHANGED: Use SingleModelDataset instead of MeanTeacherDataset\n",
    "supervised_train_dataset = SingleModelDataset(\n",
    "    csv_file = \"train_2.csv\", \n",
    "    root_dir = \"~/Workspace/data-v2/train\",\n",
    "    supervised = True,\n",
    ")\n",
    "train_dataset = SingleModelDataset(\n",
    "    csv_file = \"train_2.csv\", \n",
    "    root_dir = \"~/Workspace/data-v2/train\",\n",
    "    val = False,\n",
    ")\n",
    "val_dataset = SingleModelDataset(\n",
    "    csv_file = \"val_2.csv\", \n",
    "    root_dir = \"~/Workspace/data-v2/val\",\n",
    "    val = True,\n",
    ")\n",
    "\n",
    "supervised_sampler = RandomVersionSampler(supervised_train_dataset)\n",
    "supervised_train_dataloader = DataLoader(\n",
    "    supervised_train_dataset, \n",
    "    batch_size=config.batch_size, \n",
    "    sampler=supervised_sampler,\n",
    "    collate_fn=custom_collate_fn,\n",
    "    num_workers=get_num_workers(),\n",
    "    persistent_workers=False,\n",
    "    prefetch_factor=2 if get_num_workers() > 0 else None,\n",
    ")\n",
    "\n",
    "sampler = RandomVersionSampler(train_dataset)\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=config.batch_size, \n",
    "    sampler=sampler,\n",
    "    collate_fn=custom_collate_fn,\n",
    "    num_workers=get_num_workers(),\n",
    "    persistent_workers=False,\n",
    "    prefetch_factor=2 if get_num_workers() > 0 else None,\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=config.batch_size, \n",
    "    collate_fn=custom_collate_fn,\n",
    "    num_workers=get_num_workers(),\n",
    "    persistent_workers=False,\n",
    "    prefetch_factor=3 if get_num_workers() > 0 else None,\n",
    ")\n",
    "\n",
    "len(train_dataset) - len(train_dataloader) * config.batch_size\n",
    "\n",
    "len(supervised_train_dataloader), len(train_dataloader)\n",
    "\n",
    "\n",
    "\n",
    "# CHANGED: Only create single model (no teacher model)\n",
    "model = BiggerClassifier().to(config.device)\n",
    "# REMOVED: teacher_model creation and EMA setup\n",
    "\n",
    "clip_params = []\n",
    "yolo_params = []\n",
    "resnet_params = []\n",
    "classifier_params = []\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if 'clip' in name:\n",
    "        clip_params.append(param)\n",
    "    elif 'yolo' in name:\n",
    "        yolo_params.append(param)\n",
    "    elif 'resnet' in name:\n",
    "        resnet_params.append(param)\n",
    "    else:\n",
    "        classifier_params.append(param)\n",
    "\n",
    "print(\"Freezing backbone parameters initially...\")\n",
    "for param in clip_params + yolo_params:\n",
    "    param.requires_grad = False\n",
    "\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {'params': clip_params, 'lr': config.lr_backbone*0.1, 'name': 'clip'},\n",
    "    {'params': yolo_params, 'lr': config.lr_backbone*0.5, 'name': 'yolo'},\n",
    "    {'params': resnet_params, 'lr': config.lr_backbone, 'name': 'resnet'},\n",
    "    {'params': classifier_params, 'lr': config.initial_lr, 'name': 'classifier'}\n",
    "])\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.7, patience=1)\n",
    "print('scheduler lr:', scheduler.get_last_lr())\n",
    "\n",
    "\n",
    "class AsymmetricFocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2.0, alpha=None, confusion_penalty_matrix=None):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        if confusion_penalty_matrix is None:\n",
    "            confusion_penalty_matrix = torch.tensor([\n",
    "                [1.0, 1.0, 1.0],\n",
    "                [1.0, 1.0, 1.0],\n",
    "                [1.0, 1.0, 1.0]\n",
    "            ])\n",
    "        self.confusion_penalty_matrix = confusion_penalty_matrix\n",
    "        \n",
    "    def forward(self, logits, targets):\n",
    "        ce_loss = F.cross_entropy(logits, targets, reduction='none')\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        p_t = probs.gather(1, targets.view(-1, 1)).squeeze(1)\n",
    "        \n",
    "        focal_weight = (1 - p_t) ** self.gamma\n",
    "\n",
    "        penalty_for_true = self.confusion_penalty_matrix[targets]\n",
    "        expected_penalty = (probs * penalty_for_true).sum(dim=1)\n",
    "\n",
    "        loss = focal_weight * ce_loss * expected_penalty\n",
    "\n",
    "        if self.alpha is not None:\n",
    "            alpha_t = self.alpha.gather(0, targets)\n",
    "            loss = alpha_t * loss\n",
    "            \n",
    "        return loss.mean()\n",
    "\n",
    "\n",
    "ce_criterion = nn.CrossEntropyLoss().to(config.device)\n",
    "\n",
    "af_criterion = AsymmetricFocalLoss(\n",
    "    gamma=1.2,\n",
    "    alpha=torch.tensor([1.1, 0.9, 1.2]).to(config.device),  \n",
    "    confusion_penalty_matrix=torch.tensor([\n",
    "        [1.0, 1.1, 1.2], \n",
    "        [0.85, 1.0, 0.85],\n",
    "        [1.15, 1.1, 1.0]\n",
    "    ]).to(config.device)\n",
    ").to(config.device)\n",
    "\n",
    "# CHANGED: Simplified PerformanceMonitor (removed consistency tracking)\n",
    "class PerformanceMonitor:\n",
    "    def __init__(self):\n",
    "        self.best_accuracy = 0.0\n",
    "        self.epochs_without_improvement = 0\n",
    "        self.accuracy_history = []\n",
    "\n",
    "monitor = PerformanceMonitor()\n",
    "\n",
    "# CHANGED: Simplified checkpoint saving (removed teacher model)\n",
    "def save_checkpoint(model, optimizer, scheduler, epoch, global_step, config, \n",
    "                    val_accuracy, monitor, loss_history):\n",
    "\n",
    "    checkpoint_dir = config.checkpoint_path\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    filename = f\"checkpoint_epoch{epoch}_step{global_step}_acc{val_accuracy:.4f}_{timestamp}.pth\"\n",
    "    \n",
    "    checkpoint_path = os.path.join(checkpoint_dir, filename)\n",
    "    \n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'global_step': global_step,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        # REMOVED: teacher_state_dict\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'val_accuracy': val_accuracy,\n",
    "        'loss_history': loss_history,\n",
    "        'monitor_state': {\n",
    "            'best_accuracy': monitor.best_accuracy,\n",
    "            'epochs_without_improvement': monitor.epochs_without_improvement,\n",
    "            'accuracy_history': monitor.accuracy_history,\n",
    "        },\n",
    "        'config': {k: v for k, v in vars(config).items() if not k.startswith('_')},\n",
    "        'save_timestamp': datetime.now().isoformat(),\n",
    "        'pytorch_version': torch.__version__,\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        torch.save(checkpoint, checkpoint_path)\n",
    "        print(f\"Checkpoint saved successfully: {checkpoint_path}\")\n",
    "        print(f\"Epoch: {epoch} | Step: {global_step} | Val Acc: {val_accuracy:.4f}\")\n",
    "        return checkpoint_path\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving checkpoint: {e}\")\n",
    "        raise\n",
    "\n",
    "# CHANGED: Simplified checkpoint loading (removed teacher model)\n",
    "def load_checkpoint(checkpoint_path, model, optimizer, scheduler, config, \n",
    "                    monitor, device='cuda', strict=True):\n",
    "\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        raise FileNotFoundError(f\"Checkpoint not found: {checkpoint_path}\")\n",
    "    print(f\"Loading checkpoint from: {checkpoint_path}\")\n",
    "    \n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    \n",
    "    model.load_state_dict(checkpoint['model_state_dict'], strict=strict)\n",
    "    # REMOVED: teacher model loading\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    \n",
    "    epoch = checkpoint['epoch']\n",
    "    global_step = checkpoint['global_step']\n",
    "    val_accuracy = checkpoint['val_accuracy']\n",
    "    loss_history = checkpoint['loss_history']\n",
    "    \n",
    "    monitor_state = checkpoint['monitor_state']\n",
    "    monitor.best_accuracy = monitor_state['best_accuracy']\n",
    "    monitor.accuracy_history = monitor_state['accuracy_history']\n",
    "    \n",
    "    saved_config = checkpoint['config']\n",
    "    for key, value in saved_config.items():\n",
    "        if hasattr(config, key) and key not in ['checkpoint_path', 'log_dir']:\n",
    "            setattr(config, key, value)\n",
    "    \n",
    "    config.cur_epoch = epoch + 1\n",
    "    \n",
    "    print(f\"Checkpoint loaded successfully\")\n",
    "    print(f\"Resuming from Epoch: {epoch + 1} | Step: {global_step}\")\n",
    "    print(f\"Previous Val Acc: {val_accuracy:.4f} | Best Acc: {monitor.best_accuracy:.4f}\")\n",
    "    print(f\"Loaded len(loss_history) = {len(loss_history)}\")\n",
    "    \n",
    "    return epoch, global_step, loss_history, val_accuracy\n",
    "\n",
    "def validate(model, val_loader, config, epoch):\n",
    "    torch.cuda.empty_cache()\n",
    "    model.eval()\n",
    "\n",
    "    image_data = []\n",
    "    batch_data = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_pbar = tqdm(val_loader, desc=f'Validation Epoch {epoch}', total=len(val_loader), leave=False)\n",
    "\n",
    "        # CHANGED: Updated to match new data structure\n",
    "        for batch_num, (clip_inputs, yolo_tensors, labels, metadata) in enumerate(val_pbar):\n",
    "            clip_inputs['pixel_values'] = clip_inputs['pixel_values'].to(config.device)\n",
    "            yolo_tensors = yolo_tensors.to(config.device)\n",
    "            labels = labels.to(config.device)\n",
    "\n",
    "            outputs = model(clip_inputs, yolo_tensors)\n",
    "            loss = af_criterion(outputs, labels).cpu().numpy()\n",
    "            probs = F.softmax(outputs, dim=1).cpu().numpy()\n",
    "            predictions = outputs.argmax(dim=1).cpu().numpy()\n",
    "            labels = labels.cpu().numpy()\n",
    "\n",
    "            batch_data.append({\n",
    "                'batch_num': batch_num,\n",
    "                'epoch': epoch,\n",
    "                'loss': loss.item()\n",
    "            })\n",
    "\n",
    "            for i in range(len(labels)):\n",
    "                true_label = labels[i]\n",
    "                pred_label = predictions[i]\n",
    "\n",
    "                image_data.append({\n",
    "                    'batch_num': batch_num,\n",
    "                    'img_path': metadata['img_path'][i],\n",
    "                    'label': int(true_label),\n",
    "                    'width': int(metadata['width'][i]),\n",
    "                    'height': int(metadata['height'][i]),\n",
    "                    'size_kb': float(metadata['size_kb'][i]),\n",
    "                    'source': metadata['source'][i],\n",
    "                    'prediction': int(pred_label),\n",
    "                    'bad': float(probs[i, 0]),\n",
    "                    'neutral': float(probs[i, 1]),\n",
    "                    'good': float(probs[i, 2])\n",
    "                })\n",
    "\n",
    "            val_pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    image_df = pd.DataFrame(image_data)\n",
    "    batch_df = pd.DataFrame(batch_data)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    return image_df, batch_df, total_loss\n",
    "\n",
    "def analyse(image_df, batch_df, config, monitor, epoch):\n",
    "    # Calculate metrics from dataframe\n",
    "    labeled_df = image_df[image_df['label'] != 1]\n",
    "    total = len(labeled_df)\n",
    "    correct = (labeled_df['label'] == labeled_df['prediction']).sum()\n",
    "    accuracy = correct / total\n",
    "\n",
    "    # Calculate per-class metrics\n",
    "    class_names = ['bad', 'neutral', 'good']\n",
    "    labels=[0, 1, 2]\n",
    "    all_labels = image_df['label'].values\n",
    "    all_preds = image_df['prediction'].values\n",
    "\n",
    "    # Confusion matrix and classification report\n",
    "    cm = confusion_matrix(y_true=all_labels, y_pred=all_preds, labels=labels)\n",
    "    report = classification_report(y_true=all_labels, y_pred=all_preds, labels=labels, target_names=class_names,\n",
    "                                   output_dict=True, zero_division=0)\n",
    "\n",
    "    # Calculate per-class recall\n",
    "    class_recall = []\n",
    "    for class_idx in range(config.num_classes):\n",
    "        class_mask = image_df['label'] == class_idx\n",
    "        if class_mask.sum() > 0:\n",
    "            class_correct = ((image_df['label'] == class_idx) &\n",
    "                           (image_df['prediction'] == class_idx)).sum()\n",
    "            class_recall.append(class_correct / class_mask.sum())\n",
    "        else:\n",
    "            class_recall.append(0.0)\n",
    "\n",
    "    # Compile metrics dictionary\n",
    "    avg_val_loss = batch_df['loss'].mean()\n",
    "\n",
    "    metrics = {\n",
    "        \"val/accuracy\": accuracy,\n",
    "        \"val/loss\": avg_val_loss,\n",
    "        \"val/recall_bad\": class_recall[0],\n",
    "        \"val/recall_neutral\": class_recall[1],\n",
    "        \"val/recall_good\": class_recall[2],\n",
    "    }\n",
    "\n",
    "    # Add precision and F1 scores from classification report\n",
    "    for class_name in class_names:\n",
    "        if class_name in report:\n",
    "            metrics[f\"val/precision_{class_name}\"] = report[class_name]['precision']\n",
    "            metrics[f\"val/f1_{class_name}\"] = report[class_name]['f1-score']\n",
    "\n",
    "    # Add severe misclassification counts\n",
    "    good_as_bad = image_df[(image_df['label'] == 2) & (image_df['prediction'] == 0)]\n",
    "    bad_as_good = image_df[(image_df['label'] == 0) & (image_df['prediction'] == 2)]\n",
    "    metrics[\"val/bad_as_good_count\"] = len(bad_as_good)\n",
    "    metrics[\"val/good_as_bad_count\"] = len(good_as_bad)\n",
    "\n",
    "    # Update monitor\n",
    "    monitor.accuracy_history.append(accuracy)\n",
    "    if accuracy > monitor.best_accuracy:\n",
    "        monitor.best_accuracy = accuracy\n",
    "        monitor.epochs_without_improvement = 0\n",
    "    else:\n",
    "        monitor.epochs_without_improvement += 1\n",
    "\n",
    "    print(f\"Validation Summary - Epoch {epoch}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f} | Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    return accuracy, metrics, cm\n",
    "\n",
    "\n",
    "scaler = GradScaler()\n",
    "\n",
    "# CHANGED: Simplified epoch analysis (removed consistency loss tracking)\n",
    "def analyse_epoch(image_df, batch_df, config, epoch):\n",
    "\n",
    "    avg_loss = batch_df['loss'].mean()\n",
    "    # REMOVED: avg_cls_loss and avg_consistency_loss tracking\n",
    "    \n",
    "    metrics = {\n",
    "        \"epoch/loss\": avg_loss,\n",
    "        # REMOVED: consistency-related metrics\n",
    "    }\n",
    "    \n",
    "    if image_df is not None:\n",
    "        total_samples = len(image_df)\n",
    "        correct = (image_df['label'] == image_df['prediction']).sum()\n",
    "        train_accuracy = correct / total_samples\n",
    "        \n",
    "        # Per-class accuracy\n",
    "        for class_idx in range(config.num_classes):\n",
    "            class_mask = image_df['label'] == class_idx\n",
    "            if class_mask.sum() > 0:\n",
    "                class_correct = ((image_df['label'] == class_idx) & \n",
    "                               (image_df['prediction'] == class_idx)).sum()\n",
    "                class_acc = class_correct / class_mask.sum()\n",
    "                metrics[f\"epoch/accuracy_class_{class_idx}\"] = class_acc\n",
    "        \n",
    "        metrics[\"epoch/train_accuracy\"] = train_accuracy\n",
    "        \n",
    "        # Severe misclassifications\n",
    "        good_as_bad = image_df[(image_df['label'] == 2) & (image_df['prediction'] == 0)]\n",
    "        bad_as_good = image_df[(image_df['label'] == 0) & (image_df['prediction'] == 2)]\n",
    "        metrics[\"epoch/bad_as_good_count\"] = len(bad_as_good)\n",
    "        metrics[\"epoch/good_as_bad_count\"] = len(good_as_bad)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def plot_running_loss(loss_history, save_path, window_size=10):\n",
    "    \"\"\"\n",
    "    Plot running loss with moving average and save to file\n",
    "    \"\"\"\n",
    "    if len(loss_history) < window_size:\n",
    "        return\n",
    "    \n",
    "    # Calculate moving average\n",
    "    moving_avg = []\n",
    "    for i in range(window_size - 1, len(loss_history)):\n",
    "        window = loss_history[i - window_size + 1:i + 1]\n",
    "        moving_avg.append(sum(window) / window_size)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    x_raw = [i * config.log_interval for i in range(len(loss_history))]\n",
    "    x_moving = [i * config.log_interval for i in range(window_size - 1, len(loss_history))]\n",
    "\n",
    "    # Plot raw loss in light color\n",
    "    plt.plot(x_raw, loss_history, alpha=0.3, color='blue', label='Raw Loss')\n",
    "    \n",
    "    # Plot moving average in bold\n",
    "    plt.plot(x_moving, moving_avg, color='red', linewidth=2, label=f'Moving Avg (window={window_size})')\n",
    "    plt.xlabel('Batch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss Over Time')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add statistics\n",
    "    if moving_avg:\n",
    "        current_avg = moving_avg[-1]\n",
    "        min_avg = min(moving_avg)\n",
    "        plt.axhline(y=current_avg, color='green', linestyle='--', alpha=0.5, label=f'Current: {current_avg:.4f}')\n",
    "        plt.axhline(y=min_avg, color='orange', linestyle='--', alpha=0.5, label=f'Min: {min_avg:.4f}')\n",
    "                \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=100)\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"  Loss graph saved to: {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f659730",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.cur_epoch = 4\n",
    "config.num_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6539b8d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.66 13271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4:   4%|▍         | 500/13271 [03:32<1:24:51,  2.51it/s, loss=0.2256, img/s=110.6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unfreezing backbone at step 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4:  31%|███       | 4055/13271 [28:01<1:04:53,  2.37it/s, loss=0.2070, img/s=109.4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running intermittent validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4:  31%|███       | 4056/13271 [31:43<172:05:30, 67.23s/it, loss=0.2070, img/s=109.4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Summary - Epoch step4055\n",
      "Accuracy: 0.4600 | Loss: 0.8823\n",
      "scheduler lr: [2.8247524899999995e-08, 9.886633714999994e-08, 1.9773267429999988e-07, 9.886633714999995e-07]\n",
      "  Validation Accuracy: 0.4600\n",
      "  Best Accuracy: 0.5033\n",
      "  Loss graph saved to: ./logs/sanitycheck_20251030_013129/loss_graph.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4:  61%|██████    | 8110/13271 [59:33<35:01,  2.46it/s, loss=0.5156, img/s=109.6]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running intermittent validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4:  61%|██████    | 8111/13271 [1:03:32<103:22:52, 72.13s/it, loss=0.5156, img/s=109.6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Summary - Epoch step8110\n",
      "Accuracy: 0.4554 | Loss: 0.8821\n",
      "scheduler lr: [2.8247524899999995e-08, 6.920643600499995e-08, 1.384128720099999e-07, 6.920643600499996e-07]\n",
      "  Validation Accuracy: 0.4554\n",
      "  Best Accuracy: 0.5033\n",
      "  Loss graph saved to: ./logs/sanitycheck_20251030_013129/loss_graph.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4:  92%|█████████▏| 12165/13271 [1:31:27<07:12,  2.56it/s, loss=0.1865, img/s=106.3]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running intermittent validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4:  92%|█████████▏| 12166/13271 [1:35:16<21:12:38, 69.10s/it, loss=0.1865, img/s=106.3]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Summary - Epoch step12165\n",
      "Accuracy: 0.4608 | Loss: 0.8802\n",
      "scheduler lr: [2.8247524899999995e-08, 6.920643600499995e-08, 1.384128720099999e-07, 6.920643600499996e-07]\n",
      "  Validation Accuracy: 0.4608\n",
      "  Best Accuracy: 0.5033\n",
      "  Loss graph saved to: ./logs/sanitycheck_20251030_013129/loss_graph.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 13271/13271 [1:42:48<00:00,  2.15it/s, loss=0.1953, img/s=362.8]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Summary:\n",
      "  Time: 6170.7s\n",
      "  Avg Loss: 0.2208\n",
      "================================================================================\n",
      "Running end of epoch validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Summary - Epoch 4\n",
      "Accuracy: 0.4527 | Loss: 0.8883\n",
      "  Validation Accuracy: 0.4527\n",
      "  Best Accuracy: 0.5033\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Checkpoint saved successfully: ./checkpoints/sanitycheck_20251030_013129/checkpoint_epoch4_step13271_acc0.4527_20251030_160832.pth\n",
      "Epoch: 4 | Step: 13271 | Val Acc: 0.4527\n",
      "scheduler lr: [2.8247524899999995e-08, 4.844450520349996e-08, 9.688901040699992e-08, 4.844450520349997e-07]\n",
      "  Loss graph saved to: ./logs/sanitycheck_20251030_013129/loss_graph.png\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "criterion = ce_criterion\n",
    "validation_frequency = len(train_dataloader) // 7\n",
    "validation_frequency = validation_frequency - (validation_frequency % config.log_interval)\n",
    "validation_frequency\n",
    "\n",
    "global_step = 0\n",
    "track_images = False\n",
    "loss_history = []\n",
    "# REMOVED: cls_loss_history (no separate classification loss tracking)\n",
    "\n",
    "# monitor mode collapse\n",
    "recent_predictions = [] \n",
    "max_recent_batches = 15\n",
    "\n",
    "if global_step >= 500:\n",
    "    print(f\"unfreezing backbone at step {global_step}\")\n",
    "    for param in clip_params + yolo_params:\n",
    "        param.requires_grad = True\n",
    "\n",
    "for epoch in range(config.cur_epoch, config.num_epochs):\n",
    "    collapse_flag = False\n",
    "    epoch_start = time.time()\n",
    "\n",
    "    # region Train one epoch ########################################\n",
    "    model.train()\n",
    "    # REMOVED: teacher_model.train()\n",
    "    \n",
    "    batch_data = []\n",
    "\n",
    "    if epoch <= 5:\n",
    "        ratios = [0.66, 0.55, 0.44, 0.33, 0.66]\n",
    "        # CHANGED: Use SingleModelDataset instead of MeanTeacherDataset\n",
    "        supervised_train_dataset = SingleModelDataset(\n",
    "            csv_file = \"train_2.csv\", \n",
    "            root_dir = \"~/Workspace/data-v2/train\",\n",
    "            supervised = True,\n",
    "            supervised_ratio = ratios[epoch],\n",
    "            # upsample = 3,\n",
    "        )\n",
    "        supervised_sampler = RandomVersionSampler(supervised_train_dataset)\n",
    "        train_loader = DataLoader(\n",
    "            supervised_train_dataset, \n",
    "            batch_size=config.batch_size, \n",
    "            sampler=supervised_sampler,\n",
    "            collate_fn=custom_collate_fn,\n",
    "            num_workers=get_num_workers(),\n",
    "            persistent_workers=False,\n",
    "            prefetch_factor=3 if get_num_workers() > 0 else None,\n",
    "        )\n",
    "        print(ratios[epoch], len(train_loader))\n",
    "    else: \n",
    "        train_loader = train_dataloader\n",
    "    if epoch >= 1:\n",
    "        criterion = af_criterion\n",
    "    else:\n",
    "        criterion = ce_criterion\n",
    "    \n",
    "    epoch_pbar = tqdm(train_loader, desc=f'Epoch {epoch}', total=len(train_loader))\n",
    "    # CHANGED: Updated batch unpacking to match new data structure\n",
    "    for batch_num, (clip_inputs, yolo_tensors, labels, metadata) in enumerate(epoch_pbar):\n",
    "        if global_step == 500:\n",
    "            print(f\"unfreezing backbone at step {global_step}\")\n",
    "            for param in clip_params + yolo_params:\n",
    "                param.requires_grad = True\n",
    "                \n",
    "        batch_start = time.time()\n",
    "\n",
    "        # 1. Grab data from dataloader\n",
    "        # CHANGED: Only single version of images (no weak/strong split)\n",
    "        clip_inputs['pixel_values'] = clip_inputs['pixel_values'].to(device, non_blocking=True)\n",
    "        yolo_tensors = yolo_tensors.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "        \n",
    "        # 2. Forward pass \n",
    "        # CHANGED: Single forward pass only (no teacher model)\n",
    "        with autocast(device_type='cuda'):\n",
    "            outputs = model(clip_inputs, yolo_tensors)\n",
    "            loss = criterion(outputs, labels)\n",
    "            # REMOVED: consistency loss computation\n",
    "        \n",
    "        # 3. Backward pass\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # REMOVED: EMA teacher model update\n",
    "        \n",
    "        if global_step % config.log_interval != 0:\n",
    "            global_step += 1\n",
    "            continue\n",
    "        \n",
    "        # monitor mode collapse\n",
    "        with torch.no_grad():\n",
    "            current_predictions = F.softmax(outputs, dim=1).argmax(dim=1).cpu().numpy()\n",
    "        recent_predictions.append(current_predictions)\n",
    "        if len(recent_predictions) > max_recent_batches:\n",
    "            recent_predictions.pop(0)\n",
    "        all_recent_preds = np.concatenate(recent_predictions)\n",
    "        unique, counts = np.unique(all_recent_preds, return_counts=True)\n",
    "        dominant_class = unique[np.argmax(counts)]\n",
    "        dominant_ratio = counts.max() / len(all_recent_preds)\n",
    "        if collapse_flag==False and dominant_ratio > 0.95:\n",
    "            print(f\"[WARN] Possible mode collapse at step {global_step}!\")\n",
    "            print(f\"       Class {dominant_class} represents {dominant_ratio:.1%} of last {len(all_recent_preds)} predictions\")\n",
    "            print(f\"       Distribution: {dict(zip(unique, counts))}\")\n",
    "            collapse_flag = True\n",
    "\n",
    "        # 5. Calculate metrics #############################################\n",
    "        batch_time = time.time() - batch_start\n",
    "        images_per_second = config.batch_size / batch_time\n",
    "        \n",
    "        # Move to CPU for storage\n",
    "        loss_cpu = loss.item()\n",
    "        # REMOVED: separate cls_loss and consistency_loss tracking\n",
    "        \n",
    "        loss_history.append(loss_cpu)\n",
    "\n",
    "        # CHANGED: Simplified batch data (removed consistency tracking)\n",
    "        batch_data.append({\n",
    "            'batch_num': batch_num,\n",
    "            'epoch': epoch,\n",
    "            'global_step': global_step,\n",
    "            'loss': loss_cpu,\n",
    "        })\n",
    "        \n",
    "        # Update progress bar\n",
    "        # CHANGED: Simplified progress bar (removed consistency info)\n",
    "        epoch_pbar.set_postfix({\n",
    "            'loss': f'{loss_cpu:.4f}',\n",
    "            'img/s': f'{images_per_second:.1f}'\n",
    "        })\n",
    "        \n",
    "        # Log to tensorboard/CSV at intervals\n",
    "        if global_step % config.log_interval == 0:\n",
    "            current_lr = optimizer.param_groups[-1]['lr']\n",
    "            \n",
    "            # CHANGED: Removed consistency metrics\n",
    "            train_metrics = {\n",
    "                \"train/loss\": loss_cpu,\n",
    "                \"train/learning_rate\": current_lr,\n",
    "                \"system/gpu_memory_mb\": get_gpu_memory_usage()\n",
    "            }\n",
    "            \n",
    "            logger.log_metrics(train_metrics, global_step)\n",
    "            logger.log_train_step(global_step, epoch, {\n",
    "                'loss': loss_cpu,\n",
    "                'learning_rate': current_lr,\n",
    "            })\n",
    "\n",
    "        # 6. Intermittent validation\n",
    "        if global_step > 0 and global_step % validation_frequency == 0:\n",
    "            intermittent_epoch = \"step\" + str(global_step)\n",
    "            print(\"Running intermittent validation...\")\n",
    "            # CHANGED: Validate with single model (not teacher)\n",
    "            val_image_df, val_batch_df, val_loss = validate(model, val_dataloader, config, intermittent_epoch)\n",
    "            val_accuracy, val_metrics, cm = analyse(val_image_df, val_batch_df, config, monitor, intermittent_epoch)\n",
    "            scheduler.step(val_accuracy)\n",
    "            print('scheduler lr:', scheduler.get_last_lr())\n",
    "\n",
    "            logger.log_validation(intermittent_epoch, val_metrics)\n",
    "            logger.log_confusion_matrix(cm, ['bad', 'neutral', 'good'], intermittent_epoch)\n",
    "            \n",
    "            print(f\"  Validation Accuracy: {val_accuracy:.4f}\")\n",
    "            print(f\"  Best Accuracy: {monitor.best_accuracy:.4f}\")\n",
    "\n",
    "            if val_accuracy >= monitor.best_accuracy:    \n",
    "                # CHANGED: Simplified checkpoint saving (removed teacher model and cls_loss_history)\n",
    "                save_checkpoint(\n",
    "                    model, optimizer, scheduler, epoch, global_step, \n",
    "                    config, val_accuracy, monitor, loss_history\n",
    "                )\n",
    "            \n",
    "            plot_running_loss(loss_history, os.path.join(logger.get_log_dir(), f'loss_graph.png'))\n",
    "            # REMOVED: cls_loss_history plotting\n",
    "        global_step += 1\n",
    "\n",
    "    # Create dataframes\n",
    "    batch_df = pd.DataFrame(batch_data)\n",
    "\n",
    "    train_image_df = None\n",
    "    train_batch_df = batch_df\n",
    "\n",
    "    # endregion #####################################################\n",
    "    \n",
    "    # Analyze epoch\n",
    "    epoch_metrics = analyse_epoch(train_image_df, train_batch_df, config, epoch)\n",
    "    logger.log_metrics(epoch_metrics, global_step)\n",
    "    \n",
    "    print(f\"Epoch {epoch} Summary:\")\n",
    "    print(f\"  Time: {(time.time() - epoch_start):.1f}s\")\n",
    "    print(f\"  Avg Loss: {epoch_metrics['epoch/loss']:.4f}\")\n",
    "    # REMOVED: Consistency loss printing\n",
    "\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Validation\n",
    "    print(\"Running end of epoch validation...\")\n",
    "    # CHANGED: Validate with single model (not teacher)\n",
    "    val_image_df, val_batch_df, val_loss = validate(model, val_dataloader, config, epoch)\n",
    "    val_accuracy, val_metrics, cm = analyse(val_image_df, val_batch_df, config, monitor, epoch)\n",
    "    \n",
    "    logger.log_metrics(val_metrics, epoch)\n",
    "    logger.log_validation(epoch, val_metrics)\n",
    "    logger.log_confusion_matrix(cm, ['bad', 'neutral', 'good'], epoch)\n",
    "    \n",
    "    print(f\"  Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    print(f\"  Best Accuracy: {monitor.best_accuracy:.4f}\")\n",
    "    print(\"=\"*80)\n",
    "    print()\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # CHANGED: Simplified checkpoint saving\n",
    "    save_checkpoint(\n",
    "        model, optimizer, scheduler, epoch, global_step, \n",
    "        config, val_accuracy, monitor, loss_history\n",
    "    )\n",
    "    \n",
    "    scheduler.step(val_accuracy)\n",
    "    print('scheduler lr:', scheduler.get_last_lr())\n",
    "\n",
    "plot_running_loss(loss_history, os.path.join(logger.get_log_dir(), f'loss_graph.png'))\n",
    "# REMOVED: Separate cls_loss and cons_loss plotting\n",
    "logger.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "31208c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Summary - Epoch -1\n",
      "Accuracy: 0.4527 | Loss: 0.8883\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 5111,  5563,   733],\n",
       "       [ 6436, 15358,  1354],\n",
       "       [  511,  2090,  2248]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_image_df, val_batch_df, val_loss = validate(model, val_dataloader, config, -1)\n",
    "val_accuracy, val_metrics, cm = analyse(val_image_df, val_batch_df, config, monitor, -1)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "761a2b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# good_as_bad = val_image_df[(val_image_df['label'] == 2) & (val_image_df['prediction'] == 0)]\n",
    "# bad_as_good = val_image_df[(val_image_df['label'] == 0) & (val_image_df['prediction'] == 2)]\n",
    "\n",
    "# for s in bad_as_good[\"img_path\"]:\n",
    "#     print(s)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039e0496",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a60b44e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Summary - Epoch -1\n",
      "Accuracy: 0.3584 | Loss: 1.4201\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[12886, 13842,   786],\n",
       "       [16691, 38291,  3604],\n",
       "       [ 2390, 12738,  3736]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = SingleModelDataset(\n",
    "    csv_file = \"test_2.csv\", \n",
    "    root_dir = \"~/Workspace/data-v2/test\",\n",
    "    val = True,\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=config.batch_size, \n",
    "    collate_fn=custom_collate_fn,\n",
    "    num_workers=get_num_workers(),\n",
    "    persistent_workers=False, # True if get_num_workers() > 0 else False,\n",
    "    # pin_memory=False, # WSL does not support pin_memory well\n",
    "    prefetch_factor=3 if get_num_workers() > 0 else None,\n",
    ")\n",
    "test_image_df, test_batch_df, loss = validate(model, test_dataloader, config, -1)\n",
    "test_accuracy, test_metrics, cm = analyse(test_image_df, test_batch_df, config, monitor, -1)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "56779a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# good_as_bad = test_image_df[(test_image_df['label'] == 2) & (test_image_df['prediction'] == 0)]\n",
    "# bad_as_good = test_image_df[(test_image_df['label'] == 0) & (test_image_df['prediction'] == 2)]\n",
    "\n",
    "# for s in good_as_bad[\"img_path\"]:\n",
    "#     print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "517dc5f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved successfully: ./checkpoints/sanitycheck_20251030_013129/checkpoint_epoch5_step13271_acc0.4527_20251030_170239.pth\n",
      "Epoch: 5 | Step: 13271 | Val Acc: 0.4527\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./checkpoints/sanitycheck_20251030_013129/checkpoint_epoch5_step13271_acc0.4527_20251030_170239.pth'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_checkpoint(\n",
    "                    model, optimizer, scheduler, 5, global_step, \n",
    "                    config, val_accuracy, monitor, loss_history\n",
    "                )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "img-classifier",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
