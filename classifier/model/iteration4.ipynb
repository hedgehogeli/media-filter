{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb3e625f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "import torchvision.transforms.v2 as transforms\n",
    "from torchvision.transforms.v2 import functional as v2F\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler\n",
    "\n",
    "from ultralytics import YOLO\n",
    "from transformers import CLIPVisionModel, CLIPImageProcessor\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "import platform\n",
    "import gc\n",
    "import psutil\n",
    "import time\n",
    "\n",
    "# from models import *\n",
    "from custom_logging import *\n",
    "# from mean_teacher import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691d38e6",
   "metadata": {},
   "source": [
    "# Hardware/system stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a67b9496",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1f81511",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('linux', 10)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_system_type():\n",
    "        system = platform.system()\n",
    "        \n",
    "        if system == \"Linux\":\n",
    "            if \"microsoft\" in platform.uname().release.lower() or \\\n",
    "            \"wsl\" in platform.uname().release.lower():\n",
    "                return \"wsl\"\n",
    "            return \"linux\"\n",
    "        elif system == \"Windows\":\n",
    "            return \"windows\"\n",
    "        else:\n",
    "            return \"other\"\n",
    "\n",
    "def get_num_workers():\n",
    "    \n",
    "    system_type = get_system_type()\n",
    "    if system_type == \"linux\":\n",
    "        return 10\n",
    "    elif system_type == \"windows\":\n",
    "        return 0\n",
    "    elif system_type == \"wsl\":\n",
    "        return 4\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "get_system_type(), get_num_workers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fd72e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gpu_memory_usage():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.cuda.memory_allocated() / 1024 / 1024  # MB\n",
    "    return 0\n",
    "\n",
    "def print_memory_usage():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    mem_info = process.memory_info()\n",
    "    print(f\"Main process RSS: {mem_info.rss / 1024**3:.2f} GB\")\n",
    "    \n",
    "    # Check worker processes\n",
    "    children = process.children()\n",
    "    for i, child in enumerate(children):\n",
    "        try:\n",
    "            child_mem = child.memory_info()\n",
    "            print(f\"Worker {i} RSS: {child_mem.rss / 1024**3:.2f} GB\")\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "def get_system_memory_usage():\n",
    "    return psutil.virtual_memory().percent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f8b668",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc7d5b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to: ./logs/resnet_clip_yolo_mean_teacher_20260115_233447\n"
     ]
    }
   ],
   "source": [
    "run_name = f\"resnet_clip_yolo_mean_teacher_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "\n",
    "logger = Logger(log_dir=\"./logs\", experiment_name=run_name)\n",
    "\n",
    "class Config:\n",
    "    device = device\n",
    "    use_amp = True\n",
    "    batch_size = 40 # 20 # basically the max\n",
    "\n",
    "    num_classes = 3\n",
    "\n",
    "    initial_lr = 9e-5\n",
    "    lr_backbone = 3e-5\n",
    "    # consistency_weight = 2.2 # 0.6\n",
    "    ema_decay = 0.999\n",
    "\n",
    "    warmup_steps = 25000 # scaling (in num batches) from 0 to consistency_weight for consistency loss\n",
    "\n",
    "    cur_epoch = 0\n",
    "    num_epochs = 4\n",
    "    freeze_until_epoch = 0\n",
    "\n",
    "    checkpoint_path = f\"./checkpoints/{run_name}\"\n",
    "    log_interval = 5\n",
    "    \n",
    "    # Create directories\n",
    "    os.makedirs(checkpoint_path, exist_ok=True)\n",
    "\n",
    "config = Config()\n",
    "\n",
    "logger.log_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4442ae51",
   "metadata": {},
   "outputs": [],
   "source": [
    "dry_run = None # set to None if not a dry run, set to desired num of rows if dry run\n",
    "# dry_run = config.batch_size * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffe78cd",
   "metadata": {},
   "source": [
    "# MEAN TEACHER UTILS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61857c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_ema_variables(model, ema_model, alpha, global_step):\n",
    "    \"\"\"Update EMA variables with warmup\"\"\"\n",
    "    with torch.no_grad():\n",
    "        for s_param, t_param in zip(model.parameters(), ema_model.parameters()):\n",
    "            t_param.data.mul_(alpha).add_(s_param.data, alpha=(1.0 - alpha))\n",
    "\n",
    "    # # Copy buffers (for BatchNorm running_mean/var etc.)\n",
    "    # # Note: buffers are not parameters but they matter for forward behavior.\n",
    "    # for t_buf, s_buf in zip(model.buffers(), ema_model.buffers()):\n",
    "    #     t_buf.data.copy_(s_buf.data)\n",
    "    # 2) Buffers: EMA for floating buffers, copy others\n",
    "        # Use named_buffers to keep alignment in case order differs\n",
    "        s_buffers = dict(model.named_buffers())\n",
    "        t_buffers = dict(ema_model.named_buffers())\n",
    "\n",
    "        for name, s_buf in s_buffers.items():\n",
    "            if name not in t_buffers:\n",
    "                continue\n",
    "            t_buf = t_buffers[name]\n",
    "            if s_buf.dtype.is_floating_point:\n",
    "                # EMA for running_mean, running_var, etc.\n",
    "                t_buf.data.mul_(alpha).add_(s_buf.data, alpha=(1.0 - alpha))\n",
    "            else:\n",
    "                # integer counters like num_batches_tracked: just copy\n",
    "                t_buf.data.copy_(s_buf.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65e38d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_consistency_loss(student_outputs, teacher_outputs, temperature=4.0, entropy_threshold=0.4):\n",
    "    \"\"\"Compute consistency loss using KL divergence with per-sample reduction\"\"\"\n",
    "    \n",
    "    # 1. clamp logits to avoid huge exponentials (safety)\n",
    "    student_outputs = student_outputs.clamp(min=-1e2, max=1e2)\n",
    "    teacher_outputs = teacher_outputs.clamp(min=-1e2, max=1e2)\n",
    "\n",
    "    # 2. compute log-probs for both (stable)\n",
    "    student_logp = F.log_softmax(student_outputs / temperature, dim=1)\n",
    "    teacher_logp = F.log_softmax(teacher_outputs / temperature, dim=1)\n",
    "\n",
    "    # 3. Use log_target=True to pass log-prob target directly (avoids 0*log0 issues)\n",
    "    # reduction='none' keeps per-sample results; sum across classes to get per-sample.\n",
    "    per_sample = F.kl_div(student_logp, teacher_logp, reduction='none', log_target=True).sum(dim=1)\n",
    "\n",
    "    # 4. temperature scaling factor (common in distillation)\n",
    "    per_sample = per_sample * (temperature ** 2)\n",
    "\n",
    "    # disable thresholding because we are using entropy in the training loop now. just returning raw KL divergence\n",
    "    # # 5. exponential thresholding:\n",
    "    # # compute teacher probs (without temperature for entropy)\n",
    "    # teacher_probs = F.softmax(teacher_outputs, dim=1)\n",
    "    # # Shannon entropy per sample\n",
    "    # teacher_entropy = -torch.sum(\n",
    "    #     teacher_probs * torch.log(teacher_probs + 1e-8),\n",
    "    #     dim=1\n",
    "    # )\n",
    "    # # normalize entropy to [0, 1]\n",
    "    # num_classes = teacher_outputs.shape[1]\n",
    "    # max_entropy = torch.log(torch.tensor(num_classes, dtype=teacher_entropy.dtype, device=teacher_entropy.device))\n",
    "    # normalized_entropy = teacher_entropy / max_entropy\n",
    "    # # Exponential decay weighting\n",
    "    # confidence_weights = torch.exp(-normalized_entropy / entropy_threshold)\n",
    "    # per_sample = per_sample * confidence_weights\n",
    "\n",
    "    # 6. clamp/nan-safety: replace NaN/inf with large finite values instead of letting them propagate\n",
    "    per_sample = torch.nan_to_num(per_sample, nan=1e3, posinf=1e3, neginf=1e3)\n",
    "\n",
    "    return per_sample  # shape [B]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7cdb9b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_teacher_confidence(teacher_outputs, labels=None, mode=None):\n",
    "    \"\"\"\n",
    "    Compute teacher confidence for each sample\n",
    "    \n",
    "    Args:\n",
    "        teacher_outputs: logits from teacher [B, num_classes]\n",
    "        labels: ground truth labels [B] (optional, only for 'agreement' mode)\n",
    "        mode: 'max' (highest prob), 'entropy' (inverse entropy), or 'agreement' (prob of true class)\n",
    "    \n",
    "    Returns:\n",
    "        confidence scores [B] in range [0, 1]\n",
    "    \"\"\"\n",
    "    teacher_probs = F.softmax(teacher_outputs, dim=1)\n",
    "    \n",
    "    # if mode == 'max':\n",
    "    #     # Confidence = highest predicted probability\n",
    "    #     confidence = teacher_probs.max(dim=1)[0]\n",
    "        \n",
    "    # elif mode == 'entropy':\n",
    "    # Confidence = 1 - normalized_entropy\n",
    "    teacher_entropy = -torch.sum(\n",
    "        teacher_probs * torch.log(teacher_probs + 1e-8),\n",
    "        dim=1\n",
    "    )\n",
    "    num_classes = teacher_outputs.shape[1]\n",
    "    max_entropy = torch.log(torch.tensor(num_classes, dtype=teacher_entropy.dtype, device=teacher_entropy.device))\n",
    "    normalized_entropy = teacher_entropy / max_entropy\n",
    "    confidence = 1.0 - normalized_entropy\n",
    "        \n",
    "    # elif mode == 'agreement':\n",
    "    #     # Confidence = teacher's probability for the true label\n",
    "    #     # Only meaningful for labeled data\n",
    "    #     if labels is None:\n",
    "    #         raise ValueError(\"'agreement' mode requires labels\")\n",
    "    #     confidence = teacher_probs.gather(1, labels.view(-1, 1)).squeeze(1)\n",
    "        \n",
    "    # else:\n",
    "    #     raise ValueError(f\"Unknown mode: {mode}\")\n",
    "    \n",
    "    return confidence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50f83e2",
   "metadata": {},
   "source": [
    "# MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9aa95d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLOv11(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = YOLO(\"yolov11l-face.pt\").model\n",
    "        # self.backbone = torch.nn.Sequential(*list(self.model.model.children())[:7])  # Stops after C3k2 (layer 6)\n",
    "        self.feature_model = torch.nn.Sequential(*list(self.model.model.children())[:10])  # Stops after SPPF (layer 9)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.feature_model(x)\n",
    "\n",
    "# model = YOLOv11().to(device)\n",
    "# features = model(images) \n",
    "# features.shape # torch.Size([B, 512, 20, 20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00b0641a",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIP_PROCESSOR = CLIPImageProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "class CLIP(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.clip_model = CLIPVisionModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        # CLIP's final hidden state before projection (not the projection itself)\n",
    "        self.clip_output_dim = self.clip_model.config.hidden_size \n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        outputs = self.clip_model(**x)\n",
    "        pooled_output = outputs.pooler_output  # shape: [batch_size, 512]\n",
    "        return pooled_output\n",
    "\n",
    "# img = CLIP_PROCESSOR(Image.open(\"image.jpg\"), return_tensors=\"pt\").to(device)\n",
    "# img['pixel_values'].shape # torch.Size([1, 3, 224, 224])\n",
    "# model = CLIP().to(device)\n",
    "# outputs = model(img) \n",
    "# outputs.shape # torch.Size([B, 768])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53b0595d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # self.resnet = models.resnet152(weights=models.ResNet152_Weights.DEFAULT)\n",
    "        self.resnet = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "        \n",
    "        self.feature_extractor = nn.Sequential(*list(self.resnet.children())[:-3])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        features = self.feature_extractor(x) # [B, 1024, H/16, W/16]\n",
    "        return features\n",
    "    \n",
    "# model = ResNet152().to(device)\n",
    "# outputs = model(torch.randn(16, 3, 224, 224).to(device)) \n",
    "# outputs.shape # torch.Size([B, 1024, 14, 14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c31e31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiggerClassifier(torch.nn.Module):\n",
    "    def __init__(self, output_dim=3):\n",
    "        super().__init__()\n",
    "        self.clip = CLIP() # CLIP outputs: [B, 768]\n",
    "        self.yolo = YOLOv11() # YOLO outputs: [B, 512, 20, 20]\n",
    "        self.resnet = ResNet() # ResNet outputs: [B, 1024, H/16, W/16]\n",
    "\n",
    "        # Global average pooling for feature maps\n",
    "        self.yolo_pool = torch.nn.AdaptiveAvgPool2d((1, 1))\n",
    "        # self.resnet_pool = torch.nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        # self.fc1 = torch.nn.Linear(768 + 512 + 1024, 2048)\n",
    "        self.fc1 = torch.nn.Linear(768 + 512, 2048)\n",
    "        self.activation1 = torch.nn.GELU()\n",
    "        self.dropout1 = torch.nn.Dropout(0.3)\n",
    "        self.fc2 = torch.nn.Linear(2048, 1024)\n",
    "        self.activation2 = torch.nn.GELU()\n",
    "        self.dropout2 = torch.nn.Dropout(0.3)\n",
    "        self.fc3 = torch.nn.Linear(1024, output_dim)\n",
    "        \n",
    "    def forward(self, clip_inputs, img_tensor):\n",
    "        clip_features = self.clip(clip_inputs)  # [B, 768]\n",
    "        yolo_features = self.yolo(img_tensor)  # [B, 512, 20, 20]\n",
    "        # resnet_features = self.resnet(img_tensor) # [B, 1024, _, _]\n",
    "\n",
    "        # Pool YOLO features to [B, 512, 1, 1] then to [B, 512]\n",
    "        yolo_features = self.yolo_pool(yolo_features).flatten(1)\n",
    "        # resnet_features = self.resnet_pool(resnet_features).flatten(1)\n",
    "\n",
    "        # combined_features = torch.cat([clip_features, yolo_features, resnet_features], dim=1)  # [B, 2304]\n",
    "        combined_features = torch.cat([clip_features, yolo_features], dim=1)\n",
    "        \n",
    "        x = self.fc1(combined_features)\n",
    "        x = self.activation1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.activation2(x)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387fdf8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cfbe9a77",
   "metadata": {},
   "source": [
    "# DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7afbcd",
   "metadata": {},
   "source": [
    "### transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "927a8895",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_tensor = transforms.Compose([\n",
    "    transforms.ToImage(), \n",
    "    transforms.ToDtype(torch.float32, scale=True),\n",
    "])\n",
    "\n",
    "yolo_intermediate_input_size = 700\n",
    "yolo_final_input_size = 640\n",
    "\n",
    "base_transform = transforms.Compose([\n",
    "    to_tensor,\n",
    "    transforms.Resize(size=yolo_intermediate_input_size, interpolation=transforms.InterpolationMode.BILINEAR, antialias=True), # Resize maintaining aspect ratio, then pad to square\n",
    "    transforms.RandomRotation(degrees=(-15, 15), interpolation=transforms.InterpolationMode.BILINEAR, expand=True, fill=0),\n",
    "    transforms.RandomCrop(yolo_final_input_size),\n",
    "    transforms.RandomHorizontalFlip(p=0.5), # random flip\n",
    "])\n",
    "\n",
    "\n",
    "yolo_weak_transform = transforms.Compose([    \n",
    "    transforms.ColorJitter(brightness=0.05, contrast=0.05, saturation=0.05, hue=0.02),\n",
    "    transforms.Lambda(lambda x: x + torch.randn_like(x) * 0.005), transforms.Lambda(lambda x: torch.clamp(x, 0, 1)),\n",
    "])\n",
    "yolo_strong_transform = transforms.Compose([    \n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),\n",
    "    transforms.Lambda(lambda x: x + torch.randn_like(x) * 0.01), transforms.Lambda(lambda x: torch.clamp(x, 0, 1)),\n",
    "])\n",
    "\n",
    "clip_base_transform = transforms.Compose([ \n",
    "    transforms.Resize(size=224, interpolation=transforms.InterpolationMode.BILINEAR, antialias=True),\n",
    "])\n",
    "clip_weak_transform = transforms.Compose([    \n",
    "    transforms.ColorJitter(brightness=0.05, contrast=0.05, saturation=0.05, hue=0.02),\n",
    "    transforms.Lambda(lambda x: x + torch.randn_like(x) * 0.005), transforms.Lambda(lambda x: torch.clamp(x, 0, 1)),\n",
    "])\n",
    "clip_strong_transform = transforms.Compose([    \n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),\n",
    "    transforms.Lambda(lambda x: x + torch.randn_like(x) * 0.005), transforms.Lambda(lambda x: torch.clamp(x, 0, 1)),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "769389db",
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_val_transform = transforms.Compose([\n",
    "    to_tensor,\n",
    "    transforms.Resize(size=yolo_intermediate_input_size, interpolation=transforms.InterpolationMode.BILINEAR, antialias=True), # Resize maintaining aspect ratio, then pad to square\n",
    "    transforms.CenterCrop(yolo_final_input_size), \n",
    "    transforms.Lambda(lambda x: torch.clamp(x, 0, 1)),\n",
    "])\n",
    "\n",
    "clip_val_transform = transforms.Compose([ \n",
    "    to_tensor,\n",
    "    transforms.Resize(size=256, interpolation=transforms.InterpolationMode.BILINEAR, antialias=True),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.Lambda(lambda x: torch.clamp(x, 0, 1)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e42e48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8bbc79ed",
   "metadata": {},
   "source": [
    "### datasets/loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "851d961b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_images = []\n",
    "\n",
    "class MeanTeacherDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, val=False, supervised=False, supervised_ratio=0.5, upsample=None):\n",
    "        self.root_dir = os.path.expanduser(root_dir) # root_dir\n",
    "        self.annotations = pd.read_csv(os.path.join(self.root_dir, csv_file))\n",
    "        if dry_run:\n",
    "            self.annotations = self.annotations.sample(n=min(dry_run, len(self.annotations)), random_state=42)\n",
    "            self.annotations = self.annotations.reset_index(drop=True)  # Reset index after sampling\n",
    "        if supervised: # only provide supervised data\n",
    "            # only labeled data:\n",
    "            # self.annotations = self.annotations[self.annotations['label'] != 1].reset_index(drop=True)\n",
    "\n",
    "            labeled = self.annotations[self.annotations['label'].isin([0, 2])]\n",
    "            unlabeled = self.annotations[self.annotations['label'] == 1]\n",
    "            n_labeled = len(labeled) \n",
    "            total_len = int(n_labeled / supervised_ratio)\n",
    "            target_len = total_len - n_labeled\n",
    "            # target_len = int(n_labeled * 0.5)\n",
    "            if len(unlabeled) > target_len:\n",
    "                unlabeled = unlabeled.sample(n=target_len) #, random_state=42)\n",
    "            else: \n",
    "                print(f\"not enough unlabeled. asked for {target_len}, only have {len(unlabeled)}\")\n",
    "            self.annotations = pd.concat([labeled, unlabeled], ignore_index=True)\n",
    "        if upsample: # upsample supervised labels. Either an int, or default as None\n",
    "            labeled = self.annotations[self.annotations['label'].isin([0, 2])].copy()\n",
    "            unlabeled = self.annotations[self.annotations['label'] == 1].copy()\n",
    "            n_labeled = len(labeled)\n",
    "            n_unlabeled = len(unlabeled)\n",
    "            print(f\"Upsampling labeled data: {n_labeled} samples * {upsample} = {n_labeled * upsample}\")\n",
    "            print(f\"Unlabeled data: {n_unlabeled} samples\")\n",
    "            # Create upsampled copies with unique IDs\n",
    "            labeled_copies = []\n",
    "            for i in range(upsample):\n",
    "                labeled_copy = labeled.copy()\n",
    "                # Append suffix to unique_id for each copy\n",
    "                labeled_copy['unique_id'] = labeled_copy['unique_id'].astype(str) + f'_copy{i}'\n",
    "                labeled_copies.append(labeled_copy)\n",
    "            labeled_upsampled = pd.concat(labeled_copies, ignore_index=True)\n",
    "            self.annotations = pd.concat([labeled_upsampled, unlabeled], ignore_index=True)\n",
    "            self.annotations = self.annotations.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "            print(f\"Final dataset size: {len(self.annotations)} samples\")\n",
    "            print(f\"Labeled ratio: {len(labeled_upsampled) / len(self.annotations):.2%}\")\n",
    "        # csv headers: relative_path,label,width,height,size_kb,source\n",
    "        self.transform_times = []\n",
    "        self.val = val\n",
    "\n",
    "        self.error_log_path = f'dataset_errors_{\"val\" if val else \"train\"}.log'\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "    \n",
    "    def get_id(self, idx):\n",
    "        return self.annotations.at[idx, 'unique_id']\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        try:\n",
    "            # 1. process metadata\n",
    "\n",
    "            original_label = self.annotations.iloc[idx, 1]\n",
    "            label = torch.tensor(original_label, dtype=torch.long)\n",
    "            \n",
    "            img_path = os.path.join(self.root_dir, self.annotations.iloc[idx, 0])\n",
    "            img_path = img_path.replace('\\\\', '/')\n",
    "            image_pil = Image.open(img_path).convert('RGB')\n",
    "\n",
    "            metadata = {\n",
    "                'img_path': img_path,\n",
    "                'label': original_label,\n",
    "                'width': self.annotations.iloc[idx, 2],\n",
    "                'height': self.annotations.iloc[idx, 3],\n",
    "                'size_kb': self.annotations.iloc[idx, 4],\n",
    "                'source': str(self.annotations.iloc[idx, 5]),\n",
    "            }\n",
    "\n",
    "            # 2. process images \n",
    "\n",
    "            if not self.val: # for training loop\n",
    "                item_transform_start = time.time()\n",
    "                \n",
    "                base_image = base_transform(image_pil)\n",
    "                \n",
    "                # YOLO branch\n",
    "                # yolo_base_image = base_transform(base_image)\n",
    "                yolo_weak_image = yolo_weak_transform(base_image)\n",
    "                yolo_strong_image = yolo_strong_transform(base_image)\n",
    "                \n",
    "                # CLIP branch\n",
    "                clip_base_image = clip_base_transform(base_image)\n",
    "                clip_weak_image = clip_weak_transform(clip_base_image)\n",
    "                clip_strong_image = clip_strong_transform(clip_base_image)\n",
    "\n",
    "                clip_weak_image = CLIP_PROCESSOR(images=clip_weak_image, return_tensors=\"pt\", do_rescale=False)\n",
    "                clip_weak_image['pixel_values'] = clip_weak_image['pixel_values'].squeeze(0)  # [1, 3, 224, 224] -> [3, 224, 224]\n",
    "                clip_strong_image = CLIP_PROCESSOR(images=clip_strong_image, return_tensors=\"pt\", do_rescale=False)\n",
    "                clip_strong_image['pixel_values'] = clip_strong_image['pixel_values'].squeeze(0)  # [1, 3, 224, 224] -> [3, 224, 224]\n",
    "\n",
    "                item_transform_time = time.time() - item_transform_start\n",
    "                if len(self.transform_times) < 1000:\n",
    "                    self.transform_times.append(item_transform_time)\n",
    "\n",
    "                # clip_weak_image['pixel_values'] = clip_weak_image['pixel_values'].to(device, non_blocking=True)\n",
    "                # clip_strong_image['pixel_values'] = clip_strong_image['pixel_values'].to(device, non_blocking=True)\n",
    "                # yolo_weak_image = yolo_weak_image.to(device, non_blocking=True)\n",
    "                # yolo_strong_image = yolo_strong_image.to(device, non_blocking=True)\n",
    "                # label = label.to(device, non_blocking=True)\n",
    "\n",
    "                return clip_weak_image, clip_strong_image, yolo_weak_image, yolo_strong_image, label, metadata\n",
    "\n",
    "            else: # for validation\n",
    "                yolo_image = yolo_val_transform(image_pil)\n",
    "\n",
    "                clip_image = clip_val_transform(image_pil)\n",
    "                clip_image = CLIP_PROCESSOR(images=clip_image, return_tensors=\"pt\", do_rescale=False)\n",
    "                clip_image['pixel_values'] = clip_image['pixel_values'].squeeze(0)\n",
    "                return clip_image, clip_image, yolo_image, yolo_image, label, metadata\n",
    "            \n",
    "        except Exception as e:\n",
    "            \n",
    "            with open(os.path.join(logger.get_log_dir(), self.error_log_path), 'a') as f:\n",
    "                f.write(f\"Error loading image at index {idx}: {e}\\n\\n\")\n",
    "\n",
    "            # Create dummy CLIP inputs (matching CLIP_PROCESSOR output format)\n",
    "            dummy_clip_input = {'pixel_values': torch.zeros(3, 224, 224)}\n",
    "            dummy_yolo_tensor = torch.zeros(3, 640, 640)\n",
    "            dummy_label = torch.tensor(1, dtype=torch.long)\n",
    "            dummy_metadata = {\n",
    "                'img_path': f'error_at_idx_{idx}',\n",
    "                'label': 1,\n",
    "                'width': 640,\n",
    "                'height': 640,\n",
    "                'size_kb': 0.0,\n",
    "                'source': -1,\n",
    "            }\n",
    "            \n",
    "            return dummy_clip_input, dummy_clip_input, dummy_yolo_tensor, dummy_yolo_tensor, dummy_label, dummy_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "32c591b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomVersionSampler(Sampler):\n",
    "    def __init__(self, base_dataset):\n",
    "        self.base_dataset = base_dataset\n",
    "        \n",
    "        self.grouped = {} # Group indices by ID\n",
    "        for idx in range(len(base_dataset)):\n",
    "            img_id = base_dataset.get_id(idx)\n",
    "            if img_id not in self.grouped:\n",
    "                self.grouped[img_id] = []\n",
    "            self.grouped[img_id].append(idx)\n",
    "        self.ids = list(self.grouped.keys())\n",
    "\n",
    "    def __iter__(self):\n",
    "        # For each ID, pick a random index\n",
    "        chosen_indices = [random.choice(self.grouped[img_id]) for img_id in self.ids]\n",
    "        # Shuffle the chosen indices for batching\n",
    "        random.shuffle(chosen_indices)\n",
    "        return iter(chosen_indices)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbb2b68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d06b267b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data._utils.collate import default_collate\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    # Collate everything except metadata normally\n",
    "    clip_weak = default_collate([item[0] for item in batch])\n",
    "    clip_strong = default_collate([item[1] for item in batch])\n",
    "    yolo_weak = default_collate([item[2] for item in batch])\n",
    "    yolo_strong = default_collate([item[3] for item in batch])\n",
    "    labels = default_collate([item[4] for item in batch])\n",
    "    \n",
    "    # Keep metadata as a list of dicts (no tensor conversion)\n",
    "    metadata_list = [item[5] for item in batch]\n",
    "    \n",
    "    # Restructure to dict of lists for easier access\n",
    "    metadata = {\n",
    "        'img_path': [m['img_path'] for m in metadata_list],\n",
    "        'label': [m['label'] for m in metadata_list],\n",
    "        'width': [m['width'] for m in metadata_list],\n",
    "        'height': [m['height'] for m in metadata_list],\n",
    "        'size_kb': [m['size_kb'] for m in metadata_list],\n",
    "        'source': [m['source'] for m in metadata_list],\n",
    "    }\n",
    "    \n",
    "    return clip_weak, clip_strong, yolo_weak, yolo_strong, labels, metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb37da2",
   "metadata": {},
   "source": [
    "##### The actual datasets and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3c239a75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "487327"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "supervised_train_dataset = MeanTeacherDataset(\n",
    "    csv_file = \"train_2.csv\", \n",
    "    root_dir = \"~/Workspace/data-v2/train\",\n",
    "    supervised = True,\n",
    ")\n",
    "train_dataset = MeanTeacherDataset(\n",
    "    csv_file = \"train_2.csv\", \n",
    "    root_dir = \"~/Workspace/data-v2/train\",\n",
    "    val = False,\n",
    ")\n",
    "val_dataset = MeanTeacherDataset(\n",
    "    csv_file = \"val_2.csv\", \n",
    "    root_dir = \"~/Workspace/data-v2/val\",\n",
    "    val = True,\n",
    ")\n",
    "\n",
    "supervised_sampler = RandomVersionSampler(supervised_train_dataset)\n",
    "supervised_train_dataloader = DataLoader(\n",
    "    supervised_train_dataset, \n",
    "    batch_size=config.batch_size, \n",
    "    sampler=supervised_sampler,\n",
    "    collate_fn=custom_collate_fn,\n",
    "    num_workers=get_num_workers(),\n",
    "    persistent_workers=False, # True if get_num_workers() > 0 else False,\n",
    "    # pin_memory=False, # WSL does not support pin_memory well\n",
    "    prefetch_factor=2 if get_num_workers() > 0 else None,\n",
    ")\n",
    "\n",
    "sampler = RandomVersionSampler(train_dataset)\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=config.batch_size, \n",
    "    sampler=sampler,\n",
    "    collate_fn=custom_collate_fn,\n",
    "    num_workers=get_num_workers(),\n",
    "    persistent_workers=False, # True if get_num_workers() > 0 else False,\n",
    "    # pin_memory=False, # WSL does not support pin_memory well\n",
    "    prefetch_factor=2 if get_num_workers() > 0 else None,\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=config.batch_size, \n",
    "    collate_fn=custom_collate_fn,\n",
    "    num_workers=get_num_workers(),\n",
    "    persistent_workers=False, # True if get_num_workers() > 0 else False,\n",
    "    # pin_memory=False, # WSL does not support pin_memory well\n",
    "    prefetch_factor=3 if get_num_workers() > 0 else None,\n",
    ")\n",
    "\n",
    "len(train_dataset) - len(train_dataloader) * config.batch_size # > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e84e14d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18487, 28399)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(supervised_train_dataloader), len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb393b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dd17ea35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_and_sanity_check(base_dataset, num_batches = 50):\n",
    "    total_samples = 0\n",
    "    print(f\"Benchmarking dataloader for {num_batches} batches...\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    for i, (student_clip_inputs, teacher_clip_inputs, yolo_weak_tensors, yolo_strong_tensors, labels, _) in enumerate(base_dataset):\n",
    "        clip_tensors = teacher_clip_inputs['pixel_values']\n",
    "        yolo_tensors = yolo_strong_tensors\n",
    "        if i >= num_batches:\n",
    "            break\n",
    "        total_samples += len(labels)\n",
    "    total_time = time.time() - start_time\n",
    "\n",
    "    avg_time_per_batch = total_time / num_batches\n",
    "    avg_time_per_sample = total_time / total_samples\n",
    "    samples_per_second = total_samples / total_time\n",
    "\n",
    "    print(f\"Total time: {total_time:.2f} seconds.\", f\"Total samples: {total_samples}\")\n",
    "    print(f\"Average time per batch: {avg_time_per_batch:.4f} seconds.\", f\"Average time per sample: {avg_time_per_sample:.4f} seconds\")\n",
    "    print(f\"Throughput: {samples_per_second:.2f} samples/second\")\n",
    "\n",
    "    # sanity check dataset\n",
    "    clip, _, yolo, _, _, _= base_dataset[0]\n",
    "    first_img = yolo.permute(1, 2, 0).cpu().numpy()  # [640, 640, 3]\n",
    "    first_img = (first_img * 255).astype('uint8')\n",
    "    pil_img = Image.fromarray(first_img)\n",
    "    plt.imshow(pil_img)\n",
    "    plt.show()\n",
    "\n",
    "    # sanity check clip img\n",
    "    first_img = clip_tensors[0]\n",
    "    mean = torch.tensor([0.48145466, 0.4578275, 0.40821073]).view(3, 1, 1)\n",
    "    std = torch.tensor([0.26862954, 0.26130258, 0.27577711]).view(3, 1, 1)\n",
    "    first_img = first_img * std + mean\n",
    "    first_img = first_img.permute(1, 2, 0).cpu().numpy()  # [224, 224, 3]\n",
    "    first_img = (first_img * 255).astype('uint8')\n",
    "    pil_img = Image.fromarray(first_img)\n",
    "    plt.imshow(pil_img)\n",
    "    plt.show()\n",
    "\n",
    "    # sanity check yolo img\n",
    "    first_img = yolo_tensors[0]\n",
    "    first_img = first_img.permute(1, 2, 0).cpu().numpy()  # [640, 640, 3]\n",
    "    first_img = (first_img * 255).astype('uint8')\n",
    "    pil_img = Image.fromarray(first_img)\n",
    "    plt.imshow(pil_img)\n",
    "    plt.show()\n",
    "\n",
    "    print(\"mean transform compute time:\", np.mean(base_dataset.transform_times))\n",
    "\n",
    "# benchmark_and_sanity_check(train_dataset, 10)\n",
    "# benchmark_and_sanity_check(val_dataset, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e929f863",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f2b9450",
   "metadata": {},
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f8a42476",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BiggerClassifier().to(config.device)\n",
    "teacher_model = copy.deepcopy(model).to(config.device)\n",
    "teacher_model.eval()\n",
    "# teacher_model.train()\n",
    "for p in teacher_model.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b39328e",
   "metadata": {},
   "source": [
    "### learn rate, scheduler, optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "98ee4a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing backbone parameters initially...\n",
      "scheduler lr: [6e-06, 1.2e-05, 3e-05, 9e-05]\n"
     ]
    }
   ],
   "source": [
    "clip_params = []\n",
    "yolo_params = []\n",
    "resnet_params = []\n",
    "classifier_params = []\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if 'clip' in name:\n",
    "        clip_params.append(param)\n",
    "    elif 'yolo' in name:\n",
    "        yolo_params.append(param)\n",
    "    elif 'resnet' in name:\n",
    "        resnet_params.append(param)\n",
    "        # i can separate out earlier layers if i want to\n",
    "    else:\n",
    "        classifier_params.append(param)\n",
    "\n",
    "print(\"Freezing backbone parameters initially...\")\n",
    "for param in clip_params + yolo_params: # + resnet_params:\n",
    "    param.requires_grad = False\n",
    "\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {'params': clip_params, 'lr': config.lr_backbone*0.2, 'name': 'clip'},\n",
    "    {'params': yolo_params, 'lr': config.lr_backbone*0.4, 'name': 'yolo'},\n",
    "    {'params': resnet_params, 'lr': config.lr_backbone, 'name': 'resnet'},\n",
    "    {'params': classifier_params, 'lr': config.initial_lr, 'name': 'classifier'}\n",
    "])\n",
    "\n",
    "# scheduler =  torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config.num_epochs, eta_min=1e-6)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.7, patience=2)\n",
    "print('scheduler lr:', scheduler.get_last_lr())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cb60fa5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn as nn\n",
    "\n",
    "# for name, module in model.named_modules():\n",
    "#     if isinstance(module, (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d)):\n",
    "#         print(f\"Found BatchNorm: {name} ({type(module).__name__})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c4f75e",
   "metadata": {},
   "source": [
    "### loss and criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "711d304d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AsymmetricFocalLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Asymmetric Focal Loss variant that combines focal loss with asymmetric penalties.\n",
    "    Useful when you also want to handle class imbalance.\n",
    "    \"\"\"\n",
    "    def __init__(self, gamma=2.0, alpha=None, confusion_penalty_matrix=None):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        if confusion_penalty_matrix is None:\n",
    "            confusion_penalty_matrix = torch.tensor([\n",
    "                [1.0, 1.0, 1.0],   # True: BAD \n",
    "                [1.0, 1.0, 1.0],   # True: NEUTRAL\n",
    "                [1.0, 1.0, 1.0]    # True: GOOD\n",
    "            ])\n",
    "        self.confusion_penalty_matrix = confusion_penalty_matrix\n",
    "        \n",
    "    def forward(self, logits, targets, reduction='mean'):\n",
    "        ce_loss = F.cross_entropy(logits, targets, reduction='none')\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        p_t = probs.gather(1, targets.view(-1, 1)).squeeze(1)\n",
    "        \n",
    "        # Focal term\n",
    "        focal_weight = (1 - p_t) ** self.gamma\n",
    "\n",
    "        # === Expected penalty ===\n",
    "        # penalty_matrix: [C, C] where penalty_matrix[true, pred] gives penalty\n",
    "        # penalty_for_true: [B, C] rows correspond to each sample's true class\n",
    "        penalty_for_true = self.confusion_penalty_matrix[targets]  # shape [B, num_classes]\n",
    "        # Expected penalty under predicted distribution\n",
    "        expected_penalty = (probs * penalty_for_true).sum(dim=1)  # shape [B]\n",
    "\n",
    "        # Combine focal weight with expected penalties\n",
    "        loss = focal_weight * ce_loss * expected_penalty\n",
    "\n",
    "        # Optional alpha weighting\n",
    "        if self.alpha is not None:\n",
    "            alpha_t = self.alpha.gather(0, targets)\n",
    "            loss = alpha_t * loss\n",
    "\n",
    "        # if reduction == 'mean':\n",
    "        #     return loss.mean()\n",
    "        # elif reduction == 'sum':\n",
    "        #     return loss.sum()\n",
    "        # else:  # 'none'\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d9e9dd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ce_criterion = nn.CrossEntropyLoss().to(config.device)\n",
    "\n",
    "af_criterion = AsymmetricFocalLoss(\n",
    "    gamma=1.2,\n",
    "    alpha=torch.tensor([1.0, 1.0, 1.1]).to(config.device),  \n",
    "    confusion_penalty_matrix=torch.tensor([\n",
    "        [1.0, 1.1, 1.15], \n",
    "        [0.88, 1.0, 0.90],\n",
    "        [1.15, 1.05, 1.0]\n",
    "    ]).to(config.device)\n",
    ").to(config.device)\n",
    "\n",
    "af_criterion2 = AsymmetricFocalLoss(\n",
    "    gamma=1.2,\n",
    "    alpha=torch.tensor([1.15, 0.9, 1.3]).to(config.device),  \n",
    "    confusion_penalty_matrix=torch.tensor([\n",
    "        [1.0, 1.1, 1.2], \n",
    "        [0.88, 1.0, 0.90],\n",
    "        [1.15, 1.05, 1.0]\n",
    "    ]).to(config.device)\n",
    ").to(config.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fc80c7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i=0\n",
    "# logits = torch.tensor([[10, 1, 1]]).to(device) * 1.5  # 5 samples, 3 classes\n",
    "# # logits = torch.randn(5,3).to(device) * 1.5  # 5 samples, 3 classes\n",
    "# targets = torch.tensor([0] * 10).to(device)\n",
    "\n",
    "# # Forward pass\n",
    "# probs = F.softmax(logits[i].unsqueeze(0), dim=1)\n",
    "# loss = af_criterion(logits[i].unsqueeze(0), targets[i].unsqueeze(0))\n",
    "# ce_loss = ce_criterion(logits[i].unsqueeze(0), targets[i].unsqueeze(0))\n",
    "\n",
    "# print(probs, targets[i].unsqueeze(0).item())\n",
    "# print(loss.item())\n",
    "# print(ce_loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf376aa",
   "metadata": {},
   "source": [
    "### util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b75f72d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler = GradScaler() \n",
    "monitor = PerformanceMonitor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dbe3d884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# At the end of each epoch, after validation:\\nif val_accuracy > monitor.best_accuracy:\\n    save_best_checkpoint(\\n        model, teacher_model, optimizer, scheduler,\\n        epoch, global_step, config, val_accuracy,\\n        monitor, loss_history, cls_loss_history\\n    )\\n\\n# To resume training:\\ncheckpoint_path = \"./checkpoints/your_run_name/checkpoint_epoch10_step50000.pth\"\\nepoch, global_step, loss_history, cls_loss_history, val_accuracy = load_checkpoint(\\n    checkpoint_path, model, teacher_model, optimizer, scheduler,\\n    config, monitor, device=config.device\\n)\\n\\n# Then continue training loop from config.cur_epoch\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def save_checkpoint(model, teacher_model, optimizer, scheduler, epoch, global_step, config, \n",
    "                    val_accuracy, monitor, loss_history, cls_loss_history):\n",
    "\n",
    "    checkpoint_dir = config.checkpoint_path\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    filename = f\"checkpoint_epoch{epoch}_step{global_step}_acc{val_accuracy:.4f}_{timestamp}.pth\"\n",
    "    \n",
    "    checkpoint_path = os.path.join(checkpoint_dir, filename)\n",
    "    \n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'global_step': global_step,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'teacher_state_dict': teacher_model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'val_accuracy': val_accuracy,\n",
    "        'loss_history': loss_history,\n",
    "        'cls_loss_history': cls_loss_history,\n",
    "        'monitor_state': {\n",
    "            'best_accuracy': monitor.best_accuracy,\n",
    "            'epochs_without_improvement': monitor.epochs_without_improvement,\n",
    "            'accuracy_history': monitor.accuracy_history,\n",
    "        },\n",
    "        'config': {k: v for k, v in vars(config).items() if not k.startswith('_')},\n",
    "        \n",
    "        'save_timestamp': datetime.now().isoformat(),\n",
    "        'pytorch_version': torch.__version__,\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        torch.save(checkpoint, checkpoint_path)\n",
    "        print(f\"Checkpoint saved successfully: {checkpoint_path}\")\n",
    "        print(f\"Epoch: {epoch} | Step: {global_step} | Val Acc: {val_accuracy:.4f}\")\n",
    "        return checkpoint_path\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving checkpoint: {e}\")\n",
    "        raise\n",
    "\n",
    "def load_checkpoint(checkpoint_path, model, teacher_model, optimizer, scheduler, config, \n",
    "                    monitor, device='cuda', strict=True):\n",
    "\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        raise FileNotFoundError(f\"Checkpoint not found: {checkpoint_path}\")\n",
    "    print(f\"Loading checkpoint from: {checkpoint_path}\")\n",
    "    \n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    \n",
    "    model.load_state_dict(checkpoint['model_state_dict'], strict=strict)\n",
    "    teacher_model.load_state_dict(checkpoint['teacher_state_dict'], strict=strict)\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    \n",
    "    # Restore training state\n",
    "    epoch = checkpoint['epoch']\n",
    "    global_step = checkpoint['global_step']\n",
    "    val_accuracy = checkpoint['val_accuracy']\n",
    "    loss_history = checkpoint['loss_history']\n",
    "    cls_loss_history = checkpoint['cls_loss_history']\n",
    "    \n",
    "    # Restore performance monitor\n",
    "    monitor_state = checkpoint['monitor_state']\n",
    "    monitor.best_accuracy = monitor_state['best_accuracy']\n",
    "    monitor.accuracy_history = monitor_state['accuracy_history']\n",
    "    \n",
    "    # Update config with saved values (optional - be careful with paths)\n",
    "    saved_config = checkpoint['config']\n",
    "    for key, value in saved_config.items():\n",
    "        if hasattr(config, key) and key not in ['checkpoint_path', 'log_dir']:\n",
    "            setattr(config, key, value)\n",
    "    \n",
    "    config.cur_epoch = epoch + 1\n",
    "    \n",
    "    print(f\"Checkpoint loaded successfully\")\n",
    "    print(f\"Resuming from Epoch: {epoch + 1} | Step: {global_step}\")\n",
    "    print(f\"Previous Val Acc: {val_accuracy:.4f} | Best Acc: {monitor.best_accuracy:.4f}\")\n",
    "    print(f\"Loaded len(loss_history) = {len(loss_history)}\")\n",
    "    \n",
    "    return epoch, global_step, loss_history, cls_loss_history, val_accuracy\n",
    "\n",
    "# Example usage in your training loop:\n",
    "\"\"\"\n",
    "# At the end of each epoch, after validation:\n",
    "if val_accuracy > monitor.best_accuracy:\n",
    "    save_best_checkpoint(\n",
    "        model, teacher_model, optimizer, scheduler,\n",
    "        epoch, global_step, config, val_accuracy,\n",
    "        monitor, loss_history, cls_loss_history\n",
    "    )\n",
    "\n",
    "# To resume training:\n",
    "checkpoint_path = \"./checkpoints/your_run_name/checkpoint_epoch10_step50000.pth\"\n",
    "epoch, global_step, loss_history, cls_loss_history, val_accuracy = load_checkpoint(\n",
    "    checkpoint_path, model, teacher_model, optimizer, scheduler,\n",
    "    config, monitor, device=config.device\n",
    ")\n",
    "\n",
    "# Then continue training loop from config.cur_epoch\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b9d7a764",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, val_loader, config, epoch):\n",
    "    torch.cuda.empty_cache()\n",
    "    model.eval()\n",
    "\n",
    "    # cumulative lists for dataframes\n",
    "    image_data = []\n",
    "    batch_data = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_pbar = tqdm(val_loader, desc=f'Validation Epoch {epoch}', total=len(val_loader), leave=False)\n",
    "\n",
    "        for batch_num, batch_input in enumerate(val_pbar):\n",
    "            clip_inputs, _, yolo_tensors, _, labels, metadata = batch_input\n",
    "            clip_inputs['pixel_values'] = clip_inputs['pixel_values'].to(config.device)\n",
    "            yolo_tensors = yolo_tensors.to(config.device)\n",
    "            labels = labels.to(config.device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(clip_inputs, yolo_tensors)\n",
    "            loss = af_criterion(outputs, labels).mean().cpu().numpy()\n",
    "            probs = F.softmax(outputs, dim=1).cpu().numpy()\n",
    "            predictions = outputs.argmax(dim=1).cpu().numpy()\n",
    "            labels = labels.cpu().numpy()\n",
    "\n",
    "            # Add batch-level data\n",
    "            batch_data.append({\n",
    "                'batch_num': batch_num,\n",
    "                'epoch': epoch,\n",
    "                'loss': loss.item()\n",
    "            })\n",
    "\n",
    "            # Add image-level data\n",
    "            for i in range(len(labels)):\n",
    "                true_label = labels[i]\n",
    "                pred_label = predictions[i]\n",
    "\n",
    "                # Check for severe misclassifications\n",
    "                # if true_label == 2 and pred_label == 0:  # True: GOOD, Pred: BAD\n",
    "                    # good_predicted_as_bad += 1\n",
    "                    # print(f\"[WARNING] Predicted BAD on true GOOD: {metadata['img_path'][i]}\")\n",
    "                # elif true_label == 0 and pred_label == 2:  # True: BAD, Pred: GOOD\n",
    "                    # bad_predicted_as_good += 1\n",
    "                    # print(f\"[WARNING] Predicted GOOD on true BAD: {metadata['img_path'][i]}\")\n",
    "\n",
    "\n",
    "                image_data.append({\n",
    "                    'batch_num': batch_num,\n",
    "                    'img_path': metadata['img_path'][i],\n",
    "                    'label': int(true_label),\n",
    "                    'width': int(metadata['width'][i]),\n",
    "                    'height': int(metadata['height'][i]),\n",
    "                    'size_kb': float(metadata['size_kb'][i]),\n",
    "                    'source': metadata['source'][i],\n",
    "                    'prediction': int(pred_label),\n",
    "                    'bad': float(probs[i, 0]),\n",
    "                    'neutral': float(probs[i, 1]),\n",
    "                    'good': float(probs[i, 2])\n",
    "                })\n",
    "\n",
    "            val_pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    image_df = pd.DataFrame(image_data)\n",
    "    batch_df = pd.DataFrame(batch_data)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    return image_df, batch_df, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bb0bc72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse(image_df, batch_df, config, monitor, epoch):\n",
    "    # Calculate metrics from dataframe\n",
    "    labeled_df = image_df[image_df['label'] != 1]\n",
    "    total = len(labeled_df)\n",
    "    correct = (labeled_df['label'] == labeled_df['prediction']).sum()\n",
    "    incorrect = ((labeled_df['label'] == 0) & (labeled_df['prediction'] == 2)).sum() + \\\n",
    "                ((labeled_df['label'] == 2) & (labeled_df['prediction'] == 0)).sum()\n",
    "    halfcorrect = ((labeled_df['label'] == 0) & (labeled_df['prediction'] == 1)).sum() + \\\n",
    "                  ((labeled_df['label'] == 2) & (labeled_df['prediction'] == 1)).sum()\n",
    "    accuracy = (correct - incorrect - 0.5*halfcorrect) / total\n",
    "\n",
    "    # Calculate per-class metrics\n",
    "    class_names = ['bad', 'neutral', 'good']\n",
    "    labels=[0, 1, 2]\n",
    "    all_labels = image_df['label'].values\n",
    "    all_preds = image_df['prediction'].values\n",
    "\n",
    "    # Confusion matrix and classification report\n",
    "    cm = confusion_matrix(y_true=all_labels, y_pred=all_preds, labels=labels)\n",
    "    report = classification_report(y_true=all_labels, y_pred=all_preds, labels=labels, target_names=class_names,\n",
    "                                   output_dict=True, zero_division=0)\n",
    "\n",
    "    # Calculate per-class recall\n",
    "    class_recall = []\n",
    "    for class_idx in range(config.num_classes):\n",
    "        class_mask = image_df['label'] == class_idx\n",
    "        if class_mask.sum() > 0:\n",
    "            class_correct = ((image_df['label'] == class_idx) &\n",
    "                           (image_df['prediction'] == class_idx)).sum()\n",
    "            class_recall.append(class_correct / class_mask.sum())\n",
    "        else:\n",
    "            class_recall.append(0.0)\n",
    "\n",
    "    # Compile metrics dictionary\n",
    "    avg_val_loss = batch_df['loss'].mean()\n",
    "\n",
    "    metrics = {\n",
    "        \"val/accuracy\": accuracy,\n",
    "        \"val/loss\": avg_val_loss,\n",
    "        \"val/recall_bad\": class_recall[0],\n",
    "        \"val/recall_neutral\": class_recall[1],\n",
    "        \"val/recall_good\": class_recall[2],\n",
    "    }\n",
    "\n",
    "    # Add precision and F1 scores from classification report\n",
    "    for class_name in class_names:\n",
    "        if class_name in report:\n",
    "            metrics[f\"val/precision_{class_name}\"] = report[class_name]['precision']\n",
    "            metrics[f\"val/f1_{class_name}\"] = report[class_name]['f1-score']\n",
    "\n",
    "    # Add severe misclassification counts\n",
    "    good_as_bad = image_df[(image_df['label'] == 2) & (image_df['prediction'] == 0)]\n",
    "    bad_as_good = image_df[(image_df['label'] == 0) & (image_df['prediction'] == 2)]\n",
    "    metrics[\"val/bad_as_good_count\"] = len(bad_as_good)\n",
    "    metrics[\"val/good_as_bad_count\"] = len(good_as_bad)\n",
    "\n",
    "    # Update monitor\n",
    "    monitor.accuracy_history.append(accuracy)\n",
    "    if accuracy > monitor.best_accuracy:\n",
    "        monitor.best_accuracy = accuracy\n",
    "        monitor.epochs_without_improvement = 0\n",
    "    else:\n",
    "        monitor.epochs_without_improvement += 1\n",
    "\n",
    "    print(f\"Validation Summary - Epoch {epoch}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f} | Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    return accuracy, metrics, cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8abc6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dec08a21",
   "metadata": {},
   "source": [
    "##### train_one_epoch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "854b6cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f880af4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8197c6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_epoch(image_df, batch_df, config, epoch):\n",
    "\n",
    "    avg_loss = batch_df['loss'].mean()\n",
    "    avg_cls_loss = batch_df['cls_loss'].mean()\n",
    "    avg_consistency_loss = batch_df['consistency_loss'].mean()\n",
    "    # avg_consistency_weight = batch_df['consistency_weight'].mean()\n",
    "    # total_time = batch_df['batch_time'].sum()\n",
    "    # avg_throughput = batch_df['images_per_second'].mean()\n",
    "    \n",
    "    metrics = {\n",
    "        \"epoch/loss\": avg_loss,\n",
    "        \"epoch/cls_loss\": avg_cls_loss,\n",
    "        \"epoch/consistency_loss\": avg_consistency_loss,\n",
    "        # \"epoch/consistency_weight\": avg_consistency_weight,\n",
    "        # \"epoch/time_minutes\": total_time / 60,\n",
    "        # \"epoch/avg_throughput\": avg_throughput,\n",
    "    }\n",
    "    \n",
    "    if image_df is not None:\n",
    "        total_samples = len(image_df)\n",
    "        correct = (image_df['label'] == image_df['prediction']).sum()\n",
    "        train_accuracy = correct / total_samples\n",
    "        \n",
    "        # Per-class accuracy\n",
    "        for class_idx in range(config.num_classes):\n",
    "            class_mask = image_df['label'] == class_idx\n",
    "            if class_mask.sum() > 0:\n",
    "                class_correct = ((image_df['label'] == class_idx) & \n",
    "                               (image_df['prediction'] == class_idx)).sum()\n",
    "                class_acc = class_correct / class_mask.sum()\n",
    "                metrics[f\"epoch/accuracy_class_{class_idx}\"] = class_acc\n",
    "        \n",
    "        # Average consistency loss per image\n",
    "        metrics[\"epoch/avg_image_consistency_loss\"] = image_df['consistency_loss'].mean()\n",
    "        metrics[\"epoch/train_accuracy\"] = train_accuracy\n",
    "        \n",
    "        # Severe misclassifications\n",
    "        good_as_bad = image_df[(image_df['label'] == 2) & (image_df['prediction'] == 0)]\n",
    "        bad_as_good = image_df[(image_df['label'] == 0) & (image_df['prediction'] == 2)]\n",
    "        metrics[\"epoch/bad_as_good_count\"] = len(bad_as_good)\n",
    "        metrics[\"epoch/good_as_bad_count\"] = len(good_as_bad)\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c2d1cc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_running_loss(loss_history, save_path, window_size=10):\n",
    "    \"\"\"\n",
    "    Plot running loss with moving average and save to file\n",
    "    \n",
    "    Args:\n",
    "        loss_history: List of loss values\n",
    "        save_path: Path to save the plot\n",
    "        window_size: Window size for moving average\n",
    "    \"\"\"\n",
    "    if len(loss_history) < window_size:\n",
    "        return\n",
    "    \n",
    "    # Calculate moving average\n",
    "    moving_avg = []\n",
    "    for i in range(window_size - 1, len(loss_history)):\n",
    "        window = loss_history[i - window_size + 1:i + 1]\n",
    "        moving_avg.append(sum(window) / window_size)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "\n",
    "    # Plot moving average in bold\n",
    "    x_raw = [i * config.log_interval for i in range(len(loss_history))]\n",
    "    x_moving = [i * config.log_interval for i in range(window_size - 1, len(loss_history))]\n",
    "\n",
    "    # Plot raw loss in light color\n",
    "    plt.plot(x_raw, loss_history, alpha=0.3, color='blue', label='Raw Loss')\n",
    "    \n",
    "    # Plot moving average in bold\n",
    "    plt.plot(x_moving, moving_avg, color='red', linewidth=2, label=f'Moving Avg (window={window_size})')\n",
    "    plt.xlabel('Batch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss Over Time')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add statistics\n",
    "    if moving_avg:\n",
    "        current_avg = moving_avg[-1]\n",
    "        min_avg = min(moving_avg)\n",
    "        plt.axhline(y=current_avg, color='green', linestyle='--', alpha=0.5, label=f'Current: {current_avg:.4f}')\n",
    "        plt.axhline(y=min_avg, color='orange', linestyle='--', alpha=0.5, label=f'Min: {min_avg:.4f}')\n",
    "    \n",
    "    # if 'train_loader' in globals() and len(train_loader) > 0:\n",
    "    #     batches_per_epoch = len(train_loader)\n",
    "    #     num_epochs = len(loss_history) // batches_per_epoch\n",
    "    #     for epoch in range(1, num_epochs + 1):\n",
    "    #         epoch_batch = epoch * batches_per_epoch\n",
    "    #         if epoch_batch < len(loss_history):\n",
    "    #             plt.axvline(x=epoch_batch, color='gray', linestyle=':', alpha=0.5)\n",
    "    #             plt.text(epoch_batch, plt.ylim()[1] * 0.95, f'Epoch {epoch}', \n",
    "    #                     rotation=90, verticalalignment='top', fontsize=8, alpha=0.7)\n",
    "                \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=100)\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"  Loss graph saved to: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316365d9",
   "metadata": {},
   "source": [
    "# TRAINING LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "55eed306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint from: /home/hedge/Workspace/Code/img-classifier/model/checkpoints/resnet_clip_yolo_mean_teacher_20251101_130004/checkpoint_epoch2_step113365_acc0.1185_20251102_103555.pth\n",
      "Checkpoint loaded successfully\n",
      "Resuming from Epoch: 3 | Step: 113365\n",
      "Previous Val Acc: 0.1185 | Best Acc: 0.4647\n",
      "Loaded len(loss_history) = 22673\n"
     ]
    }
   ],
   "source": [
    "# checkpoint_path = \"./checkpoints/resnet_clip_yolo_mean_teacher_20251103_215223/checkpoint_epoch1_step84966_acc0.2893_20251104_161438.pth\"\n",
    "checkpoint_path = \"/home/hedge/Workspace/Code/img-classifier/model/checkpoints/resnet_clip_yolo_mean_teacher_20251101_130004/checkpoint_epoch2_step113365_acc0.1185_20251102_103555.pth\"\n",
    "epoch, global_step, loss_history, cls_loss_history, val_accuracy = load_checkpoint(\n",
    "    checkpoint_path, model, teacher_model, optimizer, scheduler,\n",
    "    config, monitor, device=config.device\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0e31d882",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4055"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion = ce_criterion\n",
    "validation_frequency = len(train_dataloader) // 7 # validate every X batches\n",
    "validation_frequency = validation_frequency - (validation_frequency % config.log_interval)\n",
    "validation_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbab8af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "19edf299",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW([\n",
    "    {'params': clip_params, 'lr': config.lr_backbone*0.02, 'name': 'clip'},\n",
    "    {'params': yolo_params, 'lr': config.lr_backbone*0.04, 'name': 'yolo'},\n",
    "    {'params': resnet_params, 'lr': config.lr_backbone*0.1, 'name': 'resnet'},\n",
    "    {'params': classifier_params, 'lr': config.initial_lr*0.1, 'name': 'classifier'}\n",
    "])\n",
    "\n",
    "# scheduler =  torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config.num_epochs, eta_min=1e-6)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.7, patience=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "72badbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ema_decay = 0.999\n",
    "# try warmup_steps = 60000 or more\n",
    "# low consistency weight = 0.2 NOT REALLY\n",
    "# teacher receives student buffers for batchnorms NOPE\n",
    "# try freezing backbone\n",
    "# lower consistency weight on 0 and 2\n",
    "# \"semi supervised\" start epoch\n",
    "\n",
    "# next - try high consistency weight - the magnitudes should be roughly equal ... not like 1:50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "49966e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# global_step = 0\n",
    "loss_history = []\n",
    "# cls_loss_history = []\n",
    "cons_loss_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470f61e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL IS INCAPABLE OF PREDICTING GOOD ON VIDS??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "27f5d3b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unfreezing backbone at step 84966\n",
      "Upsampling labeled data: 512053 samples * 2 = 1024106\n",
      "Unlabeled data: 1111234 samples\n",
      "Final dataset size: 2135340 samples\n",
      "Labeled ratio: 47.96%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:   1%|          | 189/35441 [02:21<6:48:20,  1.44it/s, loss=0.2127, cls=0.2377, cons=0.0109, mod_cls=0.2079, mod_cons=0.0048]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running intermittent validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Summary - Epoch step85155\n",
      "Accuracy: 0.2949 | Loss: 1.1354\n",
      "scheduler lr: [6.000000000000001e-07, 1.2000000000000002e-06, 3e-06, 9e-06]\n",
      "  Validation Accuracy: 0.2949\n",
      "  Best Accuracy: 0.3996\n",
      "  Loss graph saved to: ./logs/resnet_clip_yolo_mean_teacher_20251105_123053/loss_graph.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:   1%|          | 190/35441 [06:35<752:36:08, 76.86s/it, loss=0.2127, cls=0.2377, cons=0.0109, mod_cls=0.2079, mod_cons=0.0048]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Loss graph saved to: ./logs/resnet_clip_yolo_mean_teacher_20251105_123053/cls_loss_graph.png\n",
      "  Loss graph saved to: ./logs/resnet_clip_yolo_mean_teacher_20251105_123053/cons_loss_graph.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  12%|        | 4244/35441 [51:18<5:45:12,  1.51it/s, loss=0.1639, cls=0.2168, cons=0.0296, mod_cls=0.1488, mod_cons=0.0151] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running intermittent validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Summary - Epoch step89210\n",
      "Accuracy: 0.1675 | Loss: 1.2636\n",
      "scheduler lr: [6.000000000000001e-07, 1.2000000000000002e-06, 3e-06, 9e-06]\n",
      "  Validation Accuracy: 0.1675\n",
      "  Best Accuracy: 0.3996\n",
      "  Loss graph saved to: ./logs/resnet_clip_yolo_mean_teacher_20251105_123053/loss_graph.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  12%|        | 4245/35441 [55:34<671:11:31, 77.46s/it, loss=0.1639, cls=0.2168, cons=0.0296, mod_cls=0.1488, mod_cons=0.0151]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Loss graph saved to: ./logs/resnet_clip_yolo_mean_teacher_20251105_123053/cls_loss_graph.png\n",
      "  Loss graph saved to: ./logs/resnet_clip_yolo_mean_teacher_20251105_123053/cons_loss_graph.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  23%|       | 8299/35441 [1:41:07<4:55:38,  1.53it/s, loss=0.2030, cls=0.2194, cons=0.0356, mod_cls=0.1951, mod_cons=0.0079]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running intermittent validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Summary - Epoch step93265\n",
      "Accuracy: 0.1348 | Loss: 1.2850\n",
      "scheduler lr: [6.000000000000001e-07, 1.2000000000000002e-06, 3e-06, 9e-06]\n",
      "  Validation Accuracy: 0.1348\n",
      "  Best Accuracy: 0.3996\n",
      "  Loss graph saved to: ./logs/resnet_clip_yolo_mean_teacher_20251105_123053/loss_graph.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  23%|       | 8300/35441 [1:45:09<552:40:30, 73.31s/it, loss=0.2030, cls=0.2194, cons=0.0356, mod_cls=0.1951, mod_cons=0.0079]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Loss graph saved to: ./logs/resnet_clip_yolo_mean_teacher_20251105_123053/cls_loss_graph.png\n",
      "  Loss graph saved to: ./logs/resnet_clip_yolo_mean_teacher_20251105_123053/cons_loss_graph.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  35%|      | 12354/35441 [2:30:17<4:18:08,  1.49it/s, loss=0.1252, cls=0.1392, cons=0.0265, mod_cls=0.1159, mod_cons=0.0093] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running intermittent validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Summary - Epoch step97320\n",
      "Accuracy: 0.1496 | Loss: 1.3112\n",
      "scheduler lr: [4.2e-07, 8.4e-07, 2.1e-06, 6.3e-06]\n",
      "  Validation Accuracy: 0.1496\n",
      "  Best Accuracy: 0.3996\n",
      "  Loss graph saved to: ./logs/resnet_clip_yolo_mean_teacher_20251105_123053/loss_graph.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  35%|      | 12355/35441 [2:34:20<471:19:20, 73.50s/it, loss=0.1252, cls=0.1392, cons=0.0265, mod_cls=0.1159, mod_cons=0.0093]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Loss graph saved to: ./logs/resnet_clip_yolo_mean_teacher_20251105_123053/cls_loss_graph.png\n",
      "  Loss graph saved to: ./logs/resnet_clip_yolo_mean_teacher_20251105_123053/cons_loss_graph.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  46%|     | 16409/35441 [3:19:24<3:32:33,  1.49it/s, loss=0.2801, cls=0.2934, cons=0.0260, mod_cls=0.2749, mod_cons=0.0053]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running intermittent validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Summary - Epoch step101375\n",
      "Accuracy: 0.1496 | Loss: 1.4420\n",
      "scheduler lr: [4.2e-07, 8.4e-07, 2.1e-06, 6.3e-06]\n",
      "  Validation Accuracy: 0.1496\n",
      "  Best Accuracy: 0.3996\n",
      "  Loss graph saved to: ./logs/resnet_clip_yolo_mean_teacher_20251105_123053/loss_graph.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  46%|     | 16410/35441 [3:23:27<388:01:50, 73.40s/it, loss=0.2801, cls=0.2934, cons=0.0260, mod_cls=0.2749, mod_cons=0.0053]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Loss graph saved to: ./logs/resnet_clip_yolo_mean_teacher_20251105_123053/cls_loss_graph.png\n",
      "  Loss graph saved to: ./logs/resnet_clip_yolo_mean_teacher_20251105_123053/cons_loss_graph.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  58%|    | 20464/35441 [4:08:30<2:46:59,  1.49it/s, loss=0.2184, cls=0.2179, cons=0.0759, mod_cls=0.1986, mod_cons=0.0198]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running intermittent validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Summary - Epoch step105430\n",
      "Accuracy: 0.1403 | Loss: 1.6062\n",
      "scheduler lr: [4.2e-07, 8.4e-07, 2.1e-06, 6.3e-06]\n",
      "  Validation Accuracy: 0.1403\n",
      "  Best Accuracy: 0.3996\n",
      "  Loss graph saved to: ./logs/resnet_clip_yolo_mean_teacher_20251105_123053/loss_graph.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  58%|    | 20465/35441 [4:12:33<305:25:37, 73.42s/it, loss=0.2184, cls=0.2179, cons=0.0759, mod_cls=0.1986, mod_cons=0.0198]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Loss graph saved to: ./logs/resnet_clip_yolo_mean_teacher_20251105_123053/cls_loss_graph.png\n",
      "  Loss graph saved to: ./logs/resnet_clip_yolo_mean_teacher_20251105_123053/cons_loss_graph.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  69%|   | 24519/35441 [4:57:36<2:01:32,  1.50it/s, loss=0.1573, cls=0.1632, cons=0.0183, mod_cls=0.1518, mod_cons=0.0056]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running intermittent validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Summary - Epoch step109485\n",
      "Accuracy: 0.1389 | Loss: 1.7532\n",
      "scheduler lr: [2.9399999999999996e-07, 5.879999999999999e-07, 1.4699999999999997e-06, 4.409999999999999e-06]\n",
      "  Validation Accuracy: 0.1389\n",
      "  Best Accuracy: 0.3996\n",
      "  Loss graph saved to: ./logs/resnet_clip_yolo_mean_teacher_20251105_123053/loss_graph.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  69%|   | 24520/35441 [5:01:38<223:06:57, 73.55s/it, loss=0.1573, cls=0.1632, cons=0.0183, mod_cls=0.1518, mod_cons=0.0056]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Loss graph saved to: ./logs/resnet_clip_yolo_mean_teacher_20251105_123053/cls_loss_graph.png\n",
      "  Loss graph saved to: ./logs/resnet_clip_yolo_mean_teacher_20251105_123053/cons_loss_graph.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  81%|  | 28574/35441 [5:46:29<1:13:28,  1.56it/s, loss=0.1784, cls=0.1915, cons=0.0408, mod_cls=0.1702, mod_cons=0.0081]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running intermittent validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Summary - Epoch step113540\n",
      "Accuracy: 0.1465 | Loss: 1.8494\n",
      "scheduler lr: [2.9399999999999996e-07, 5.879999999999999e-07, 1.4699999999999997e-06, 4.409999999999999e-06]\n",
      "  Validation Accuracy: 0.1465\n",
      "  Best Accuracy: 0.3996\n",
      "  Loss graph saved to: ./logs/resnet_clip_yolo_mean_teacher_20251105_123053/loss_graph.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  81%|  | 28575/35441 [5:50:30<139:21:31, 73.07s/it, loss=0.1784, cls=0.1915, cons=0.0408, mod_cls=0.1702, mod_cons=0.0081]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Loss graph saved to: ./logs/resnet_clip_yolo_mean_teacher_20251105_123053/cls_loss_graph.png\n",
      "  Loss graph saved to: ./logs/resnet_clip_yolo_mean_teacher_20251105_123053/cons_loss_graph.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  92%|| 32629/35441 [6:34:06<30:03,  1.56it/s, loss=0.1612, cls=0.1997, cons=0.0165, mod_cls=0.1562, mod_cons=0.0050]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running intermittent validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Summary - Epoch step117595\n",
      "Accuracy: 0.1411 | Loss: 1.9657\n",
      "scheduler lr: [2.9399999999999996e-07, 5.879999999999999e-07, 1.4699999999999997e-06, 4.409999999999999e-06]\n",
      "  Validation Accuracy: 0.1411\n",
      "  Best Accuracy: 0.3996\n",
      "  Loss graph saved to: ./logs/resnet_clip_yolo_mean_teacher_20251105_123053/loss_graph.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  92%|| 32630/35441 [6:38:08<57:10:29, 73.22s/it, loss=0.1612, cls=0.1997, cons=0.0165, mod_cls=0.1562, mod_cons=0.0050]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Loss graph saved to: ./logs/resnet_clip_yolo_mean_teacher_20251105_123053/cls_loss_graph.png\n",
      "  Loss graph saved to: ./logs/resnet_clip_yolo_mean_teacher_20251105_123053/cons_loss_graph.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|| 35441/35441 [7:08:22<00:00,  1.38it/s, loss=0.1766, cls=0.2245, cons=0.0298, mod_cls=0.1636, mod_cons=0.0130]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Summary:\n",
      "  Time: 25708.1s\n",
      "  Avg Loss: 0.2046\n",
      "  Avg Classification Loss: 0.2207\n",
      "  Avg Consistency Loss: 0.0553\n",
      "================================================================================\n",
      "Running end of epoch validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Summary - Epoch 2\n",
      "Accuracy: 0.1377 | Loss: 2.0605\n",
      "  Validation Accuracy: 0.1377\n",
      "  Best Accuracy: 0.3996\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Checkpoint saved successfully: ./checkpoints/resnet_clip_yolo_mean_teacher_20251105_123053/checkpoint_epoch2_step120407_acc0.1377_20251105_194332.pth\n",
      "Epoch: 2 | Step: 120407 | Val Acc: 0.1377\n",
      "scheduler lr: [2.0579999999999995e-07, 4.115999999999999e-07, 1.0289999999999998e-06, 3.0869999999999994e-06]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:   4%|         | 1243/28399 [13:33<4:55:39,  1.53it/s, loss=0.1041, cls=0.1419, cons=0.0256, mod_cls=0.0949, mod_cons=0.0092]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running intermittent validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Summary - Epoch step121650\n",
      "Accuracy: 0.0261 | Loss: 1.9105\n",
      "scheduler lr: [2.0579999999999995e-07, 4.115999999999999e-07, 1.0289999999999998e-06, 3.0869999999999994e-06]\n",
      "  Validation Accuracy: 0.0261\n",
      "  Best Accuracy: 0.3996\n",
      "  Loss graph saved to: ./logs/resnet_clip_yolo_mean_teacher_20251105_123053/loss_graph.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:   4%|         | 1244/28399 [17:33<548:56:42, 72.77s/it, loss=0.1041, cls=0.1419, cons=0.0256, mod_cls=0.0949, mod_cons=0.0092]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Loss graph saved to: ./logs/resnet_clip_yolo_mean_teacher_20251105_123053/cls_loss_graph.png\n",
      "  Loss graph saved to: ./logs/resnet_clip_yolo_mean_teacher_20251105_123053/cons_loss_graph.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  19%|        | 5298/28399 [1:01:19<4:04:23,  1.58it/s, loss=0.1624, cls=0.1146, cons=0.1686, mod_cls=0.0837, mod_cons=0.0788]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running intermittent validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Summary - Epoch step125705\n",
      "Accuracy: -0.0461 | Loss: 2.1273\n",
      "scheduler lr: [2.0579999999999995e-07, 4.115999999999999e-07, 1.0289999999999998e-06, 3.0869999999999994e-06]\n",
      "  Validation Accuracy: -0.0461\n",
      "  Best Accuracy: 0.3996\n",
      "  Loss graph saved to: ./logs/resnet_clip_yolo_mean_teacher_20251105_123053/loss_graph.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  19%|        | 5299/28399 [1:05:22<470:47:01, 73.37s/it, loss=0.1624, cls=0.1146, cons=0.1686, mod_cls=0.0837, mod_cons=0.0788]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Loss graph saved to: ./logs/resnet_clip_yolo_mean_teacher_20251105_123053/cls_loss_graph.png\n",
      "  Loss graph saved to: ./logs/resnet_clip_yolo_mean_teacher_20251105_123053/cons_loss_graph.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  21%|        | 6029/28399 [1:14:06<4:34:59,  1.36it/s, loss=0.1360, cls=0.1719, cons=0.0362, mod_cls=0.1148, mod_cons=0.0211]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 61\u001b[39m\n\u001b[32m     58\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m clip_params + yolo_params:\n\u001b[32m     59\u001b[39m         param.requires_grad = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m batch_start = \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtime\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[38;5;66;03m# 1. Grab data from dataloader\u001b[39;00m\n\u001b[32m     64\u001b[39m clip_weak, clip_strong, yolo_weak, yolo_strong, labels, metadata = batch_input\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# monitor mode collapse\n",
    "recent_predictions = [] \n",
    "max_recent_batches = 15\n",
    "\n",
    "# if epoch == config.freeze_until_epoch:\n",
    "if global_step >= 500:\n",
    "    # print(f\"Unfreezing backbone at epoch {epoch}\")\n",
    "    print(f\"unfreezing backbone at step {global_step}\")\n",
    "    for param in clip_params + yolo_params:\n",
    "        param.requires_grad = True\n",
    "\n",
    "for epoch in range(config.cur_epoch, config.num_epochs):\n",
    "    collapse_flag = 0 # model pred mode collapse\n",
    "    epoch_start = time.time()\n",
    "\n",
    "    # region Train one epoch ########################################\n",
    "    model.train()\n",
    "    teacher_model.train()\n",
    "    # Cumulative lists for dataframes\n",
    "    # image_data = [] if track_images else None\n",
    "    batch_data = []\n",
    "\n",
    "    if epoch <= 5:\n",
    "        # train_loader = supervised_train_dataloader\n",
    "        ratios = [0.66, 0.55, 0.44, 0.33, 0.1]\n",
    "        ratios = [3, 3, 2, None]\n",
    "        supervised_train_dataset = MeanTeacherDataset(\n",
    "            csv_file = \"train_2.csv\", \n",
    "            root_dir = \"~/Workspace/data-v2/train\",\n",
    "            # supervised = True,\n",
    "            # supervised_ratio = ratios[epoch],\n",
    "            upsample = ratios[epoch],\n",
    "        )\n",
    "        supervised_sampler = RandomVersionSampler(supervised_train_dataset)\n",
    "        train_loader = DataLoader(\n",
    "            supervised_train_dataset, \n",
    "            batch_size=config.batch_size, \n",
    "            sampler=supervised_sampler,\n",
    "            collate_fn=custom_collate_fn,\n",
    "            num_workers=get_num_workers(),\n",
    "            persistent_workers=False, # True if get_num_workers() > 0 else False,\n",
    "            # pin_memory=False, # WSL does not support pin_memory well\n",
    "            prefetch_factor=2 if get_num_workers() > 0 else None,\n",
    "        )\n",
    "        # print(ratios[epoch], len(train_loader))\n",
    "    else: \n",
    "        train_loader = train_dataloader\n",
    "    if epoch <= 0:\n",
    "        criterion = af_criterion\n",
    "    else:\n",
    "        criterion = af_criterion2\n",
    "    # criterion = af_criterion\n",
    "    \n",
    "    epoch_pbar = tqdm(train_loader, desc=f'Epoch {epoch}', total=len(train_loader))\n",
    "    for batch_num, batch_input in enumerate(epoch_pbar):\n",
    "        if global_step == 500 : #len(train_dataloader)//4:\n",
    "            print(f\"unfreezing backbone at step {global_step}\")\n",
    "            for param in clip_params + yolo_params:\n",
    "                param.requires_grad = True\n",
    "                \n",
    "        batch_start = time.time()\n",
    "\n",
    "        # 1. Grab data from dataloader\n",
    "        clip_weak, clip_strong, yolo_weak, yolo_strong, labels, metadata = batch_input\n",
    "        clip_weak['pixel_values'] = clip_weak['pixel_values'].to(device, non_blocking=True)\n",
    "        clip_strong['pixel_values'] = clip_strong['pixel_values'].to(device, non_blocking=True)\n",
    "        yolo_weak = yolo_weak.to(device, non_blocking=True)\n",
    "        yolo_strong = yolo_strong.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "        \n",
    "        # 2. Forward pass \n",
    "        with torch.no_grad():\n",
    "            with autocast(device_type='cuda', enabled=False):\n",
    "                teacher_outputs = teacher_model(clip_weak, yolo_weak)\n",
    "        with autocast(device_type='cuda'):\n",
    "            student_outputs = model(clip_strong, yolo_strong)\n",
    "\n",
    "            \n",
    "            cls_loss = criterion(student_outputs, labels, reduction='none') # [B]\n",
    "            \n",
    "            cons_loss = compute_consistency_loss(\n",
    "                student_outputs, \n",
    "                teacher_outputs, \n",
    "                entropy_threshold=max(0.25, 0.7 - global_step / 90000)\n",
    "            ) # [B]\n",
    "\n",
    "            teacher_confidence = compute_teacher_confidence(teacher_outputs, mode='entropy') # [B]\n",
    "            # Confidence-based weighting for supervised loss\n",
    "            supervised_confidence_threshold = 0.2\n",
    "            teacher_confidence = torch.clamp(\n",
    "                (teacher_confidence - supervised_confidence_threshold) / (1.0 - supervised_confidence_threshold),\n",
    "                min=0.0,\n",
    "                max=1.0\n",
    "            ) \n",
    "            # we just have linear scaling for simplicity for now\n",
    "            \n",
    "            # Create per-image consistency weights with warmup # currently sigmoid scheduling\n",
    "            if global_step > config.warmup_steps:\n",
    "                warmup_factor = 1.0\n",
    "            else:\n",
    "                phase = 1.0 - global_step / config.warmup_steps # p(x) = 1 - x/30000\n",
    "                warmup_factor = np.exp(-5.0 * phase * phase) # f(x) = e^(-5*p(x)*p(x))\n",
    "\n",
    "            unlabeled_mask = (labels == 1)\n",
    "            labeled_mask = ~unlabeled_mask\n",
    "            supervised_factor = torch.zeros_like(cls_loss)\n",
    "            supervised_factor[labeled_mask] =  1.0\n",
    "            supervised_factor[unlabeled_mask] = 0.5\n",
    "            unsupervised_factor = torch.zeros_like(cls_loss)\n",
    "            unsupervised_factor[labeled_mask] =  1.0\n",
    "            unsupervised_factor[unlabeled_mask] = 1.5\n",
    "\n",
    "\n",
    "            alpha = teacher_confidence * warmup_factor\n",
    "            # (0.5)*cls_loss + (1-alpha)*cls_loss + (alpha)*cons_loss # keep using a baseline amt of cls_loss\n",
    "            # where cls_loss is halved when img is unlabeled, and cons_loss is halved when img is labeled\n",
    "            supervised_loss = (1.5 - alpha) * cls_loss * supervised_factor\n",
    "            consistency_loss = (alpha) * cons_loss * unsupervised_factor\n",
    "            per_sample_loss = supervised_loss + consistency_loss\n",
    "            loss = per_sample_loss.mean()\n",
    "        \n",
    "        # 3. Backward pass\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        \n",
    "        # 4. Update teacher model with EMA\n",
    "        update_ema_variables(model, teacher_model, alpha=config.ema_decay, global_step=global_step)\n",
    "        \n",
    "        if global_step % config.log_interval != 0:\n",
    "            global_step += 1\n",
    "            continue\n",
    "        \n",
    "        # monitor mode collapse\n",
    "        with torch.no_grad():\n",
    "            current_predictions = F.softmax(student_outputs, dim=1).argmax(dim=1).cpu().numpy()\n",
    "        recent_predictions.append(current_predictions)\n",
    "        if len(recent_predictions) > max_recent_batches:\n",
    "            recent_predictions.pop(0)\n",
    "        all_recent_preds = np.concatenate(recent_predictions)\n",
    "        unique, counts = np.unique(all_recent_preds, return_counts=True)\n",
    "        dominant_class = unique[np.argmax(counts)]\n",
    "        dominant_ratio = counts.max() / len(all_recent_preds)\n",
    "        if collapse_flag<=10 and dominant_ratio > 0.95:  # If >95% of predictions are same class\n",
    "            print(f\"[WARN] Possible mode collapse at step {global_step}!\")\n",
    "            print(f\"       Class {dominant_class} represents {dominant_ratio:.1%} of last {len(all_recent_preds)} predictions\")\n",
    "            print(f\"       Distribution: {dict(zip(unique, counts))}\")\n",
    "            collapse_flag += 1\n",
    "\n",
    "        # 5. Calculate metrics #############################################\n",
    "        batch_time = time.time() - batch_start\n",
    "        images_per_second = config.batch_size / batch_time\n",
    "        \n",
    "        # Move to CPU for storage\n",
    "        loss_cpu = loss.item()\n",
    "        cls_loss_cpu = cls_loss.mean().item()\n",
    "        cons_loss_cpu = cons_loss.mean().item()\n",
    "        \n",
    "        loss_history.append(loss_cpu)\n",
    "        cls_loss_history.append(cls_loss_cpu)\n",
    "        cons_loss_history.append(cons_loss_cpu)\n",
    "\n",
    "        # Add batch-level data\n",
    "        batch_data.append({\n",
    "            'batch_num': batch_num,\n",
    "            'epoch': epoch,\n",
    "            'global_step': global_step,\n",
    "            'loss': loss_cpu,\n",
    "            'cls_loss': cls_loss_cpu,\n",
    "            'consistency_loss': cons_loss_cpu,\n",
    "            # 'batch_time': batch_time,\n",
    "            # 'images_per_second': images_per_second,\n",
    "        })\n",
    "        \n",
    "        # Update progress bar\n",
    "        epoch_pbar.set_postfix({\n",
    "            'loss': f'{loss_cpu:.4f}',\n",
    "            'cls': f'{cls_loss_cpu:.4f}',\n",
    "            'cons': f'{cons_loss_cpu:.4f}',\n",
    "            'mod_cls': f'{supervised_loss.mean().item():.4f}',\n",
    "            'mod_cons': f'{consistency_loss.mean().item():.4f}',\n",
    "        })\n",
    "        \n",
    "        # Log to tensorboard/CSV at intervals\n",
    "        if global_step % config.log_interval == 0:\n",
    "            current_lr = optimizer.param_groups[-1]['lr'] # should correspond to my FC classifier layers\n",
    "            \n",
    "            train_metrics = {\n",
    "                \"train/loss\": loss_cpu,\n",
    "                \"train/cls_loss\": cls_loss_cpu,\n",
    "                \"train/consistency\": cons_loss_cpu,\n",
    "                \"train/learning_rate\": current_lr,\n",
    "                # \"train/images_per_second\": images_per_second,\n",
    "                \"system/gpu_memory_mb\": get_gpu_memory_usage()\n",
    "            }\n",
    "            \n",
    "            logger.log_metrics(train_metrics, global_step)\n",
    "            logger.log_train_step(global_step, epoch, {\n",
    "                'loss': loss_cpu,\n",
    "                'cls_loss': cls_loss_cpu,\n",
    "                'cons_loss': cons_loss_cpu,\n",
    "                'mod_cls_loss': supervised_loss.mean().item(),\n",
    "                'mod_cons_loss': consistency_loss.mean().item(),\n",
    "            })\n",
    "\n",
    "        # 6. Intermittent validation\n",
    "        if global_step > 0 and global_step % validation_frequency == 0:\n",
    "            intermittent_epoch = \"step\" + str(global_step)\n",
    "            print(\"Running intermittent validation...\")\n",
    "            val_image_df, val_batch_df, val_loss = validate(teacher_model, val_dataloader, config, intermittent_epoch)\n",
    "            val_accuracy, val_metrics, cm = analyse(val_image_df, val_batch_df, config, monitor, intermittent_epoch)\n",
    "            scheduler.step(val_accuracy)\n",
    "            print('scheduler lr:', scheduler.get_last_lr())\n",
    "\n",
    "            # logger.log_metrics(val_metrics, epoch)\n",
    "            logger.log_validation(intermittent_epoch, val_metrics)\n",
    "            logger.log_confusion_matrix(cm, ['bad', 'neutral', 'good'], intermittent_epoch)\n",
    "            \n",
    "            print(f\"  Validation Accuracy: {val_accuracy:.4f}\")\n",
    "            print(f\"  Best Accuracy: {monitor.best_accuracy:.4f}\")\n",
    "\n",
    "            if val_accuracy >= monitor.best_accuracy:    \n",
    "                save_checkpoint(\n",
    "                    model, teacher_model, optimizer, scheduler, epoch, global_step, \n",
    "                    config, val_accuracy, monitor, loss_history, cls_loss_history\n",
    "                )\n",
    "            \n",
    "            plot_running_loss(loss_history, os.path.join(logger.get_log_dir(), f'loss_graph.png'))\n",
    "            plot_running_loss(cls_loss_history, os.path.join(logger.get_log_dir(), f'cls_loss_graph.png'))\n",
    "            plot_running_loss(cons_loss_history, os.path.join(logger.get_log_dir(), f'cons_loss_graph.png'))\n",
    "        global_step += 1\n",
    "\n",
    "    # Create dataframes\n",
    "    # image_df = pd.DataFrame(image_data) if track_images else None\n",
    "    batch_df = pd.DataFrame(batch_data)\n",
    "\n",
    "    train_image_df = None # image_df\n",
    "    train_batch_df = batch_df\n",
    "\n",
    "    # endregion #####################################################\n",
    "    \n",
    "    # Analyze epoch\n",
    "    epoch_metrics = analyse_epoch(train_image_df, train_batch_df, config, epoch)\n",
    "    logger.log_metrics(epoch_metrics, epoch)\n",
    "    \n",
    "    print(f\"Epoch {epoch} Summary:\")\n",
    "    print(f\"  Time: {(time.time() - epoch_start):.1f}s\")\n",
    "    print(f\"  Avg Loss: {epoch_metrics['epoch/loss']:.4f}\")\n",
    "    print(f\"  Avg Classification Loss: {epoch_metrics['epoch/cls_loss']:.4f}\")\n",
    "    print(f\"  Avg Consistency Loss: {epoch_metrics['epoch/consistency_loss']:.4f}\")\n",
    "    # print(f\"  Throughput: {epoch_metrics['epoch/avg_throughput']:.1f} images/second\")\n",
    "\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Validation\n",
    "    print(\"Running end of epoch validation...\")\n",
    "    val_image_df, val_batch_df, val_loss = validate(teacher_model, val_dataloader, config, epoch)\n",
    "    val_accuracy, val_metrics, cm = analyse(val_image_df, val_batch_df, config, monitor, epoch)\n",
    "    \n",
    "    logger.log_metrics(val_metrics, epoch)\n",
    "    logger.log_validation(epoch, val_metrics)\n",
    "    logger.log_confusion_matrix(cm, ['bad', 'neutral', 'good'], epoch)\n",
    "    \n",
    "    print(f\"  Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    print(f\"  Best Accuracy: {monitor.best_accuracy:.4f}\")\n",
    "    print(\"=\"*80)\n",
    "    print()\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    save_checkpoint(\n",
    "        model, teacher_model, optimizer, scheduler, epoch, global_step, \n",
    "        config, val_accuracy, monitor, loss_history, cls_loss_history\n",
    "    )\n",
    "    \n",
    "    scheduler.step(val_accuracy)\n",
    "    print('scheduler lr:', scheduler.get_last_lr())\n",
    "\n",
    "plot_running_loss(loss_history, os.path.join(logger.get_log_dir(), f'loss_graph.png'))\n",
    "plot_running_loss(cls_loss_history, os.path.join(logger.get_log_dir(), f'cls_loss_graph.png'))\n",
    "plot_running_loss(cons_loss_history, os.path.join(logger.get_log_dir(), f'cons_loss_graph.png'))\n",
    "logger.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62345d12",
   "metadata": {},
   "source": [
    "### the lands down under"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2642223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my loss function focal should change every epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9151b259",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Summary - Epoch -1\n",
      "Accuracy: 0.1185 | Loss: 4.2157\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 5550,  5513,   344],\n",
       "       [ 7076, 15780,   292],\n",
       "       [  747,  2585,  1517]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_image_df, val_batch_df, val_total_loss = validate(teacher_model, val_dataloader, config, -1)\n",
    "val_accuracy, val_metrics, cm = analyse(val_image_df, val_batch_df, config, monitor, -1)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e7216d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_as_bad = val_image_df[(val_image_df['label'] == 2) & (val_image_df['prediction'] == 0)]\n",
    "bad_as_good = val_image_df[(val_image_df['label'] == 0) & (val_image_df['prediction'] == 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "95d756b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ACCURACY PER SUBDIRECTORY (k=20 samples)\n",
      "================================================================================\n",
      "\n",
      "Subdirectory                   Total    Correct  Accuracy   Album OK   True Label  \n",
      "--------------------------------------------------------------------------------\n",
      "BAD\\186bRmb5                   20       20       100.00%    YES        BAD         \n",
      "BAD\\7k8dvzfY                   9        9        100.00%    YES        BAD         \n",
      "BAD\\5jshYuFd                   18       18       100.00%    YES        BAD         \n",
      "BAD\\KXiRbrjZ                   9        9        100.00%    YES        BAD         \n",
      "BAD\\YYvOAYiT                   20       20       100.00%    YES        BAD         \n",
      "BAD\\YLRnvEUM                   3        3        100.00%    YES        BAD         \n",
      "BAD\\aR2IzMl1                   2        2        100.00%    YES        BAD         \n",
      "BAD\\pFh8tp7T                   6        6        100.00%    YES        BAD         \n",
      "BAD\\Y5RlaWdj                   20       19       95.00%     YES        BAD         \n",
      "BAD\\UWETzgci                   20       18       90.00%     YES        BAD         \n",
      "BAD\\FHuzuQnA                   20       18       90.00%     YES        BAD         \n",
      "BAD\\YDsrmLY1                   20       18       90.00%     YES        BAD         \n",
      "BAD\\JtynsQER                   20       17       85.00%     YES        BAD         \n",
      "BAD\\Ymg8sp9D                   20       17       85.00%     YES        BAD         \n",
      "BAD\\jlp8xmUk                   20       17       85.00%     YES        BAD         \n",
      "BAD\\hIxvT3bF                   20       16       80.00%     YES        BAD         \n",
      "BAD\\sNQbi7vv                   20       15       75.00%     YES        BAD         \n",
      "BAD\\gqSIlwDh                   20       15       75.00%     YES        BAD         \n",
      "BAD\\EvggLR4G                   20       15       75.00%     YES        BAD         \n",
      "BAD\\AEYhFQ48                   20       15       75.00%     YES        BAD         \n",
      "BAD\\jx9YtPzT                   20       14       70.00%     YES        BAD         \n",
      "BAD\\LldXlG26                   20       14       70.00%     YES        BAD         \n",
      "BAD\\qcjY9n0Z                   20       14       70.00%     YES        BAD         \n",
      "BAD\\28hT3uvy                   20       14       70.00%     YES        BAD         \n",
      "BAD\\NJJhds4q                   18       12       66.67%     YES        BAD         \n",
      "BAD\\cJwDSfbl                   9        6        66.67%     YES        BAD         \n",
      "BAD\\wHnN4BUQ                   3        2        66.67%     YES        BAD         \n",
      "BAD\\dPvpOqZy                   20       13       65.00%     YES        BAD         \n",
      "BAD\\FZj2FdKC                   20       13       65.00%     YES        BAD         \n",
      "BAD\\NR2BfPbk                   20       13       65.00%     YES        BAD         \n",
      "BAD\\jij3FtYE                   20       13       65.00%     YES        BAD         \n",
      "BAD\\wzoHKHSs                   20       13       65.00%     YES        BAD         \n",
      "BAD\\HeDl5axh                   20       13       65.00%     YES        BAD         \n",
      "BAD\\1tGDUSlK                   20       12       60.00%     YES        BAD         \n",
      "BAD\\VC4oHVg1                   20       11       55.00%     YES        BAD         \n",
      "BAD\\ECJx80YD                   20       11       55.00%     YES        BAD         \n",
      "BAD\\KHaWM5pA                   20       11       55.00%     YES        BAD         \n",
      "BAD\\Xo96s8GW                   20       11       55.00%     YES        BAD         \n",
      "BAD\\mfcJtVC7                   20       10       50.00%     YES        BAD         \n",
      "BAD\\LlZwZmHX                   6        3        50.00%     YES        BAD         \n",
      "BAD\\0EoWgmx7                   20       10       50.00%     YES        BAD         \n",
      "BAD\\PjRwdBO2                   20       9        45.00%     NO         BAD         \n",
      "BAD\\I4Bx6kQ3                   20       9        45.00%     NO         BAD         \n",
      "BAD\\pCs17zNF                   20       9        45.00%     NO         BAD         \n",
      "BAD\\B5Zl18Db                   20       9        45.00%     NO         BAD         \n",
      "BAD\\XqyCCPfo                   20       9        45.00%     NO         BAD         \n",
      "BAD\\48YjpvBw                   9        4        44.44%     NO         BAD         \n",
      "BAD\\0jynzF5y                   20       8        40.00%     NO         BAD         \n",
      "BAD\\NjM3SP6C                   20       8        40.00%     NO         BAD         \n",
      "BAD\\MjU36SYA                   20       8        40.00%     NO         BAD         \n",
      "BAD\\8EQR5gCN                   13       5        38.46%     NO         BAD         \n",
      "BAD\\21sWdYsx                   20       7        35.00%     NO         BAD         \n",
      "BAD\\ceb96lBn                   20       7        35.00%     NO         BAD         \n",
      "BAD\\rUyxUINU                   20       7        35.00%     NO         BAD         \n",
      "BAD\\OBOI2iIW                   20       7        35.00%     NO         BAD         \n",
      "BAD\\HaFGuvSU                   20       7        35.00%     NO         BAD         \n",
      "BAD\\2UpxarRQ                   20       7        35.00%     NO         BAD         \n",
      "BAD\\n1jsH1OL                   3        1        33.33%     NO         BAD         \n",
      "BAD\\k1zTLufk                   20       6        30.00%     NO         BAD         \n",
      "BAD\\MQMht2Oc                   20       6        30.00%     NO         BAD         \n",
      "BAD\\wLimZ1bR                   20       6        30.00%     NO         BAD         \n",
      "BAD\\STTo9RmF                   20       5        25.00%     NO         BAD         \n",
      "BAD\\xnbywvL8                   20       5        25.00%     NO         BAD         \n",
      "BAD\\tm70aPw1                   20       5        25.00%     NO         BAD         \n",
      "BAD\\D5D1Fc3U                   20       5        25.00%     NO         BAD         \n",
      "BAD\\Nm0syYOh                   20       4        20.00%     NO         BAD         \n",
      "BAD\\Kho9QIFx                   20       4        20.00%     NO         BAD         \n",
      "BAD\\m9ByCReE                   20       4        20.00%     NO         BAD         \n",
      "BAD\\9r9kr63o                   20       3        15.00%     NO         BAD         \n",
      "BAD\\4Bel8zPG                   20       3        15.00%     NO         BAD         \n",
      "BAD\\ZRcFD6xD                   20       3        15.00%     NO         BAD         \n",
      "BAD\\G9xyBIdY                   9        1        11.11%     NO         BAD         \n",
      "BAD\\nxohmURF                   12       1        8.33%      NO         BAD         \n",
      "BAD\\Dc3LWaFs                   20       1        5.00%      NO         BAD         \n",
      "BAD\\ZtVzZitk                   20       1        5.00%      NO         BAD         \n",
      "BAD\\9PQwHq5S                   20       1        5.00%      NO         BAD         \n",
      "BAD\\MaGp5FLD                   20       1        5.00%      NO         BAD         \n",
      "BAD\\DDc3YKoq                   20       0        0.00%      NO         BAD         \n",
      "BAD\\9nPT4KNj                   20       0        0.00%      NO         BAD         \n",
      "BAD\\A1Uhb05S                   6        0        0.00%      NO         BAD         \n",
      "BAD\\UqR25MfP                   20       0        0.00%      NO         BAD         \n",
      "BAD\\QlSVbmPt                   6        0        0.00%      NO         BAD         \n",
      "BAD\\Eu3Av3pf                   16       0        0.00%      NO         BAD         \n",
      "BAD\\lVzuYhQF                   6        0        0.00%      NO         BAD         \n",
      "\n",
      "================================================================================\n",
      "Overall Image Accuracy (on sampled data): 48.26%\n",
      "Total subdirectories: 84\n",
      "Total images analyzed: 1463\n",
      "Album Accuracy (>=50% correct on sampled data): 41 / 84 = 48.81%\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_40954/1115296504.py:45: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  subdir_stats = sampled_df.groupby('subdirectory').apply(lambda x: pd.Series({\n"
     ]
    }
   ],
   "source": [
    "def analyze_subdirectory_accuracy(df, k=None, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Analyze accuracy per subdirectory with optional sampling.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with columns ['img_path', 'label', 'prediction']\n",
    "        k: Number of samples per subdirectory (None = use all images)\n",
    "        threshold: Accuracy threshold for album correctness (default 0.5)\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (sampled_df, subdir_stats)\n",
    "    \"\"\"\n",
    "    # Extract subdirectory from img_path\n",
    "    def extract_subdirectory(img_path):\n",
    "        \"\"\"Extract the subdirectory group (e.g., 'BAD\\\\0EoWgmx7' from path)\"\"\"\n",
    "        try:\n",
    "            parts = img_path.replace('/', '\\\\').split('\\\\')\n",
    "            for i, part in enumerate(parts):\n",
    "                if part in ['BAD']:  # Add 'GOOD', 'NEUTRAL' if needed\n",
    "                    if i + 1 < len(parts):\n",
    "                        return f\"{part}\\\\{parts[i+1]}\"\n",
    "            return None\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    # Create a copy to avoid modifying original\n",
    "    df = df.copy()\n",
    "    df['subdirectory'] = df['img_path'].apply(extract_subdirectory)\n",
    "    \n",
    "    # Sample k rows from each subdirectory if k is specified\n",
    "    if k is not None:\n",
    "        sampled_dfs = []\n",
    "        for subdir, group in df.groupby('subdirectory'):\n",
    "            if len(group) >= k:\n",
    "                sampled = group.sample(n=k)\n",
    "            else:\n",
    "                # If subdirectory has fewer than k images, take all of them\n",
    "                sampled = group\n",
    "            sampled_dfs.append(sampled)\n",
    "        sampled_df = pd.concat(sampled_dfs, ignore_index=True)\n",
    "    else:\n",
    "        sampled_df = df\n",
    "    \n",
    "    # Calculate accuracy per subdirectory ON THE SAMPLED DATA\n",
    "    subdir_stats = sampled_df.groupby('subdirectory').apply(lambda x: pd.Series({\n",
    "        'total': len(x),\n",
    "        'correct': (x['label'] == x['prediction']).sum(),\n",
    "        'accuracy': (x['label'] == x['prediction']).sum() / len(x),\n",
    "        'true_label': x['label'].iloc[0] if len(x) > 0 else None\n",
    "    })).reset_index()\n",
    "    \n",
    "    # Album correctness based on sampled data accuracy\n",
    "    subdir_stats['album_correct'] = (subdir_stats['accuracy'] >= threshold).astype(int)\n",
    "    subdir_stats = subdir_stats.sort_values('accuracy', ascending=False)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"ACCURACY PER SUBDIRECTORY{f' (k={k} samples)' if k else ''}\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\n{'Subdirectory':<30} {'Total':<8} {'Correct':<8} {'Accuracy':<10} {'Album OK':<10} {'True Label':<12}\")\n",
    "    print(\"-\"*80)\n",
    "    for _, row in subdir_stats.iterrows():\n",
    "        label_name = {0: 'BAD', 1: 'NEUTRAL', 2: 'GOOD'}.get(row['true_label'], 'UNKNOWN')\n",
    "        album_ok = 'YES' if row['album_correct'] else 'NO'\n",
    "        print(f\"{str(row['subdirectory']):<30} {row['total']:<8.0f} {row['correct']:<8.0f} {row['accuracy']:<10.2%} {album_ok:<10} {label_name:<12}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"Overall Image Accuracy (on {'sampled' if k else 'all'} data): {sampled_df['label'].eq(sampled_df['prediction']).mean():.2%}\")\n",
    "    print(f\"Total subdirectories: {len(subdir_stats)}\")\n",
    "    print(f\"Total images analyzed: {len(sampled_df)}\")\n",
    "    print(f\"Album Accuracy (>={threshold:.0%} correct on {'sampled' if k else 'all'} data): {subdir_stats['album_correct'].sum()} / {len(subdir_stats)} = {subdir_stats['album_correct'].mean():.2%}\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    return sampled_df, subdir_stats\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sampled_df, subdir_stats = analyze_subdirectory_accuracy(val_image_df, k=20, threshold=0.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbec752d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in bad_as_good[\"img_path\"]:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1141566",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Summary - Epoch -1\n",
      "Accuracy: -0.0810 | Loss: 11.2829\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[13195, 14006,   313],\n",
       "       [18192, 39543,   851],\n",
       "       [ 3773, 13970,  1121]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = MeanTeacherDataset(\n",
    "    csv_file = \"test_2.csv\", \n",
    "    root_dir = \"~/Workspace/data-v2/test\",\n",
    "    val = True,\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=config.batch_size, \n",
    "    collate_fn=custom_collate_fn,\n",
    "    num_workers=get_num_workers(),\n",
    "    persistent_workers=False, # True if get_num_workers() > 0 else False,\n",
    "    # pin_memory=False, # WSL does not support pin_memory well\n",
    "    prefetch_factor=3 if get_num_workers() > 0 else None,\n",
    ")\n",
    "test_image_df, test_batch_df, test_total_loss = validate(teacher_model, test_dataloader, config, -1)\n",
    "test_accuracy, test_metrics, cm = analyse(test_image_df, test_batch_df, config, monitor, -1)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22bfbc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be8d014",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b25a86e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9006cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8813fe2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ab' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mab\u001b[49m\n",
      "\u001b[31mNameError\u001b[39m: name 'ab' is not defined"
     ]
    }
   ],
   "source": [
    "code stops here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea6153f",
   "metadata": {},
   "source": [
    "validate more frequently, my epochs are obscenely long\n",
    "can I like ... cut out the resnet for now lmao\n",
    "\n",
    "i should start with 1 epoch on labeled data\n",
    "and mostly freeze \n",
    "\n",
    "and then let the consistency loss start creeping up after ^ epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80fa57a",
   "metadata": {},
   "source": [
    "# STUFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d18b059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b61fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Summary - Epoch -1\n",
      "Accuracy: 0.3433 | Loss: 0.6820\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 3924,  7204,   279],\n",
       "       [ 3703, 18692,   753],\n",
       "       [  388,  2805,  1656]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cad6e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved successfully: ./checkpoints/resnet_clip_yolo_mean_teacher_20251027_112613/checkpoint_epoch2_step54258_acc0.3441_20251027_221157.pth\n",
      "Epoch: 2 | Step: 54258 | Val Acc: 0.3441\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./checkpoints/resnet_clip_yolo_mean_teacher_20251027_112613/checkpoint_epoch2_step54258_acc0.3441_20251027_221157.pth'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_checkpoint(\n",
    "        model, teacher_model, optimizer, scheduler, epoch, global_step, \n",
    "        config, val_accuracy, monitor, loss_history, cls_loss_history\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d649fab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def benchmark(num_workers, num_batches=100):\n",
    "    global dry_run\n",
    "    \n",
    "    # Temporarily set dry_run for benchmarking\n",
    "    original_dry_run = dry_run\n",
    "    dry_run = num_batches * config.batch_size  # Ensure we have enough samples\n",
    "    \n",
    "    # Create benchmark dataset and dataloader\n",
    "    bench_dataset = MeanTeacherDataset(\n",
    "        csv_file=\"train_2.csv\", \n",
    "        root_dir= \"/mnt/d/data-v2/train\",#\"~/data-v2/train\" if get_system_type() == \"wsl\" else \"D:\\\\data-v2\\\\train\",\n",
    "        val=False,\n",
    "    )\n",
    "    \n",
    "    bench_sampler = RandomVersionSampler(bench_dataset)\n",
    "    bench_dataloader = DataLoader(\n",
    "        bench_dataset, \n",
    "        batch_size=config.batch_size, \n",
    "        sampler=bench_sampler,\n",
    "        num_workers=num_workers,\n",
    "        persistent_workers=True if num_workers > 0 else False,\n",
    "    )\n",
    "    \n",
    "    # Reset model and optimizer to ensure fair comparison\n",
    "    temp_model = BiggerClassifier().to(config.device)\n",
    "    temp_teacher = copy.deepcopy(temp_model).to(config.device)\n",
    "    temp_teacher.eval()\n",
    "    for p in temp_teacher.parameters():\n",
    "        p.requires_grad = False\n",
    "    \n",
    "    temp_optimizer = torch.optim.AdamW([\n",
    "        {'params': temp_model.parameters(), 'lr': config.initial_lr}\n",
    "    ])\n",
    "\n",
    "    print(f\"Benchmarking with num_workers={num_workers}...\")\n",
    "    start_time = time.time()\n",
    "    batch_count = 0\n",
    "    \n",
    "    for epoch in range(1):\n",
    "        \n",
    "        global_step, train_image_df, train_batch_df = train_one_epoch(\n",
    "            temp_model, temp_teacher, bench_dataloader, temp_optimizer, criterion, \n",
    "            config, epoch, logger, 0, \n",
    "            track_images=False  # Set to True only if you need detailed per-image analysis\n",
    "        )\n",
    "\n",
    "        \n",
    "        \n",
    "        # Validation\n",
    "        # val_image_df, val_batch_df = validate(teacher_model, val_dataloader, config, epoch)\n",
    "        # val_accuracy, val_metrics, cm = analyse(val_image_df, val_batch_df, config, monitor, epoch)\n",
    "\n",
    "\n",
    "\n",
    "        batch_count += len(bench_dataloader)\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "\n",
    "    # Restore original dry_run\n",
    "    dry_run = original_dry_run\n",
    "    \n",
    "    # Clean up\n",
    "    del temp_model, temp_teacher, temp_optimizer, bench_dataset, bench_dataloader\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return total_time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597e9e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run benchmark\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BENCHMARKING NUM_WORKERS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "results = []\n",
    "for num_workers in range(0, 16):\n",
    "\n",
    "    elapsed_time = benchmark(num_workers, num_batches=100)\n",
    "    batches_per_second = 100 / elapsed_time\n",
    "    images_per_second = (100 * config.batch_size) / elapsed_time\n",
    "    \n",
    "    results.append({\n",
    "        'num_workers': num_workers,\n",
    "        'total_time': elapsed_time,\n",
    "        'batches_per_second': batches_per_second,\n",
    "        'images_per_second': images_per_second\n",
    "    })\n",
    "    \n",
    "    print(f\"\\nnum_workers={num_workers:2d} | \"\n",
    "            f\"Time: {elapsed_time:6.2f}s | \"\n",
    "            f\"Batches/s: {batches_per_second:5.2f} | \"\n",
    "            f\"Images/s: {images_per_second:6.1f}\")\n",
    "\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BENCHMARK SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Find optimal\n",
    "if len(results) > 0:\n",
    "    best_result = max(results, key=lambda x: x['images_per_second'])\n",
    "    print(f\"\\nOptimal num_workers: {best_result['num_workers']} \"\n",
    "          f\"({best_result['images_per_second']:.1f} images/s)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec42660",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a26a7f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting profiling...\n",
      "Profiling complete! Check ./logs/profiler for results\n",
      "View with: tensorboard --logdir=./logs/profiler\n"
     ]
    }
   ],
   "source": [
    "def profile_training_step(model, teacher_model, dataloader, optimizer, criterion, config):\n",
    "    model.train()\n",
    "    # Profile\n",
    "    print(\"Starting profiling...\")\n",
    "    with torch.profiler.profile(\n",
    "        activities=[torch.profiler.ProfilerActivity.CPU, \n",
    "                   torch.profiler.ProfilerActivity.CUDA],\n",
    "        schedule=torch.profiler.schedule(wait=40, warmup=2, active=3, repeat=1),\n",
    "        on_trace_ready=torch.profiler.tensorboard_trace_handler('./logs/profiler'),\n",
    "        record_shapes=True,\n",
    "        profile_memory=True,\n",
    "        with_stack=True\n",
    "    ) as prof:\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            if i >= 50: break\n",
    "            \n",
    "            clip_weak, clip_strong, yolo_weak, yolo_strong, labels, metadata = batch\n",
    "            clip_weak['pixel_values'] = clip_weak['pixel_values'].to(device, non_blocking=True)\n",
    "            clip_strong['pixel_values'] = clip_strong['pixel_values'].to(device, non_blocking=True)\n",
    "            yolo_weak = yolo_weak.to(device, non_blocking=True)\n",
    "            yolo_strong = yolo_strong.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "            \n",
    "            \n",
    "            # with autocast(device_type='cuda'):\n",
    "            student_outputs = model(clip_strong, yolo_strong)\n",
    "            cls_loss = criterion(student_outputs, labels)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                teacher_outputs = teacher_model(clip_weak, yolo_weak)\n",
    "            \n",
    "            consistency_losses = compute_consistency_loss(student_outputs, teacher_outputs)\n",
    "            warmup_factor = min(1.0, i / config.warmup_steps)\n",
    "            base_weight = config.consistency_weight * warmup_factor\n",
    "            consistency_weights = torch.full_like(consistency_losses, base_weight)\n",
    "            weighted_consistency = (consistency_losses * consistency_weights).mean()\n",
    "            loss = cls_loss + weighted_consistency\n",
    "            loss.backward()\n",
    "            # optimizer.zero_grad()\n",
    "            # scaler.scale(loss).backward()\n",
    "            # scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)\n",
    "            # scaler.step(optimizer)\n",
    "            # scaler.update()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            update_ema_variables(model, teacher_model, alpha=config.ema_decay, global_step=i)\n",
    "            \n",
    "            prof.step()\n",
    "    \n",
    "    print(\"Profiling complete! Check ./logs/profiler for results\")\n",
    "    print(\"View with: tensorboard --logdir=./logs/profiler\")\n",
    "\n",
    "# Usage:\n",
    "profile_training_step(model, teacher_model, train_dataloader, optimizer, criterion, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1c9868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "environment: wsl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a36a13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3a345abe",
   "metadata": {},
   "source": [
    "# NOTES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76856a4d",
   "metadata": {},
   "source": [
    "remember to collect the original file name without suffix when making your csv\n",
    "\n",
    "if my teacher model strongly misclassifies, toss the file name out into a log\n",
    "\n",
    "compute_consistency_loss should evolve over epochs - later on, use higher thresholding, and maybe move to sigmoid/binary thresholding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2922307c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a0885e95",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "img-classifier",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
