{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f462d99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from ultralytics import YOLO \n",
    "from transformers import CLIPVisionModel, CLIPImageProcessor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from models import *\n",
    "from custom_logging import *\n",
    "from mean_teacher import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ef276cf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b87a486d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gpu_memory_usage():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.cuda.memory_allocated() / 1024 / 1024  # MB\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a4646a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    device = device\n",
    "    num_classes = 3\n",
    "    initial_lr = 1e-4\n",
    "    lr_backbone = 3e-5\n",
    "    batch_size = 32\n",
    "    num_epochs = 4\n",
    "    freeze_until_epoch = 0\n",
    "    checkpoint_path = \"./checkpoints\"\n",
    "    log_interval = 10\n",
    "    early_stopping_patience = 3\n",
    "    use_amp = True\n",
    "    consistency_weight = 0.5  \n",
    "    ema_decay = 0.99\n",
    "    warmup_steps = 10000 # linear scaling (batches) from 0 to consistency_weight for consistency loss\n",
    "    \n",
    "    # Create directories\n",
    "    os.makedirs(checkpoint_path, exist_ok=True)\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e22ccd2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to: ./logs/clip_yolo_mean_teacher_20250915_005919\n"
     ]
    }
   ],
   "source": [
    "logger = Logger(log_dir=\"./logs\", experiment_name=f\"clip_yolo_mean_teacher_{datetime.now().strftime('%Y%m%d_%H%M%S')}\")\n",
    "logger.log_config(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c802a3f3",
   "metadata": {},
   "source": [
    "# MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e71c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLOv11(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = YOLO(\"yolov11l-face.pt\").model\n",
    "        # self.backbone = torch.nn.Sequential(*list(self.model.model.children())[:7])  # Stops after C3k2 (layer 6)\n",
    "        self.feature_model = torch.nn.Sequential(*list(self.model.model.children())[:10])  # Stops after SPPF (layer 9)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.feature_model(x)\n",
    "\n",
    "# model = YOLOv11().to(device)\n",
    "# features = model(images) \n",
    "# features.shape # torch.Size([B, 512, 20, 20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a86da204",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIP_PROCESSOR = CLIPImageProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "class CLIP(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.clip_model = CLIPVisionModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        # self.clip_processor = CLIPImageProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        # CLIP's final hidden state before projection (not the projection itself)\n",
    "        self.clip_output_dim = self.clip_model.config.hidden_size \n",
    "\n",
    "    def forward(self, x):\n",
    "        # inputs = self.clip_processor(images=x, return_tensors=\"pt\").to(device)\n",
    "        # outputs = self.clip_model(**inputs)\n",
    "        outputs = self.clip_model(**x)\n",
    "        pooled_output = outputs.pooler_output  # shape: [batch_size, 512]\n",
    "        return pooled_output\n",
    "\n",
    "# model = CLIP().to(device)\n",
    "# images = [Image.open(\"image.jpg\"), Image.open(\"image.jpg\")]\n",
    "# outputs = model(Image.open(\"image.jpg\")) \n",
    "# outputs.shape # torch.Size([B, 768])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fa030494",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiggerClassifier(torch.nn.Module):\n",
    "    def __init__(self, output_dim=3):\n",
    "        super().__init__()\n",
    "        self.clip = CLIP() # CLIP outputs: [B, 768]\n",
    "        self.yolo = YOLOv11() # YOLO outputs: [B, 512, 20, 20]\n",
    "        \n",
    "        # Global average pooling for YOLO features\n",
    "        self.yolo_pool = torch.nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        self.yolo_gate = nn.Sequential(\n",
    "            nn.Linear(512, 64),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.fc1 = torch.nn.Linear(768 + 512, 1024)  # Input dim: 768 (CLIP) + 512 (YOLO after pooling) = 1280\n",
    "        self.activation1 = torch.nn.GELU()\n",
    "        self.fc2 = torch.nn.Linear(1024, 1024)\n",
    "        self.activation2 = torch.nn.GELU()\n",
    "        self.fc3 = torch.nn.Linear(1024, output_dim)\n",
    "        \n",
    "    def forward(self, clip_inputs, img_tensor):\n",
    "        clip_features = self.clip(clip_inputs)  # [B, 768]\n",
    "\n",
    "        yolo_features = self.yolo(img_tensor)  # [B, 512, 20, 20]\n",
    "        # Pool YOLO features to [B, 512, 1, 1] then squeeze to [B, 512]\n",
    "        yolo_features = self.yolo_pool(yolo_features).squeeze(-1).squeeze(-1)\n",
    "        # Now learnable self gate \n",
    "        gate_value = self.yolo_gate(yolo_features)\n",
    "        gated_yolo_features = yolo_features * gate_value\n",
    "\n",
    "        combined_features = torch.cat([clip_features, gated_yolo_features], dim=1)  # [B, 1280]\n",
    "        \n",
    "        x = self.fc1(combined_features)\n",
    "        x = self.activation1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.activation2(x)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8302bade",
   "metadata": {},
   "source": [
    "# DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "726e7020",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_tensor = transforms.Compose([\n",
    "    transforms.ToImage(), \n",
    "    transforms.ToDtype(torch.float32, scale=True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a9444a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_intermediate_input_size = 700\n",
    "yolo_final_input_size = 640\n",
    "\n",
    "yolo_weak_transform = transforms.Compose([    \n",
    "    transforms.Resize(size=yolo_intermediate_input_size, interpolation=transforms.InterpolationMode.BILINEAR, antialias=True), # Resize maintaining aspect ratio, then pad to square\n",
    "    # transforms.RandomRotation(degrees=(-5, 5), interpolation=transforms.InterpolationMode.BILINEAR, expand=True, fill=0),\n",
    "    transforms.RandomCrop(yolo_final_input_size), \n",
    "    # transforms.ColorJitter(brightness=0.05, contrast=0.05, saturation=0.05, hue=0.02),\n",
    "    # transforms.Lambda(lambda x: x + torch.randn_like(x) * 0.005), transforms.Lambda(lambda x: torch.clamp(x, 0, 1)),\n",
    "])\n",
    "yolo_strong_transform = transforms.Compose([    \n",
    "    transforms.Resize(size=yolo_intermediate_input_size, interpolation=transforms.InterpolationMode.BILINEAR, antialias=True), # Resize maintaining aspect ratio, then pad to square\n",
    "    # transforms.RandomRotation(degrees=(-15, 15), interpolation=transforms.InterpolationMode.BILINEAR, expand=True, fill=0),\n",
    "    transforms.RandomCrop(yolo_final_input_size), \n",
    "    # transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),\n",
    "    # transforms.Lambda(lambda x: x + torch.randn_like(x) * 0.01), transforms.Lambda(lambda x: torch.clamp(x, 0, 1)),\n",
    "])\n",
    "yolo_val_transform = transforms.Compose([\n",
    "    transforms.ToImage(), transforms.ToDtype(torch.float32, scale=True),\n",
    "    transforms.Resize(size=yolo_intermediate_input_size, interpolation=transforms.InterpolationMode.BILINEAR, antialias=True), # Resize maintaining aspect ratio, then pad to square\n",
    "    transforms.CenterCrop(yolo_final_input_size), \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3f534f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# since I can't (don't want to) figure out how to do CLIP's augmentation on GPU, we simply forego the augmentation\n",
    "clip_val_transform = transforms.Compose([ \n",
    "    transforms.Resize(size=256, interpolation=transforms.InterpolationMode.BILINEAR, antialias=True),\n",
    "    transforms.CenterCrop(224),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3e89d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function to handle PIL images and tensors\n",
    "    For CLIP, expects PIL images, batches them, then puts them in CLIP_PROCESSOR for batch processing. \n",
    "    \"\"\"\n",
    "    transform_start = time.time()\n",
    "    \n",
    "    # For training dataset (5 items per sample)\n",
    "    if len(batch[0]) == 5:\n",
    "        clip_images = [item[0] for item in batch]\n",
    "        clip_inputs = CLIP_PROCESSOR(images=clip_images, return_tensors=\"pt\")\n",
    "\n",
    "        yolo_weak_tensors = torch.stack([item[2] for item in batch])\n",
    "        yolo_strong_tensors = torch.stack([item[3] for item in batch])\n",
    "\n",
    "        labels = torch.stack([item[4] for item in batch])\n",
    "        \n",
    "        ### CHANGE: Calculate and store transform time\n",
    "        transform_time = time.time() - transform_start\n",
    "        \n",
    "        # Add transform_time as an additional element in the return tuple\n",
    "        return clip_inputs, clip_inputs, yolo_weak_tensors, yolo_strong_tensors, labels, transform_time\n",
    "\n",
    "    \n",
    "    # For validation dataset (3 items per sample)\n",
    "    elif len(batch[0]) == 3:\n",
    "        clip_images = [item[0] for item in batch]\n",
    "        clip_inputs = CLIP_PROCESSOR(images=clip_images, return_tensors=\"pt\")\n",
    "\n",
    "        yolo_tensors = torch.stack([item[1] for item in batch])\n",
    "\n",
    "        labels = torch.stack([item[2] for item in batch])\n",
    "        \n",
    "        transform_time = time.time() - transform_start\n",
    "        \n",
    "        return clip_inputs, yolo_tensors, labels, transform_time\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected batch format with {len(batch[0])} items per sample\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "05158d37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7820"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TrainImageDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, \n",
    "                 yolo_weak_transform=None, yolo_strong_transform=None,\n",
    "                 clip_transform=None,\n",
    "                 device=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.annotations = pd.read_csv(os.path.join(self.root_dir, csv_file))\n",
    "        self.yolo_weak_transform = yolo_weak_transform\n",
    "        self.yolo_strong_transform = yolo_strong_transform\n",
    "        self.clip_transform = clip_transform\n",
    "        self.device = device # tensor transforms will occur on device, currently used for YOLO transforms\n",
    "        self.clip_processor = CLIP_PROCESSOR\n",
    "        self.transform_times = []\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            img_path = os.path.join(self.root_dir, self.annotations.iloc[idx, 0])\n",
    "            img_path = img_path.replace('\\\\', '/')\n",
    "            image_pil = Image.open(img_path).convert('RGB')\n",
    "\n",
    "            item_transform_start = time.time()\n",
    "            \n",
    "            clip_image = self.clip_transform(image_pil) if self.clip_transform else image_pil\n",
    "\n",
    "            # with torch.no_grad():\n",
    "            #     image_tensor = to_tensor(image_pil).to(self.device)\n",
    "            #     yolo_weak_image = self.yolo_weak_transform(image_tensor) if self.yolo_weak_transform else image_tensor\n",
    "            #     yolo_strong_image = self.yolo_strong_transform(image_tensor) if self.yolo_strong_transform else image_tensor\n",
    "            image_tensor = to_tensor(image_pil)\n",
    "            yolo_weak_image = self.yolo_weak_transform(image_tensor) if self.yolo_weak_transform else image_tensor\n",
    "            yolo_strong_image = self.yolo_strong_transform(image_tensor) if self.yolo_strong_transform else image_tensor\n",
    "            \n",
    "\n",
    "            item_transform_time = time.time() - item_transform_start\n",
    "            if len(self.transform_times) < 1000:  # Keep last 1000 for memory efficiency\n",
    "                self.transform_times.append(item_transform_time)\n",
    "                \n",
    "            original_label = self.annotations.iloc[idx, 1]\n",
    "            label = torch.tensor(original_label + 1, dtype=torch.long)\n",
    "            \n",
    "            return clip_image, clip_image, yolo_weak_image, yolo_strong_image, label\n",
    "\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_path}: {e}\")\n",
    "            dummy_clip_tensor = torch.zeros(3, 224, 224)\n",
    "            dummy_yolo_tensor = torch.zeros(3, 640, 640).to(self.device)\n",
    "            return dummy_clip_tensor, dummy_clip_tensor, dummy_yolo_tensor, dummy_yolo_tensor, torch.tensor(1, dtype=torch.long)\n",
    "\n",
    "\n",
    "train_dataset = TrainImageDataset(\n",
    "    csv_file=\"train.csv\", \n",
    "    # root_dir=\".\\\\data\\\\train\",\n",
    "    root_dir=\"./data/train\",\n",
    "    yolo_weak_transform=yolo_weak_transform,\n",
    "    yolo_strong_transform=yolo_strong_transform,\n",
    "    clip_transform=clip_val_transform,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset, \n",
    "    batch_size=config.batch_size, \n",
    "    shuffle=True, \n",
    "    collate_fn=custom_collate_fn,\n",
    "    num_workers=12,\n",
    ")\n",
    "\n",
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1205a333",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "978"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ValidationImageDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, \n",
    "                 clip_transform=None, yolo_transform=None,\n",
    "                 device=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.annotations = pd.read_csv(os.path.join(self.root_dir, csv_file))\n",
    "        self.clip_transform = clip_transform\n",
    "        self.yolo_transform = yolo_transform\n",
    "        self.clip_processor = CLIP_PROCESSOR\n",
    "        self.device = device\n",
    "        self.transform_times = []\n",
    "\n",
    "        # Pre-compute file paths for faster access\n",
    "        self.file_paths = [os.path.join(self.root_dir, self.annotations.iloc[i, 0])\n",
    "                          for i in range(len(self.annotations))]\n",
    "        self.labels = [self.annotations.iloc[i, 1] for i in range(len(self.annotations))]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            img_path = self.file_paths[idx]\n",
    "            img_path = img_path.replace('\\\\', '/')\n",
    "            image_pil = Image.open(img_path).convert('RGB')\n",
    "            \n",
    "            item_transform_start = time.time()\n",
    "            \n",
    "            clip_image = self.clip_transform(image_pil) if self.clip_transform else image_pil\n",
    "\n",
    "            # with torch.no_grad():\n",
    "            #     image_tensor = to_tensor(image_pil).to(self.device)\n",
    "            #     yolo_image = self.yolo_transform(image_tensor) if self.yolo_transform else image_tensor\n",
    "            image_tensor = to_tensor(image_pil)\n",
    "            yolo_image = self.yolo_transform(image_tensor) if self.yolo_transform else image_tensor\n",
    "            \n",
    "            item_transform_time = time.time() - item_transform_start\n",
    "            if len(self.transform_times) < 1000:\n",
    "                self.transform_times.append(item_transform_time)\n",
    "                \n",
    "            # Get label (adding 1 to correspond to indices, 0=BAD, 1=UNLABELED, 2=GOOD)\n",
    "            original_label = self.labels[idx]\n",
    "            label = torch.tensor(original_label + 1, dtype=torch.long)\n",
    "            \n",
    "            return clip_image, yolo_image, label\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in val dataset loading image {self.file_paths[idx]}: {e}\")\n",
    "            # Return dummy data in case of error\n",
    "            dummy_clip_tensor = torch.zeros(3, 224, 224)\n",
    "            dummy_yolo_tensor = torch.zeros(3, 640, 640)\n",
    "            return dummy_clip_tensor, dummy_yolo_tensor, torch.tensor(1, dtype=torch.long)\n",
    "        \n",
    "val_dataset = ValidationImageDataset(\n",
    "    csv_file=\"val.csv\", \n",
    "    root_dir=\"./data/val\", \n",
    "    # root_dir=\".\\\\data\\\\val\", \n",
    "    clip_transform=clip_val_transform,\n",
    "    yolo_transform=yolo_val_transform\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset, \n",
    "    batch_size=32, \n",
    "    shuffle=True, \n",
    "    collate_fn=custom_collate_fn,\n",
    "    num_workers=12,\n",
    ")\n",
    "\n",
    "len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c7c0360b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 224, 224]) torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 640, 640]) torch.Size([32, 3, 640, 640])\n",
      "torch.Size([32])\n",
      "single transform time in collate: 0.4663s\n"
     ]
    }
   ],
   "source": [
    "for batch_data in train_loader:\n",
    "    if len(batch_data) == 6:  # Training has 6 elements now\n",
    "        a, b, c, d, e, transform_time = batch_data\n",
    "        print(a['pixel_values'].shape, b['pixel_values'].shape)\n",
    "        print(c.shape, d.shape)\n",
    "        print(e.shape)\n",
    "        print(f\"single transform time in collate: {transform_time:.4f}s\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df52038",
   "metadata": {},
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "96a9eb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BiggerClassifier().to(config.device)\n",
    "teacher_model = copy.deepcopy(model).to(config.device)\n",
    "teacher_model.eval()\n",
    "for p in teacher_model.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6042a033",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AsymmetricFocalLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Asymmetric Focal Loss variant that combines focal loss with asymmetric penalties.\n",
    "    Useful when you also want to handle class imbalance.\n",
    "    currently unused, since suspected to be unstable. \n",
    "    \"\"\"\n",
    "    def __init__(self, gamma=2.0, alpha=None, confusion_penalty_matrix=None):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        if confusion_penalty_matrix is None:\n",
    "            confusion_penalty_matrix = torch.tensor([\n",
    "                [1.0, 1.0, 1.0],   # True: BAD \n",
    "                [1.0, 1.0, 1.0],   # True: NEUTRAL\n",
    "                [1.0, 1.0, 1.0]    # True: GOOD\n",
    "            ])\n",
    "        self.confusion_penalty_matrix = confusion_penalty_matrix\n",
    "        \n",
    "    def forward(self, logits, targets):\n",
    "        ce_loss = F.cross_entropy(logits, targets, reduction='none')\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        p_t = probs.gather(1, targets.view(-1, 1)).squeeze(1)\n",
    "        \n",
    "        # print(f\"p_t min: {p_t.min():.6f}, max: {p_t.max():.6f}\")\n",
    "        # print(f\"ce_loss min: {ce_loss.min():.6f}, max: {ce_loss.max():.6f}\")\n",
    "        \n",
    "        # Focal term\n",
    "        focal_weight = (1 - p_t) ** self.gamma\n",
    "        \n",
    "        # Get predicted classes for confusion penalties\n",
    "        pred_classes = torch.argmax(logits, dim=1)\n",
    "        \n",
    "        # Apply confusion-based penalties\n",
    "        batch_size = targets.size(0)\n",
    "        penalties = torch.zeros(batch_size, device=targets.device)\n",
    "        \n",
    "        # for i in range(batch_size):\n",
    "        #     true_class = targets[i].item()\n",
    "        #     pred_class = pred_classes[i].item()\n",
    "        #     penalties[i] = self.confusion_penalty_matrix[true_class, pred_class]\n",
    "        penalties = self.confusion_penalty_matrix[targets, pred_classes]\n",
    "        \n",
    "        # Combine focal weight with confusion penalties\n",
    "        loss = focal_weight * ce_loss * penalties\n",
    "        \n",
    "        if self.alpha is not None:\n",
    "            alpha_t = self.alpha.gather(0, targets)\n",
    "            loss = alpha_t * loss\n",
    "            \n",
    "        return loss.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ded490e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_params = []\n",
    "yolo_params = []\n",
    "classifier_params = []\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if 'clip' in name:\n",
    "        clip_params.append(param)\n",
    "    elif 'yolo' in name:\n",
    "        yolo_params.append(param)\n",
    "    else:\n",
    "        classifier_params.append(param)\n",
    "\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {'params': clip_params, 'lr': config.lr_backbone*0.1, 'weight_decay': 1e-4}, # CLIP is no longer getting data augmentations, so lowering LR\n",
    "    {'params': yolo_params, 'lr': config.lr_backbone, 'weight_decay': 1e-4},\n",
    "    {'params': classifier_params, 'lr': config.initial_lr, 'weight_decay': 1e-4},\n",
    "])\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=1, T_mult=1)\n",
    "criterion = nn.CrossEntropyLoss().to(config.device)\n",
    "scaler = GradScaler() if config.use_amp else None\n",
    "monitor = PerformanceMonitor()\n",
    "\n",
    "criterion = AsymmetricFocalLoss(\n",
    "    gamma=1.5,  # Reduced from 2.0 for less aggressive focusing\n",
    "    alpha=torch.tensor([1.2, 0.8, 1.2]).to(config.device),  \n",
    "    confusion_penalty_matrix=torch.tensor([\n",
    "        [1.0, 1.05, 1.15], \n",
    "        [0.95, 1.0, 0.95],\n",
    "        [1.1, 1.05, 1.0]  \n",
    "    ]).to(config.device)\n",
    ").to(config.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "588590f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(checkpoint_path, config, device='cuda'):\n",
    "    # check this is consistent with everything else later\n",
    "    print(f\"Loading checkpoint from: {checkpoint_path}\")\n",
    "    \n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    \n",
    "    if 'config' in checkpoint:\n",
    "        checkpoint_config = checkpoint['config']\n",
    "        print(\"Checkpoint configuration:\")\n",
    "        for key, value in checkpoint_config.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "    \n",
    "    model = BiggerClassifier().to(device)\n",
    "    teacher_model = BiggerClassifier().to(device)\n",
    "    \n",
    "    if 'model_state_dict' in checkpoint:\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(\"Loaded student model weights\")\n",
    "    \n",
    "    if 'teacher_state_dict' in checkpoint:\n",
    "        teacher_model.load_state_dict(checkpoint['teacher_state_dict'])\n",
    "        print(\"Loaded teacher model weights\")\n",
    "    \n",
    "    for p in teacher_model.parameters():\n",
    "        p.requires_grad = False\n",
    "    \n",
    "    clip_params = []\n",
    "    yolo_params = []\n",
    "    classifier_params = []\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        if 'clip' in name:\n",
    "            clip_params.append(param)\n",
    "        elif 'yolo' in name:\n",
    "            yolo_params.append(param)\n",
    "        else:\n",
    "            classifier_params.append(param)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW([\n",
    "        {'params': clip_params, 'lr': config.lr_backbone, 'weight_decay': 1e-4},\n",
    "        {'params': yolo_params, 'lr': config.lr_backbone, 'weight_decay': 1e-4},\n",
    "        {'params': classifier_params, 'lr': config.initial_lr, 'weight_decay': 1e-4},\n",
    "    ])\n",
    "    \n",
    "    if 'optimizer_state_dict' in checkpoint:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        print(\"Loaded optimizer state\")\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=2, T_mult=1)\n",
    "    \n",
    "    if 'scheduler_state_dict' in checkpoint:\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        print(\"Loaded scheduler state\")\n",
    "    \n",
    "    start_epoch = checkpoint.get('epoch', -1) + 1\n",
    "    best_accuracy = checkpoint.get('val_accuracy', 0)\n",
    "    \n",
    "    print(f\"\\nCheckpoint info:\")\n",
    "    print(f\"  Saved at epoch: {checkpoint.get('epoch', 'unknown')}\")\n",
    "    print(f\"  Best validation accuracy: {best_accuracy:.4f}\")\n",
    "    print(f\"  Will resume from epoch: {start_epoch}\")\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'teacher_model': teacher_model,\n",
    "        'optimizer': optimizer,\n",
    "        'scheduler': scheduler,\n",
    "        'start_epoch': start_epoch,\n",
    "        'best_accuracy': best_accuracy,\n",
    "        'checkpoint': checkpoint\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "18c13861",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, val_loader, config, monitor, epoch):\n",
    "    model.eval()\n",
    "    total, correct = 0, 0\n",
    "    class_correct = [0] * config.num_classes\n",
    "    class_total = [0] * config.num_classes\n",
    "    all_preds, all_labels = [], []\n",
    "    val_loss = 0\n",
    "    probs_bad = []\n",
    "    probs_neutral = []\n",
    "    probs_good = []\n",
    "\n",
    "    total_batches = len(val_loader)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        val_pbar = tqdm(val_loader, desc=f'Validation Epoch {epoch}', total=total_batches, leave=False)\n",
    "        for batch_data in val_pbar:\n",
    "            clip_inputs, yolo_tensors, labels, transform_time = batch_data\n",
    "            clip_inputs['pixel_values'] = clip_inputs['pixel_values'].to(config.device)\n",
    "            yolo_tensors = yolo_tensors.to(config.device)\n",
    "            labels = labels.to(config.device)\n",
    "            \n",
    "            outputs = model(clip_inputs, yolo_tensors)\n",
    "            loss = F.cross_entropy(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            softmax_probs = F.softmax(outputs, dim=1)\n",
    "            probs_bad.extend(softmax_probs[:, 0].cpu().numpy())\n",
    "            probs_neutral.extend(softmax_probs[:, 1].cpu().numpy())\n",
    "            probs_good.extend(softmax_probs[:, 2].cpu().numpy())\n",
    "            \n",
    "            preds = outputs.argmax(dim=1)\n",
    "            preds_cpu = preds.cpu().numpy()\n",
    "            labels_cpu = labels.cpu().numpy()\n",
    "            all_preds.extend(preds_cpu)\n",
    "            all_labels.extend(labels_cpu)\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            \n",
    "            for i, label in enumerate(labels_cpu):\n",
    "                class_total[label] += 1\n",
    "                class_correct[label] += (preds_cpu[i] == label).item()\n",
    "            \n",
    "            val_pbar.set_postfix({'loss': f'{loss.item():.4f}', \n",
    "                                  'acc': f'{correct/total:.4f}'})\n",
    "    \n",
    "    ### HISTOGRAMS FOR EACH CLASS \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # Plot histogram for BAD class probabilities\n",
    "    axes[0].hist(probs_bad, bins=50, alpha=0.7, color='red', edgecolor='black')\n",
    "    axes[0].set_title('BAD Class Probabilities')\n",
    "    axes[0].set_xlabel('Softmax Probability')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "    axes[0].set_xlim([0, 1])\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot histogram for NEUTRAL class probabilities\n",
    "    axes[1].hist(probs_neutral, bins=50, alpha=0.7, color='gray', edgecolor='black')\n",
    "    axes[1].set_title('NEUTRAL Class Probabilities')\n",
    "    axes[1].set_xlabel('Softmax Probability')\n",
    "    axes[1].set_ylabel('Frequency')\n",
    "    axes[1].set_xlim([0, 1])\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot histogram for GOOD class probabilities\n",
    "    axes[2].hist(probs_good, bins=50, alpha=0.7, color='green', edgecolor='black')\n",
    "    axes[2].set_title('GOOD Class Probabilities')\n",
    "    axes[2].set_xlabel('Softmax Probability')\n",
    "    axes[2].set_ylabel('Frequency')\n",
    "    axes[2].set_xlim([0, 1])\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle(f'Softmax Probability Distributions - Epoch {epoch}')\n",
    "    plt.tight_layout()\n",
    "    histogram_path = os.path.join(logger.get_log_dir(), f'softmax_histograms_epoch_{epoch}.png')\n",
    "    plt.savefig(histogram_path, dpi=100)\n",
    "    plt.close()\n",
    "    ### END OF HISTOGRAMS FOR EACH CLASS \n",
    "\n",
    "    accuracy = correct / total \n",
    "    avg_val_loss = val_loss / total_batches\n",
    "    class_names = ['bad', 'neutral', 'good']\n",
    "    \n",
    "    class_recall = [class_correct[i] / class_total[i] if class_total[i] else 0 for i in range(config.num_classes)]\n",
    "    \n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    report = classification_report(all_labels, all_preds, target_names=class_names, output_dict=True)\n",
    "    \n",
    "    metrics = {\n",
    "        \"val/accuracy\": accuracy,\n",
    "        \"val/loss\": avg_val_loss,\n",
    "        \"val/recall_bad\": class_recall[0],\n",
    "        \"val/recall_neutral\": class_recall[1],\n",
    "        \"val/recall_good\": class_recall[2],\n",
    "    }\n",
    "    \n",
    "    for class_name in class_names:\n",
    "        if class_name in report:\n",
    "            metrics[f\"val/precision_{class_name}\"] = report[class_name]['precision']\n",
    "            metrics[f\"val/f1_{class_name}\"] = report[class_name]['f1-score']\n",
    "    \n",
    "    monitor.accuracy_history.append(accuracy)\n",
    "    \n",
    "    if accuracy > monitor.best_accuracy:\n",
    "        monitor.best_accuracy = accuracy\n",
    "        monitor.epochs_without_improvement = 0\n",
    "    else:\n",
    "        monitor.epochs_without_improvement += 1\n",
    "    \n",
    "    model.train()\n",
    "    return accuracy, metrics, cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6aa7892d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_running_loss(loss_history, save_path, window_size=50):\n",
    "    \"\"\"\n",
    "    Plot running loss with moving average and save to file\n",
    "    \n",
    "    Args:\n",
    "        loss_history: List of loss values\n",
    "        save_path: Path to save the plot\n",
    "        window_size: Window size for moving average\n",
    "    \"\"\"\n",
    "    if len(loss_history) < window_size:\n",
    "        return\n",
    "    \n",
    "    # Calculate moving average\n",
    "    moving_avg = []\n",
    "    for i in range(window_size - 1, len(loss_history)):\n",
    "        window = loss_history[i - window_size + 1:i + 1]\n",
    "        moving_avg.append(sum(window) / window_size)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Plot raw loss in light color\n",
    "    plt.plot(loss_history, alpha=0.3, color='blue', label='Raw Loss')\n",
    "    \n",
    "    # Plot moving average in bold\n",
    "    x_moving = list(range(window_size - 1, len(loss_history)))\n",
    "    plt.plot(x_moving, moving_avg, color='red', linewidth=2, label=f'Moving Avg (window={window_size})')\n",
    "    \n",
    "    plt.xlabel('Batch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss Over Time')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add statistics\n",
    "    if moving_avg:\n",
    "        current_avg = moving_avg[-1]\n",
    "        min_avg = min(moving_avg)\n",
    "        plt.axhline(y=current_avg, color='green', linestyle='--', alpha=0.5, label=f'Current: {current_avg:.4f}')\n",
    "        plt.axhline(y=min_avg, color='orange', linestyle='--', alpha=0.5, label=f'Min: {min_avg:.4f}')\n",
    "    \n",
    "    if 'train_loader' in globals() and len(train_loader) > 0:\n",
    "        batches_per_epoch = len(train_loader)\n",
    "        num_epochs = len(loss_history) // batches_per_epoch\n",
    "        for epoch in range(1, num_epochs + 1):\n",
    "            epoch_batch = epoch * batches_per_epoch\n",
    "            if epoch_batch < len(loss_history):\n",
    "                plt.axvline(x=epoch_batch, color='gray', linestyle=':', alpha=0.5)\n",
    "                plt.text(epoch_batch, plt.ylim()[1] * 0.95, f'Epoch {epoch}', \n",
    "                        rotation=90, verticalalignment='top', fontsize=8, alpha=0.7)\n",
    "                \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=100)\n",
    "    plt.close()\n",
    "    \n",
    "    # print(f\"  Loss graph saved to: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a226f98f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training...\n",
      "Tensorboard logs: tensorboard --logdir ./logs/clip_yolo_mean_teacher_20250915_005919\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nStarting training...\")\n",
    "print(f\"Tensorboard logs: tensorboard --logdir {logger.get_log_dir()}\")\n",
    "global_step = 0\n",
    "best_accuracy = 0 \n",
    "data_loading_times = []\n",
    "gpu_compute_times = []\n",
    "transform_times = []\n",
    "start_epoch = 0\n",
    "\n",
    "loss_history = []\n",
    "\n",
    "batches_per_half_epoch = len(train_loader) // 2\n",
    "\n",
    "\n",
    "# Uncomment to load from checkpoint\n",
    "# loaded = load_checkpoint(\"checkpoints/best_model_acc0.7807.pth\", config)\n",
    "# model = loaded['model']\n",
    "# teacher_model = loaded['teacher_model']\n",
    "# optimizer = loaded['optimizer']\n",
    "# scheduler = loaded['scheduler']\n",
    "# start_epoch = loaded['start_epoch']\n",
    "# best_accuracy = loaded['best_accuracy']\n",
    "# monitor.best_accuracy = best_accuracy\n",
    "# global_step = start_epoch * len(train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6d975127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unfreezing backbone at epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   4%|â–Ž         | 274/7820 [02:34<1:10:47,  1.78it/s, loss=0.3101, img/s=71.7, data_ms=66.0, trans_ms=225.1, gpu_ms=380.1]    \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     13\u001b[39m epoch_pbar = tqdm(train_loader, desc=\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m, total=\u001b[38;5;28mlen\u001b[39m(train_loader))\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, batch_data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(epoch_pbar):\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     batch_start = \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtime\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m     clip_weak, clip_strong, yolo_weak, yolo_strong, labels, collate_transform_time = batch_data\n\u001b[32m     19\u001b[39m     clip_weak[\u001b[33m'\u001b[39m\u001b[33mpixel_values\u001b[39m\u001b[33m'\u001b[39m] = clip_weak[\u001b[33m'\u001b[39m\u001b[33mpixel_values\u001b[39m\u001b[33m'\u001b[39m].to(config.device)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(start_epoch, config.num_epochs):\n",
    "    if epoch == config.freeze_until_epoch:\n",
    "        print(f\"Unfreezing backbone at epoch {epoch}\")\n",
    "        for param in clip_params + yolo_params:\n",
    "            param.requires_grad = True\n",
    "    \n",
    "    model.train()\n",
    "    epoch_start = time.time()\n",
    "    epoch_loss = 0\n",
    "    epoch_cls_loss = 0\n",
    "    epoch_consistency_loss = 0\n",
    "\n",
    "    epoch_pbar = tqdm(train_loader, desc=f'Epoch {epoch}', total=len(train_loader))\n",
    "    \n",
    "    for batch_idx, batch_data in enumerate(epoch_pbar):\n",
    "        batch_start = time.time()\n",
    "        \n",
    "        clip_weak, clip_strong, yolo_weak, yolo_strong, labels, collate_transform_time = batch_data\n",
    "        clip_weak['pixel_values'] = clip_weak['pixel_values'].to(config.device)\n",
    "        clip_strong['pixel_values'] = clip_strong['pixel_values'].to(config.device) \n",
    "        yolo_weak = yolo_weak.to(config.device)\n",
    "        yolo_strong = yolo_strong.to(config.device)\n",
    "        labels = labels.to(config.device)\n",
    "        \n",
    "        data_load_time = time.time() - batch_start\n",
    "\n",
    "        dataset_transform_time = 0\n",
    "        if hasattr(train_dataset, 'transform_times') and train_dataset.transform_times:\n",
    "            # Get recent average from dataset\n",
    "            recent_transforms = train_dataset.transform_times[-100:]\n",
    "            dataset_transform_time = np.mean(recent_transforms) * config.batch_size  # Scale by batch size\n",
    "        \n",
    "        total_transform_time = dataset_transform_time + collate_transform_time\n",
    "\n",
    "        compute_start = time.time()\n",
    "        \n",
    "        if config.use_amp:\n",
    "            with autocast(device_type='cuda'):\n",
    "\n",
    "                student_outputs = model(clip_strong, yolo_strong)\n",
    "                cls_loss = criterion(student_outputs, labels)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    teacher_outputs = teacher_model(clip_weak, yolo_weak)\n",
    "                \n",
    "                # Compute per-sample consistency losses\n",
    "                consistency_losses = compute_consistency_loss(student_outputs, teacher_outputs)\n",
    "                \n",
    "                # Create per-image consistency weights (can be customized per image if needed)\n",
    "                warmup_factor = min(1.0, global_step / config.warmup_steps)\n",
    "                base_weight = config.consistency_weight * warmup_factor\n",
    "                consistency_weights = torch.zeros_like(consistency_losses)\n",
    "                unlabeled_mask = (labels == 1)\n",
    "                labeled_mask = ~unlabeled_mask\n",
    "                consistency_weights[unlabeled_mask] = base_weight * 2.0\n",
    "                consistency_weights[labeled_mask] = base_weight / 2.0\n",
    "                # consistency_weights = torch.full_like(consistency_losses, config.consistency_weight * warmup_factor)\n",
    "                \n",
    "                # Apply per-image weights and compute mean\n",
    "                weighted_consistency = (consistency_losses * consistency_weights).mean()\n",
    "                loss = cls_loss + weighted_consistency\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            student_outputs = model(clip_strong, yolo_strong)\n",
    "            cls_loss = criterion(student_outputs, labels)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                teacher_outputs = teacher_model(clip_weak, yolo_weak)\n",
    "            \n",
    "            # Compute per-sample consistency losses\n",
    "            consistency_losses = compute_consistency_loss(student_outputs, teacher_outputs)\n",
    "            \n",
    "            # Create per-image consistency weights (can be customized per image if needed)\n",
    "            warmup_factor = min(1.0, global_step / config.warmup_steps)\n",
    "            base_weight = config.consistency_weight * warmup_factor\n",
    "            consistency_weights = torch.zeros_like(consistency_losses)\n",
    "            unlabeled_mask = (labels == 1)\n",
    "            labeled_mask = ~unlabeled_mask\n",
    "            consistency_weights[unlabeled_mask] = base_weight * 2.0\n",
    "            consistency_weights[labeled_mask] = base_weight / 2.0\n",
    "            # consistency_weights = torch.full_like(consistency_losses, config.consistency_weight * warmup_factor)\n",
    "            \n",
    "            # Apply per-image weights and compute mean\n",
    "            weighted_consistency = (consistency_losses * consistency_weights).mean()\n",
    "            loss = cls_loss + weighted_consistency\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "        \n",
    "        update_ema_variables(model, teacher_model, alpha=config.ema_decay, global_step=global_step)\n",
    "        \n",
    "        gpu_compute_time = time.time() - compute_start\n",
    "        total_time = time.time() - batch_start\n",
    "        \n",
    "        if len(data_loading_times) < 1000:\n",
    "            data_loading_times.append(data_load_time)\n",
    "            gpu_compute_times.append(gpu_compute_time)\n",
    "            transform_times.append(total_transform_time)\n",
    "        \n",
    "        loss_history.append(loss.item())\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_cls_loss += cls_loss.item()\n",
    "        epoch_consistency_loss += weighted_consistency.item()\n",
    "        \n",
    "        images_per_second = config.batch_size / total_time\n",
    "        \n",
    "        epoch_pbar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'img/s': f'{images_per_second:.1f}',\n",
    "            'data_ms': f'{data_load_time*1000:.1f}',\n",
    "            'trans_ms': f'{total_transform_time*1000:.1f}',\n",
    "            'gpu_ms': f'{gpu_compute_time*1000:.1f}'\n",
    "        })\n",
    "\n",
    "        if global_step > 0 and global_step % 1000 == 0:\n",
    "            loss_graph_path = os.path.join(logger.get_log_dir(), f'loss_graph.png')\n",
    "            plot_running_loss(loss_history, loss_graph_path)\n",
    "        \n",
    "        if global_step % config.log_interval == 0:\n",
    "            avg_data_time = np.mean(data_loading_times[-100:]) if data_loading_times else 0\n",
    "            avg_gpu_time = np.mean(gpu_compute_times[-100:]) if gpu_compute_times else 0\n",
    "            avg_transform_time = np.mean(transform_times[-100:]) if transform_times else 0\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            \n",
    "            train_metrics = {\n",
    "                \"train/loss\": loss.item(),\n",
    "                \"train/cls_loss\": cls_loss.item(),\n",
    "                \"train/consistency\": weighted_consistency.item(),\n",
    "                \"train/consistency_weight\": consistency_weights[0].item(),  # Log first sample's weight as example\n",
    "                \"train/learning_rate\": current_lr,\n",
    "                \"train/images_per_second\": images_per_second,\n",
    "                \"timing/data_load_ms\": avg_data_time * 1000,\n",
    "                \"timing/transform_ms\": avg_transform_time * 1000,\n",
    "                \"timing/gpu_compute_ms\": avg_gpu_time * 1000,\n",
    "                \"timing/data_load_percentage\": (avg_data_time / (avg_data_time + avg_gpu_time + avg_transform_time)) * 100 if (avg_data_time + avg_gpu_time + avg_transform_time) > 0 else 0,\n",
    "                \"timing/transform_percentage\": (avg_transform_time / (avg_data_time + avg_gpu_time + avg_transform_time)) * 100 if (avg_data_time + avg_gpu_time + avg_transform_time) > 0 else 0, \n",
    "                \"timing/gpu_percentage\": (avg_gpu_time / (avg_data_time + avg_gpu_time + avg_transform_time)) * 100 if (avg_data_time + avg_gpu_time + avg_transform_time) > 0 else 0,\n",
    "                                \"system/gpu_memory_mb\": get_gpu_memory_usage()\n",
    "            }\n",
    "            \n",
    "            logger.log_metrics(train_metrics, global_step)\n",
    "            \n",
    "            logger.log_train_step(global_step, epoch, {\n",
    "                'loss': loss.item(),\n",
    "                'cls_loss': cls_loss.item(),\n",
    "                'consistency_loss': weighted_consistency.item(),\n",
    "                'learning_rate': current_lr,\n",
    "                'consistency_weight': consistency_weights[0].item()\n",
    "            })\n",
    "            \n",
    "            logger.log_system_metrics(global_step, {\n",
    "                'images_per_second': images_per_second,\n",
    "                'data_load_ms': avg_data_time * 1000,\n",
    "                'transform_ms': avg_transform_time * 1000,\n",
    "                'gpu_compute_ms': avg_gpu_time * 1000,\n",
    "                'queue_size': 0,\n",
    "                'gpu_memory_mb': get_gpu_memory_usage()\n",
    "            })\n",
    "\n",
    "        global_step += 1\n",
    "            \n",
    "    epoch_time = time.time() - epoch_start\n",
    "    avg_epoch_loss = epoch_loss / len(train_loader) if len(train_loader) > 0 else 0\n",
    "    avg_epoch_cls_loss = epoch_cls_loss / len(train_loader) if len(train_loader) > 0 else 0\n",
    "    avg_epoch_consistency_loss = epoch_consistency_loss / len(train_loader) if len(train_loader) > 0 else 0\n",
    "    avg_epoch_transform_time = np.mean(transform_times) if transform_times else 0\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch} Summary:\")\n",
    "    print(f\"  Time: {epoch_time/60:.1f} minutes\")\n",
    "    print(f\"  Avg Loss: {avg_epoch_loss:.4f}\")\n",
    "    print(f\"  Avg Classification Loss: {avg_epoch_cls_loss:.4f}\")\n",
    "    print(f\"  Avg Consistency Loss: {avg_epoch_consistency_loss:.4f}\")\n",
    "    print(f\"  Avg Transform Time: {avg_epoch_transform_time*1000:.1f}ms\")\n",
    "    print(f\"  Throughput: {len(train_dataset) / epoch_time:.1f} images/second\")\n",
    "    \n",
    "    print(\"Running end of epoch validation...\")\n",
    "    val_acc, val_metrics, cm = validate(teacher_model, val_loader, config, monitor, epoch)\n",
    "    \n",
    "    logger.log_metrics(val_metrics, epoch)\n",
    "    logger.log_validation(epoch, val_metrics)\n",
    "    logger.log_confusion_matrix(cm, ['bad', 'neutral', 'good'], epoch)\n",
    "    \n",
    "    print(f\"  Validation Accuracy: {val_acc:.4f}\")\n",
    "    print(f\"  Best Accuracy: {monitor.best_accuracy:.4f}\")\n",
    "    \n",
    "    if val_acc > best_accuracy:\n",
    "        best_accuracy = val_acc\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'teacher_state_dict': teacher_model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'val_accuracy': val_acc,\n",
    "            'config': vars(config)\n",
    "        }\n",
    "        checkpoint_path = os.path.join(config.checkpoint_path, f\"best_model_acc{val_acc:.4f}.pth\")\n",
    "        torch.save(checkpoint, checkpoint_path)\n",
    "        print(f\"Saved best model: {checkpoint_path}\")\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    if monitor.epochs_without_improvement >= config.early_stopping_patience:\n",
    "        print(f\"Early stopping at epoch {epoch}\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c631b3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_loss_graph_path = os.path.join(logger.get_log_dir(), 'final_loss_graph.png')\n",
    "plot_running_loss(loss_history, final_loss_graph_path, window_size=100)\n",
    "logger.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a85c1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.9 iteration is about as good as it gets (basically gpu bottlenecked)\n",
    "# 1.6 is without random rotation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cde3f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b563fc0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbb9d17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e639347",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "38ae1185",
   "metadata": {},
   "source": [
    "# INFERENCE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496ce72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = ValidationImageDataset(\n",
    "    csv_file=\"test.csv\", \n",
    "    root_dir=\".\\\\data\\\\test\", \n",
    "    clip_transform=clip_val_transform,\n",
    "    yolo_transform=yolo_val_transform\n",
    ")\n",
    "test_loader = DataLoader(test_dataset, config.batch_size, False, collate_fn=custom_collate_fn)\n",
    "print(f\"Test batches: {len(test_loader)}\")\n",
    "\n",
    "test_acc, test_metrics, cm = validate(teacher_model, test_loader, config, monitor, -1)\n",
    "print(f\"Test accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# Inference functions\n",
    "def load_for_inference(checkpoint_path, device='cuda'):\n",
    "    \"\"\"Load model for inference\"\"\"\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model = BiggerClassifier().to(device)\n",
    "    \n",
    "    # Try loading teacher model first (usually better), fallback to student\n",
    "    if 'teacher_state_dict' in checkpoint:\n",
    "        model.load_state_dict(checkpoint['teacher_state_dict'])\n",
    "        print(\"Loaded teacher model for inference\")\n",
    "    elif 'model_state_dict' in checkpoint:\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(\"Loaded student model for inference\")\n",
    "    else:\n",
    "        raise KeyError(\"No model found in checkpoint\")\n",
    "    \n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def infer(pil_imgs, model=None, checkpoint_path=None, device='cuda'):\n",
    "    \"\"\"\n",
    "    Inference function that takes a list of PIL images and returns softmaxed logits\n",
    "    \n",
    "    Args:\n",
    "        pil_imgs: List of PIL images or single PIL image\n",
    "        model: Loaded model (if None, will load from checkpoint_path)\n",
    "        checkpoint_path: Path to checkpoint (if model is None)\n",
    "        device: Device to run inference on\n",
    "    \n",
    "    Returns:\n",
    "        Softmaxed logits of shape [batch_size, num_classes]\n",
    "    \"\"\"\n",
    "    if model is None:\n",
    "        if checkpoint_path is None:\n",
    "            raise ValueError(\"Either model or checkpoint_path must be provided\")\n",
    "        model = load_for_inference(checkpoint_path, device)\n",
    "    \n",
    "    # Handle single image\n",
    "    if not isinstance(pil_imgs, list):\n",
    "        pil_imgs = [pil_imgs]\n",
    "    \n",
    "    # Prepare images\n",
    "    clip_images = []\n",
    "    yolo_tensors = []\n",
    "    \n",
    "    for img in pil_imgs:\n",
    "        # Ensure RGB\n",
    "        if img.mode != 'RGB':\n",
    "            img = img.convert('RGB')\n",
    "        \n",
    "        # Apply validation transforms\n",
    "        clip_img = clip_val_transform(img)\n",
    "        yolo_tensor = yolo_val_transform(img)\n",
    "        \n",
    "        clip_images.append(clip_img)\n",
    "        yolo_tensors.append(yolo_tensor)\n",
    "    \n",
    "    # Stack YOLO tensors\n",
    "    yolo_batch = torch.stack(yolo_tensors).to(device)\n",
    "    \n",
    "    # Run inference\n",
    "    with torch.no_grad():\n",
    "        logits = model(clip_images, yolo_batch)\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "    \n",
    "    return probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4141c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example usage:\n",
    "# model = load_for_inference(\"./checkpoints/best_model_acc0.8328.pth\")\n",
    "# probs = infer([Image.open(\"test.jpg\")], model=model)\n",
    "# print(f\"Predictions: {probs}\")\n",
    "# print(f\"Predicted class: {probs.argmax(dim=1)}\")  # 0=bad, 1=neutral, 2=good\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379957e5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "56d59d3d",
   "metadata": {},
   "source": [
    "# COMMENTS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff703eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9456a25c",
   "metadata": {},
   "source": [
    "SHIT I DID\n",
    "1. the consistency loss has been corrected - now is on a per image basis\n",
    "2. now using asymmetric focal loss\n",
    "3. teacher model now in eval() always \n",
    "4. clip processor should be called not in forward pass, but in dataset\n",
    "5. back to CE loss, for sanity checking\n",
    "6. CLIP no longer gets data augmentation, to save on the disgusting CPU transform time\n",
    "7. training loss graph is now shown, from beginning of training to current time, every 1000 batches \n",
    "8. also tracking the distribution of the preds in validate() as 3 histograms\n",
    "9. the cosine scheduler now makes sense, using T_0=2, T_mult=1 (cycles of a constant 2 epochs)\n",
    "10. consistency loss is halved on labeled data, and doubled on unlabeled data. \n",
    "11. Asymm focal loss has been softened, but doesn't seem to do better than CE? I need to validate more frequently... \n",
    "12. I think I'll keep running with the asymm focal loss for now? Currently: penalise predicting BAD/GOOD on UNLABELED data less. GOOD b/c we don't want to miss any goods. BAD b/c... . **the intended result of this is more true neutrals predicted bad/good in the confusion matrix**\n",
    "13. transform pipeline is correct now. sanity checked. clip and yolo now see the same rotation\n",
    "14. slapped a resnet on. can change between resnet50, 152, etc.\n",
    "\n",
    "\n",
    "FUTURE STEPS\n",
    "1. the consistency loss can be weighted by teacher confidence (already implemented in mean_teacher.py)\n",
    "2. the start epochs can be fully supervised data..  disable teacher consistency? disable unlabeled data? \n",
    "    [meh i'm basically already doing that, long warmup period and stuff]\n",
    "4. try recovering from chkpt, and also try a CE loss -> asymm focal for epoch 0-3, 4-5 respectively for example\n",
    "7. would like finer grain control on learnrates, and also how they thaw... which should I freeze more btw? the front or back of the pretraineds? I should completely freeze the first X layers, then do some sort of linearly increasing LR for the back ones i guess\n",
    "8. I need to move away from cosine anneal. **I need to get my LR shit right**. ***top priority***\n",
    "\n",
    "11. MUST: validate more frequently on the new incoming dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9382b8",
   "metadata": {},
   "source": [
    "# SANDBOX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab14e64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function torchvision.transforms.functional.to_tensor(pic: Union[PIL.Image.Image, numpy.ndarray]) -> torch.Tensor>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f376ab37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5329578",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac18960c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "img-classifier",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
